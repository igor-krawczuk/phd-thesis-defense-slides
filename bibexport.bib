@online{230914322Smallscale,
  title = {[2309.14322] {{Small-scale}} Proxies for Large-Scale {{Transformer}} Training Instabilities},
  url = {https://arxiv.org/abs/2309.14322},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/PHFUUQU5/2309.html}
}

@article{abbeNonuniversalityDeepLearning2022c,
  title = {On the Non-Universality of Deep Learning: Quantifying the Cost of Symmetry},
  shorttitle = {On the Non-Universality of Deep Learning},
  author = {Abbe, Emmanuel and Boix-Adsera, Enric},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {17188--17201},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/6d9aac9407bcb1a5957401fa0b8de693-Abstract-Conference.html},
  urldate = {2024-02-24},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YDQJETRJ/Abbe and Boix-Adsera - 2022 - On the non-universality of deep learning quantify.pdf}
}

@article{abelFUBOCOStructureSynthesis2022,
  title = {{{FUBOCO}}: {{Structure Synthesis}} of {{Basic Op-Amps}} by {{FUnctional BlOck COmposition}}},
  shorttitle = {{{FUBOCO}}},
  author = {Abel, Inga and Graeb, Helmut},
  date = {2022-06-27},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {27},
  number = {6},
  pages = {63:1--63:27},
  issn = {1084-4309},
  doi = {10.1145/3522738},
  url = {https://dl.acm.org/doi/10.1145/3522738},
  urldate = {2024-02-19},
  abstract = {This article presents a method to automatically synthesize the structure and initial sizing of an operational amplifier. It is positioned between approaches with fixed design plans and a small search space of structures and approaches with generic structural production rules and a large search space with technically impractical structures. The presented approach develops a hierarchical composition graph based on functional blocks that spans a search space of thousands of technically meaningful structure variants for single-output, fully differential, and complementary operational amplifiers. The search algorithm is a combined heuristic and enumerative process. The evaluation is based on circuit sizing with a library of behavioral equations of functional blocks. Formalizing the knowledge of functional blocks in op-amps for structural synthesis and sizing inherently reduces the search space and lessens the number of created topologies not fulfilling the specifications. Experimental results for the three op-amp classes are presented. An outlook how this method can be extended to multi-stage op-amps is given.},
  keywords = {Analog circuit design,CMOS,operational amplifiers},
  file = {/home/krawczuk/Zotero/storage/I3K3EPJL/Abel and Graeb - 2022 - FUBOCO Structure Synthesis of Basic Op-Amps by FU.pdf}
}

@online{AdaptiveLayoutDecomposition,
  title = {Adaptive {{Layout Decomposition With Graph Embedding Neural Networks}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9672139?casa_token=H7aXYoXqoRsAAAAA:FV_lv-zfdfH0hasodGZR4fYBmowWv7vRcyeYF5q8oJCYfI_5_HAH-UaEFLsFAqxVzSHRw3xTiTuQ},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/SMEZUMPA/9672139.html}
}

@article{afacanReviewMachineLearning2021a,
  title = {Review: {{Machine}} Learning Techniques in Analog/{{RF}} Integrated Circuit Design, Synthesis, Layout, and Test},
  shorttitle = {Review},
  author = {Afacan, Engin and Lourenço, Nuno and Martins, Ricardo and Dündar, Günhan},
  date = {2021-03-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {77},
  pages = {113--130},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2020.11.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926020302947},
  urldate = {2024-02-20},
  abstract = {Rapid developments in semiconductor technology have substantially increased the computational capability of computers. As a result of this and recent developments in theory, machine learning (ML) techniques have become attractive in many new applications. This trend has also inspired researchers working on integrated circuit (IC) design and optimization. ML-based design approaches have gained importance to challenge/aid conventional design methods since they can be employed at different design levels, from modeling to test, to learn any nonlinear input-output relationship of any analog and radio frequency (RF) device or circuit; thus, providing fast and accurate responses to the task that they have learned. Furthermore, employment of ML techniques in analog/RF electronic design automation (EDA) tools boosts the performance of such tools. In this paper, we summarize the recent research and present a comprehensive review on ML techniques for analog/RF circuit modeling, design, synthesis, layout, and test.},
  keywords = {Analog and radio frequency,Artificial intelligence,Artificial neural network,Deep learning,Integrated circuits,Machine learning,Optimization,Synthesis},
  file = {/home/krawczuk/Zotero/storage/EVIQXSDB/Afacan et al. - 2021 - Review Machine learning techniques in analogRF i.pdf}
}

@article{afacanReviewMachineLearning2021b,
  title = {Review: {{Machine}} Learning Techniques in Analog/{{RF}} Integrated Circuit Design, Synthesis, Layout, and Test},
  shorttitle = {Review},
  author = {Afacan, Engin and Lourenço, Nuno and Martins, Ricardo and Dündar, Günhan},
  date = {2021-03-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {77},
  pages = {113--130},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2020.11.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926020302947},
  urldate = {2024-02-20},
  abstract = {Rapid developments in semiconductor technology have substantially increased the computational capability of computers. As a result of this and recent developments in theory, machine learning (ML) techniques have become attractive in many new applications. This trend has also inspired researchers working on integrated circuit (IC) design and optimization. ML-based design approaches have gained importance to challenge/aid conventional design methods since they can be employed at different design levels, from modeling to test, to learn any nonlinear input-output relationship of any analog and radio frequency (RF) device or circuit; thus, providing fast and accurate responses to the task that they have learned. Furthermore, employment of ML techniques in analog/RF electronic design automation (EDA) tools boosts the performance of such tools. In this paper, we summarize the recent research and present a comprehensive review on ML techniques for analog/RF circuit modeling, design, synthesis, layout, and test.},
  keywords = {Analog and radio frequency,Artificial intelligence,Artificial neural network,Deep learning,Integrated circuits,Machine learning,Optimization,Synthesis},
  file = {/home/krawczuk/Zotero/storage/J6UP9WKN/Afacan et al. - 2021 - Review Machine learning techniques in analogRF i.pdf}
}

@article{agarwalReincarnatingReinforcementLearning2022a,
  title = {Reincarnating {{Reinforcement Learning}}: {{Reusing Prior Computation}} to {{Accelerate Progress}}},
  shorttitle = {Reincarnating {{Reinforcement Learning}}},
  author = {Agarwal, Rishabh and Schwarzer, Max and Castro, Pablo Samuel and Courville, Aaron C. and Bellemare, Marc},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {28955--28971},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/ba1c5356d9164bb64c446a4b690226b0-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/QH28NQ36/Agarwal et al. - 2022 - Reincarnating Reinforcement Learning Reusing Prio.pdf}
}

@inproceedings{aggarwalMultilayerGridEmbeddings1985,
  title = {Multi-Layer Grid Embeddings},
  booktitle = {26th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1985)},
  author = {Aggarwal, Alok and Klawe, Maria and Lichtentein, David and Linial, Nathan and Wigderson, Avi},
  date = {1985},
  pages = {186--196},
  publisher = {{IEEE}},
  location = {{Portland, OR, USA}},
  doi = {10.1109/SFCS.1985.37},
  url = {http://ieeexplore.ieee.org/document/4568142/},
  urldate = {2024-02-22},
  abstract = {In this paper we propose two new multi-layer grid models for VLSI layout, both of which take into account the number of contact cuts used. For the first model in which nodes "exist" only on one layer, we prove a tight area x (number of contact = cuts) 8(n2) trade-off for embedding any degree 4 n-node planar graph in two layers. For the second model in which nodes "exist" simultaneously on all layers, we prove a number of bounds on the area needed to embed graphs using no contact cuts. For example we prove that any n-node graph which is the union of two planar subgraphs can be embedded on two layers in 0(n2) area without contact cuts. This bound is tight even if more layers and an unbounded number of contact cuts are allowed. We also show that planar graphs of bounded degree can be embedded on two layers in O(n1.6) area without contact cuts.},
  eventtitle = {26th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1985)},
  isbn = {978-0-8186-0644-1},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/IKERI2Q4/Aggarwal et al. - 1985 - Multi-layer grid embeddings.pdf}
}

@article{aggarwalMultilayerGridEmbeddings1991,
  title = {Multilayer Grid Embeddings for {{VLSI}}},
  author = {Aggarwal, Alok and Klawe, Maria and Shor, Peter},
  date = {1991-06-01},
  journaltitle = {Algorithmica},
  shortjournal = {Algorithmica},
  volume = {6},
  number = {1},
  pages = {129--151},
  issn = {1432-0541},
  doi = {10.1007/BF01759038},
  url = {https://doi.org/10.1007/BF01759038},
  urldate = {2024-02-22},
  abstract = {In this paper we propose two new multilayer grid models for VLSI layout, both of which take into account the number of contact cuts used. For the first model in which nodes “exist” only on one layer, we prove a tight area × (number of contact cuts) = Θ(n2) tradeoff for embeddingn-node planar graphs of bounded degree in two layers. For the second model in which nodes “exist” simultaneously on all layers, we give a number of upper bounds on the area needed to embed groups using no contact cuts. We show that anyn-node graph of thickness 2 can be embedded on two layers inO(n2) area. This bound is tight even if more layers and any number of contact cuts are allowed. We also show that planar graphs of bounded degree can be embedded on two layers inO(n3/2(logn)2) area.},
  langid = {english},
  keywords = {Embedding,Grid,Planar graph,Thickness,VLSI},
  file = {/home/krawczuk/Zotero/storage/Q86EPHLT/Aggarwal et al. - 1991 - Multilayer grid embeddings for VLSI.pdf}
}

@article{agnesinaGeneralFrameworkVLSIa,
  title = {A {{General Framework For VLSI Tool Parameter Optimization}} with {{Deep Reinforcement Learning}}},
  author = {Agnesina, Anthony and Pentapati, Sai and Lim, Sung Kyu},
  abstract = {Electronic design automation (EDA) tools and flows have steadily increased in complexity over the years, with modern tools offering more than 10,000 parameter settings, rendering the optimum tuning of such tools possible for only expert users. Automating this parameter setting for power-performance-area optimization would democratize modern EDA tools and VLSI physical design. In this paper, we present a general way of casting the parameter optimization problem into a reinforcement learning task. The resulting agent is then assigned to optimize a 2D VLSI placement step with proof-of-concept results. We conclude with a discussion of our ongoing work and how the methodology can be applied to 3D partitioning.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/ZCD2I2M7/Agnesina et al. - A General Framework For VLSI Tool Parameter Optimi.pdf}
}

@article{agrawalOptimalScalingMCMC2023,
  title = {Optimal Scaling of {{MCMC}} beyond {{Metropolis}}},
  author = {Agrawal, Sanket and Vats, Dootika and Łatuszyński, Krzysztof and Roberts, Gareth O.},
  date = {2023-06},
  journaltitle = {Advances in Applied Probability},
  volume = {55},
  number = {2},
  pages = {492--509},
  issn = {0001-8678, 1475-6064},
  doi = {10.1017/apr.2022.37},
  url = {https://www.cambridge.org/core/journals/advances-in-applied-probability/article/abs/optimal-scaling-of-mcmc-beyond-metropolis/2E5767C32506E5F1860E59B3BECECF59},
  urldate = {2024-02-25},
  abstract = {The problem of optimally scaling the proposal distribution in a Markov chain Monte Carlo algorithm is critical to the quality of the generated samples. Much work has gone into obtaining such results for various Metropolis–Hastings (MH) algorithms. Recently, acceptance probabilities other than MH are being employed in problems with intractable target distributions. There are few resources available on tuning the Gaussian proposal distributions for this situation. We obtain optimal scaling results for a general class of acceptance functions, which includes Barker’s and lazy MH. In particular, optimal values for Barker’s algorithm are derived and found to be significantly different from that obtained for the MH algorithm. Our theoretical conclusions are supported by numerical simulations indicating that when the optimal proposal variance is unknown, tuning to the optimal acceptance probability remains an effective strategy.},
  langid = {english},
  keywords = {60F05,65C05,Barker’s acceptance,lazy MH,Metropolis–Hastings,tuning,weak convergence},
  file = {/home/krawczuk/Zotero/storage/DSHK523M/Agrawal et al. - 2023 - Optimal scaling of MCMC beyond Metropolis.pdf}
}

@article{albeattieBetterSPQRtreeDecomposition2024,
  title = {A Better {{SPQR-tree}} Decomposition of Electrical Circuits Containing Multiports and Its Application to Wave Digital Emulation},
  author = {Al Beattie, Bakr and Ochs, Karlheinz},
  date = {2024},
  journaltitle = {International Journal of Circuit Theory and Applications},
  volume = {52},
  number = {2},
  pages = {536--550},
  issn = {1097-007X},
  doi = {10.1002/cta.3781},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cta.3781},
  urldate = {2024-02-20},
  abstract = {We report on a systematic method for the port-wise decomposition of electrical networks. Our method exploits the SPQR-tree decomposition as an algorithmic way for decomposing a given circuit into a maximal set of one-port and multiports. We provide a solution for representing multiports by suitable replacement graphs, such that they are encapsulated not decomposed or processed by the SPQR-algorithm. Contrary to current methods, our replacement graphs are not sophisticated but rather simple, which, in general, enhances the performance of the algorithm, when it comes to decomposing large electrical networks containing multiports. Some electrical devices, such as transistors or op-amps, are commonly described in terms of their terminals rather than their ports. Thus, we cover the application of the template-based approach to three-terminal devices. Lastly, we make use of the new decomposition method for the emulation of a wave digital filter.},
  langid = {english},
  keywords = {graph theory,port-wise decomposition,SPQR-tree,wave digital structures},
  file = {/home/krawczuk/Zotero/storage/KD6YHJJQ/Al Beattie and Ochs - 2024 - A better SPQR-tree decomposition of electrical cir.pdf}
}

@online{alemohammadSelfConsumingGenerativeModels2023,
  title = {Self-{{Consuming Generative Models Go MAD}}},
  author = {Alemohammad, Sina and Casco-Rodriguez, Josue and Luzi, Lorenzo and Humayun, Ahmed Imtiaz and Babaei, Hossein and LeJeune, Daniel and Siahkoohi, Ali and Baraniuk, Richard G.},
  date = {2023-07-04},
  eprint = {2307.01850},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.01850},
  url = {http://arxiv.org/abs/2307.01850},
  urldate = {2024-02-20},
  abstract = {Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/J7UZ2BNV/Alemohammad et al. - 2023 - Self-Consuming Generative Models Go MAD.pdf;/home/krawczuk/Zotero/storage/4BPQEZH6/2307.html}
}

@online{AlgorithmsTopologySynthesis,
  title = {Algorithms for Topology Synthesis of Analog Circuits - {{ProQuest}}},
  url = {https://www.proquest.com/openview/633d818e7e92de309b764391a0186637/1?pq-origsite=gscholar&cbl=18750},
  urldate = {2024-02-20},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/LSIK3EHX/1.html}
}

@online{AlgorithmsVLSICircuit,
  title = {Algorithms for {{VLSI}} Circuit Optimization and {{GPU-based}} Parallelization - {{ProQuest}}},
  url = {https://www.proquest.com/docview/739030016?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/ZMXGX3WM/739030016.html}
}

@article{alvianoAnytimeAnswerSet2016,
  title = {Anytime Answer Set Optimization via Unsatisfiable Core Shrinking},
  author = {Alviano, Mario and Dodaro, Carmine},
  date = {2016-09},
  journaltitle = {Theory and Practice of Logic Programming},
  volume = {16},
  number = {5-6},
  pages = {533--551},
  issn = {1471-0684, 1475-3081},
  doi = {10.1017/S147106841600020X},
  url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/anytime-answer-set-optimization-via-unsatisfiable-core-shrinking/55F4305D2BAAD203E8177F3955C9DEEA},
  urldate = {2024-02-20},
  abstract = {Unsatisfiable core analysis can boost the computation of optimum stable models for logic programs with weak constraints. However, current solvers employing unsatisfiable core analysis either run to completion, or provide no suboptimal stable models but the one resulting from the preliminary disjoint cores analysis. This drawback is circumvented here by introducing a progression based shrinking of the analyzed unsatisfiable cores. In fact, suboptimal stable models are possibly found while shrinking unsatisfiable cores, hence resulting into an anytime algorithm. Moreover, as confirmed empirically, unsatisfiable core analysis also benefits from the shrinking process in terms of solved instances.},
  langid = {english},
  keywords = {answer set programming,unsatisfiable cores,weak constraints},
  file = {/home/krawczuk/Zotero/storage/9V877HW5/Alviano and Dodaro - 2016 - Anytime answer set optimization via unsatisfiable .pdf}
}

@online{AnalogCircuitSynthesis,
  title = {Analog Circuit Synthesis by Superimposing of Sub-Circuits | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/922076?casa_token=VQJfMEDi324AAAAA:fsCB-LbGJT0PZzx36ih_aGXXzfb4pX0_MIju9rpAyxQGBxJee3_nw5bP38DD7bPSJCTza_PvukFJ},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/PPQ2LP28/922076.html}
}

@online{AnalogIntegratedCircuitc,
  title = {Analog {{Integrated Circuit Topology Synthesis With Deep Reinforcement Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9718525?casa_token=dVivchwUEd0AAAAA:wVIv3ieX3FMg40vdPpBqhIj2Ak4TY6zQiwvDmX-Tjk52V40x9NueALgwluqFmmA3IbT3JT-vk1HW},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/X7QUN29V/9718525.html}
}

@online{AnalogIntegratedCircuitd,
  title = {Analog {{Integrated Circuit Topology Synthesis With Deep Reinforcement Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9718525?casa_token=HRVQ9BNU-cMAAAAA:UrmxLS-X4kLj-UTwjoqpwWbbu0-dJoGANMFbAuRtBrg5WmGceBA8a0FziiNJTgoHYJelutqhIbeU},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/CD2JI95B/9718525.html}
}

@online{AnalogMixedSignalCircuit,
  title = {Analog/{{Mixed-Signal Circuit Synthesis Enabled}} by the {{Advancements}} of {{Circuit Architectures}} and {{Machine Learning Algorithms}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9712577?casa_token=i6LpwT0sCxcAAAAA:sPuySgP4jlPu4cfSF2ywRxIwxFN8LgoZp22RSavexDOR5fin8Eh8ivfnkJcL80wluwk3kNzkcYmt},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/PRAEZ6RQ/9712577.html}
}

@book{aptNewPerspectivesGames2008,
  title = {New Perspectives on Games and Interaction},
  editor = {Apt, Krzysztof R. and Van Rooij, Robert},
  date = {2008},
  series = {Texts in Logic and Games},
  number = {4},
  publisher = {{Amsterdam University Press}},
  location = {{Amsterdam}},
  isbn = {978-90-8964-057-4},
  langid = {english},
  pagetotal = {328},
  file = {/home/krawczuk/Zotero/storage/229PSJ6U/Apt and Van Rooij - 2008 - New perspectives on games and interaction.pdf}
}

@article{arayaSeededGraphMatching,
  title = {Seeded {{Graph Matching}} for the {{Correlated Gaussian Wigner Model}} via the {{Projected Power Method}}},
  author = {Araya, Ernesto and Braun, Guillaume and Tyagi, Hemant},
  abstract = {In the graph matching problem we observe two graphs G, H and the goal is to find an assignment (or matching) between their vertices such that some measure of edge agreement is maximized. We assume in this work that the observed pair G, H has been drawn from the Correlated Gaussian Wigner (CGW) model – a popular model for correlated weighted graphs – where the entries of the adjacency matrices of G and H are independent Gaussians and each edge of G is correlated with one edge of H (determined by the unknown matching) with the edge correlation described by a parameter σ ∈ [0, 1). In this paper, we analyse the performance of the projected power method (PPM) as a seeded graph matching algorithm where we are given an initial partially correct matching (called the seed) as side information. We prove that if the seed is close enough to the ground-truth matching, then with high probability, PPM iteratively improves the seed and recovers the ground-truth matching (either partially or exactly) in O(log n) iterations. Our results prove that PPM works even in regimes of constant σ, thus extending the analysis in (Mao et al., 2023) for the sparse Correlated Erdős-Renyi (CER) model to the (dense) CGW model. As a byproduct of our analysis, we see that the PPM framework generalizes some of the state-of-art algorithms for seeded graph matching. We support and complement our theoretical findings with numerical experiments on synthetic data.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/2W8WCHSV/Araya et al. - Seeded Graph Matching for the Correlated Gaussian .pdf}
}

@online{atanasovNeuralNetworksKernel2021b,
  title = {Neural {{Networks}} as {{Kernel Learners}}: {{The Silent Alignment Effect}}},
  shorttitle = {Neural {{Networks}} as {{Kernel Learners}}},
  author = {Atanasov, Alexander and Bordelon, Blake and Pehlevan, Cengiz},
  date = {2021-12-02},
  eprint = {2111.00034},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2111.00034},
  url = {http://arxiv.org/abs/2111.00034},
  urldate = {2024-02-26},
  abstract = {Neural networks in the lazy training regime converge to kernel machines. Can neural networks in the rich feature learning regime learn a kernel machine with a data-dependent kernel? We demonstrate that this can indeed happen due to a phenomenon we term silent alignment, which requires that the tangent kernel of a network evolves in eigenstructure while small and before the loss appreciably decreases, and grows only in overall scale afterwards. We show that such an effect takes place in homogenous neural networks with small initialization and whitened data. We provide an analytical treatment of this effect in the linear network case. In general, we find that the kernel develops a low-rank contribution in the early phase of training, and then evolves in overall scale, yielding a function equivalent to a kernel regression solution with the final network's tangent kernel. The early spectral learning of the kernel depends on the depth. We also demonstrate that non-whitened data can weaken the silent alignment effect.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/SKSK3N7Z/Atanasov et al. - 2021 - Neural Networks as Kernel Learners The Silent Ali.pdf;/home/krawczuk/Zotero/storage/RS5IUZ64/2111.html}
}

@inproceedings{austinStructuredDenoisingDiffusion2021e,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and family=Berg, given=Rianne, prefix=van den, useprefix=true},
  date = {2021},
  volume = {34},
  pages = {17981--17993},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Denoising diffusion probabilistic models (DDPMs) [Ho et al. 2021] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [2021], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss.  For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/VPZZMZW9/Austin et al. - 2021 - Structured Denoising Diffusion Models in Discrete .pdf}
}

@online{AutomatedDesignAnalog,
  title = {Automated {{Design}} of {{Analog Circuits Using Reinforcement Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9576505?casa_token=3fLtOocO4noAAAAA:bNHmff4veiE13woBo4vZGsl3Pfs4OifW9zp7ecaMGJWq9vM3FRjCEGBbsLL2FrFuNhcD2mRH2gCc},
  urldate = {2024-02-20}
}

@online{AutomatedTopologySynthesisa,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939?casa_token=R5QoZc365Q8AAAAA:owOf8EzUqPGYmwhhu9gwcPgTm3LWL_W40JAyhtml33m6YrV12b6YU7s_4tVzCwP0NaVir_U6a0I},
  urldate = {2024-02-24},
  file = {/home/krawczuk/Zotero/storage/SFDIINX4/9022939.html}
}

@online{AutomaticGenerationSynthetic,
  title = {Automatic Generation of Synthetic Sequential Benchmark Circuits | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/1020350?casa_token=_xZfijPDv2kAAAAA:AzZcTFlVzzkSVUCb-P77q6Jg_VA3leMyffgFkCSWXmLugdY5bzZ1soIq4-9C-AGvLCwBl2XVajPl},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/IG65UUQG/1020350.html}
}

@online{AutomaticOpAmpGeneration,
  title = {Automatic {{Op-Amp Generation From Specification}} to {{Layout}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10185586?casa_token=m4V1oATbeB8AAAAA:NiuECNoipkEm_0-KAZPn2Iye-EZOOhwO5X4Ox-ExnKAraEfSN6yzplpv1ZsB_b6aTV-p-X674ghJ},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/A44ZQAVN/10185586.html}
}

@article{avinFillingGapsTrustworthy2021,
  title = {Filling Gaps in Trustworthy Development of {{AI}}},
  author = {Avin, Shahar and Belfield, Haydn and Brundage, Miles and Krueger, Gretchen and Wang, Jasmine and Weller, Adrian and Anderljung, Markus and Krawczuk, Igor and Krueger, David and Lebensold, Jonathan and Maharaj, Tegan and Zilberman, Noa},
  date = {2021-12-10},
  journaltitle = {Science},
  shortjournal = {Science},
  volume = {374},
  number = {6573},
  pages = {1327--1329},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.abi7176},
  url = {https://www.science.org/doi/10.1126/science.abi7176},
  urldate = {2024-02-27},
  abstract = {Incident sharing, auditing, and other concrete mechanisms could help verify the trustworthiness of actors           ,                             The range of application of artificial intelligence (AI) is vast, as is the potential for harm. Growing awareness of potential risks from AI systems has spurred action to address those risks while eroding confidence in AI systems and the organizations that develop them. A 2019 study (                                1                              ) found more than 80 organizations that have published and adopted “AI ethics principles,” and more have joined since. But the principles often leave a gap between the “what” and the “how” of trustworthy AI development. Such gaps have enabled questionable or ethically dubious behavior, which casts doubts on the trustworthiness of specific organizations, and the field more broadly. There is thus an urgent need for concrete methods that both enable AI developers to prevent harm and allow them to demonstrate their trustworthiness through verifiable behavior. Below, we explore mechanisms [drawn from (                                2                              )] for creating an ecosystem where AI developers can earn trust—if they are trustworthy (see the figure). Better assessment of developer trustworthiness could inform user choice, employee actions, investment decisions, legal recourse, and emerging governance regimes.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/JQS76CL4/Avin et al. - 2021 - Filling gaps in trustworthy development of AI.pdf}
}

@online{baekAccurateLearningGraph2021b,
  title = {Accurate {{Learning}} of {{Graph Representations}} with {{Graph Multiset Pooling}}},
  author = {Baek, Jinheon and Kang, Minki and Hwang, Sung Ju},
  date = {2021-06-28},
  eprint = {2102.11533},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2102.11533},
  url = {http://arxiv.org/abs/2102.11533},
  urldate = {2024-02-26},
  abstract = {Graph neural networks have been widely used on modeling graph data, achieving impressive results on node classification and link prediction tasks. Yet, obtaining an accurate representation for a graph further requires a pooling function that maps a set of node representations into a compact form. A simple sum or average over all node representations considers all node features equally without consideration of their task relevance, and any structural dependencies among them. Recently proposed hierarchical graph pooling methods, on the other hand, may yield the same representation for two different graphs that are distinguished by the Weisfeiler-Lehman test, as they suboptimally preserve information from the node features. To tackle these limitations of existing graph pooling methods, we first formulate the graph pooling problem as a multiset encoding problem with auxiliary information about the graph structure, and propose a Graph Multiset Transformer (GMT) which is a multi-head attention based global pooling layer that captures the interaction between nodes according to their structural dependencies. We show that GMT satisfies both injectiveness and permutation invariance, such that it is at most as powerful as the Weisfeiler-Lehman graph isomorphism test. Moreover, our methods can be easily extended to the previous node clustering approaches for hierarchical graph pooling. Our experimental results show that GMT significantly outperforms state-of-the-art graph pooling methods on graph classification benchmarks with high memory and time efficiency, and obtains even larger performance gain on graph reconstruction and generation tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/NB4SFDGV/Baek et al. - 2021 - Accurate Learning of Graph Representations with Gr.pdf;/home/krawczuk/Zotero/storage/FNGHJB88/2102.html}
}

@inproceedings{balcilarBreakingLimitsMessage2021b,
  title = {Breaking the {{Limits}} of {{Message Passing Graph Neural Networks}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Balcilar, Muhammet and Heroux, Pierre and Gauzere, Benoit and Vasseur, Pascal and Adam, Sebastien and Honeine, Paul},
  date = {2021-07-01},
  pages = {599--608},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/balcilar21a.html},
  urldate = {2024-02-24},
  abstract = {Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in O(n3)O(n3)\textbackslash mathcal\{O\}(n\^{}3) with memory usage in O(n2)O(n2)\textbackslash mathcal\{O\}(n\^{}2), consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/IC2ZGHKK/Balcilar et al. - 2021 - Breaking the Limits of Message Passing Graph Neura.pdf;/home/krawczuk/Zotero/storage/NNPNZ27K/Balcilar et al. - 2021 - Breaking the Limits of Message Passing Graph Neura.pdf}
}

@online{balcilarBreakingLimitsMessage2021c,
  title = {Breaking the {{Limits}} of {{Message Passing Graph Neural Networks}}},
  author = {Balcilar, Muhammet and Héroux, Pierre and Gaüzère, Benoit and Vasseur, Pascal and Adam, Sébastien and Honeine, Paul},
  date = {2021-06-08},
  eprint = {2106.04319},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.04319},
  url = {http://arxiv.org/abs/2106.04319},
  urldate = {2024-02-24},
  abstract = {Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in \$\textbackslash mathcal\{O\}(n\^{}3)\$ with memory usage in \$\textbackslash mathcal\{O\}(n\^{}2)\$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.},
  pubstate = {preprint},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.5.0},
  file = {/home/krawczuk/Zotero/storage/K44HVA6W/Balcilar et al. - 2021 - Breaking the Limits of Message Passing Graph Neura.pdf;/home/krawczuk/Zotero/storage/Z2KY3MN2/2106.html}
}

@article{basuEfficientEquivariantTransfer2024,
  title = {Efficient {{Equivariant Transfer Learning}} from {{Pretrained Models}}},
  author = {Basu, Sourya and Katdare, Pulkit and Sattigeri, Prasanna and Chenthamarakshan, Vijil and Driggs-Campbell, Katherine and Das, Payel and Varshney, Lav R.},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0d02892a0055c94584f6394f8d069c8e-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/CWM84MRY/Basu et al. - 2024 - Efficient Equivariant Transfer Learning from Pretr.pdf}
}

@online{battagliaRelationalInductiveBiases2018k,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-10-17},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.01261},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2024-02-24},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/3QVCGNQI/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/home/krawczuk/Zotero/storage/HVRS9Z3A/1806.html}
}

@online{BayesianOptimizationApproach,
  title = {Bayesian {{Optimization Approach}} for {{Analog Circuit Synthesis Using Neural Network}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/8714788},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/QL6XW7YM/8714788.html}
}

@online{BayesianOptimizationApproacha,
  title = {Bayesian {{Optimization Approach}} for {{RF Circuit Synthesis}} via {{Multitask Neural Network Enhanced Gaussian Process}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9852008?casa_token=-1QQmC7K-EcAAAAA:RV2ho8BpTRzIUNbsA27gCdQGYOXPyOBJBA-ZCvhZHQyDFRn5fVYAX_Fijuc_OrMU5eS1Z2IuXemw},
  urldate = {2024-02-20}
}

@online{belcakNeuralCombinatorialLogic2022a,
  title = {Neural {{Combinatorial Logic Circuit Synthesis}} from {{Input-Output Examples}}},
  author = {Belcak, Peter and Wattenhofer, Roger},
  date = {2022-10-29},
  eprint = {2210.16606},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.16606},
  url = {http://arxiv.org/abs/2210.16606},
  urldate = {2024-02-24},
  abstract = {We propose a novel, fully explainable neural approach to synthesis of combinatorial logic circuits from input-output examples. The carrying advantage of our method is that it readily extends to inductive scenarios, where the set of examples is incomplete but still indicative of the desired behaviour. Our method can be employed for a virtually arbitrary choice of atoms - from logic gates to FPGA blocks - as long as they can be formulated in a differentiable fashion, and consistently yields good results for synthesis of practical circuits of increasing size. In particular, we succeed in learning a number of arithmetic, bitwise, and signal-routing operations, and even generalise towards the correct behaviour in inductive scenarios. Our method, attacking a discrete logical synthesis problem with an explainable neural approach, hints at a wider promise for synthesis and reasoning-related tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {/home/krawczuk/Zotero/storage/XADZ3TZX/Belcak and Wattenhofer - 2022 - Neural Combinatorial Logic Circuit Synthesis from .pdf;/home/krawczuk/Zotero/storage/B9YGIRBV/2210.html}
}

@online{bergmeisterEfficientScalableGraph2024,
  title = {Efficient and {{Scalable Graph Generation}} through {{Iterative Local Expansion}}},
  author = {Bergmeister, Andreas and Martinkus, Karolis and Perraudin, Nathanaël and Wattenhofer, Roger},
  date = {2024-02-21},
  eprint = {2312.11529},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.11529},
  url = {http://arxiv.org/abs/2312.11529},
  urldate = {2024-02-25},
  abstract = {In the realm of generative models for graphs, extensive research has been conducted. However, most existing methods struggle with large graphs due to the complexity of representing the entire joint distribution across all node pairs and capturing both global and local graph structures simultaneously. To overcome these issues, we introduce a method that generates a graph by progressively expanding a single node to a target graph. In each step, nodes and edges are added in a localized manner through denoising diffusion, building first the global structure, and then refining the local details. The local generation avoids modeling the entire joint distribution over all node pairs, achieving substantial computational savings with subquadratic runtime relative to node count while maintaining high expressivity through multiscale generation. Our experiments show that our model achieves state-of-the-art performance on well-established benchmark datasets while successfully scaling to graphs with at least 5000 nodes. Our method is also the first to successfully extrapolate to graphs outside of the training distribution, showcasing a much better generalization capability over existing methods.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/9WX3T4FX/Bergmeister et al. - 2024 - Efficient and Scalable Graph Generation through It.pdf;/home/krawczuk/Zotero/storage/UP9FGRZF/2312.html}
}

@inproceedings{bhanjaGraphBasedSynthesis2017b,
  title = {A Graph Based Synthesis Procedure for Linear Analog Function},
  booktitle = {2017 30th {{IEEE International System-on-Chip Conference}} ({{SOCC}})},
  author = {Bhanja, Mousumi and Ray, Baidyanath},
  date = {2017-09},
  pages = {328--333},
  publisher = {{IEEE}},
  location = {{Munich}},
  doi = {10.1109/SOCC.2017.8226071},
  url = {http://ieeexplore.ieee.org/document/8226071/},
  urldate = {2024-02-20},
  abstract = {This paper presents a graph based synthesis procedure for reconfigurable linear analog function. A n-level weighted binary tree structure has been used to represent nth order linear network. Root of the binary tree has two children nodes with weights of first order lowpass filter (LPF) and first order highpass filter (HPF). Traversing through each possible path in the tree implements one filter type. The level 2 binary tree has been transformed to a hexagonal closed graph. This conversion has been done to map the proposed synthesis procedure into field programmable analog array (FPAA), which demonstrates the reusuability and programmability. First order LPF and HPF has been used as basic building blocks, whereas a hexagonal structure is denoted as a configurable analog block (CAB) of the FPAA. The hexagonal topology of the FPAA gives the versatile connectivity between two adjacent CABS of the FPAA. Performance has been verified through SPICE simulations.},
  eventtitle = {2017 30th {{IEEE International System-on-Chip Conference}} ({{SOCC}})},
  isbn = {978-1-5386-4034-0},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/449LB5KI/Bhanja and Ray - 2017 - A graph based synthesis procedure for linear analo.pdf}
}

@inproceedings{bhanjaGraphBasedSystematic2023,
  title = {Graph {{Based Systematic Synthesis Procedure}} of Gm-{{C Filter}}},
  booktitle = {2023 7th {{International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  author = {Bhanja, Mousumi and Ganguly, Anirban and Parija, Smita Rani},
  date = {2023},
  pages = {407--412},
  publisher = {{IEEE}},
  doi = {10.1109/ICECA58529.2023.10395319},
  url = {https://ieeexplore.ieee.org/abstract/document/10395319/?casa_token=Ci_nxVLq0woAAAAA:IbOCUVI_rDxU78Kk4PN08JF2dFYyHh-tXY5FyipkaE5K4tE4pYpUx2N6ibwQGMF3FbLxAEjt_Uzq},
  urldate = {2024-02-19}
}

@article{bianchiExpressivePowerPooling2023a,
  title = {The Expressive Power of Pooling in {{Graph Neural Networks}}},
  author = {Bianchi, Filippo Maria and Lachi, Veronica},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/e26f31de8b13ec569bf507e6ae2cd952-Abstract-Conference.html},
  urldate = {2024-02-24},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/89T6775T/Bianchi and Lachi - 2023 - The expressive power of pooling in Graph Neural Ne.pdf}
}

@inproceedings{biedlPointsetEmbeddabilityProblem2012,
  title = {The Point-Set Embeddability Problem for Plane Graphs},
  booktitle = {Proceedings of the Twenty-Eighth Annual Symposium on {{Computational}} Geometry},
  author = {Biedl, Therese and Vatshelle, Martin},
  date = {2012-06-17},
  series = {{{SoCG}} '12},
  pages = {41--50},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2261250.2261257},
  url = {https://dl.acm.org/doi/10.1145/2261250.2261257},
  urldate = {2024-02-22},
  abstract = {In this paper, we study the point-set-embeddability-problem, i.e., given a planar graph and a set of points, is there a mapping of the vertices to the points such that the resulting straight-line drawing is planar? It was known that this problem is NP-hard if the embedding can be chosen, but becomes polynomial for triangulated graphs of treewidth 3. We show here that in fact it can be answered for all planar graphs with a fixed combinatorial embedding that have constant treewidth and constant face-degree. We also prove that as soon as one of the conditions is dropped (i.e., either the treewidth is unbounded or some faces have large degrees), point-set-embeddability with a fixed embedding becomes NP-hard. The NP-hardness holds even for a 3-connected planar graph with constant treewidth, triangulated planar graphs, or 2-connected outer-planar graphs.},
  isbn = {978-1-4503-1299-8},
  keywords = {carving width,graph drawing,point-set embedding},
  file = {/home/krawczuk/Zotero/storage/YB9HI3YN/Biedl and Vatshelle - 2012 - The point-set embeddability problem for plane grap.pdf}
}

@inproceedings{biedlPointsetEmbeddabilityProblem2012a,
  title = {The Point-Set Embeddability Problem for Plane Graphs},
  booktitle = {Proceedings of the Twenty-Eighth Annual Symposium on {{Computational}} Geometry},
  author = {Biedl, Therese and Vatshelle, Martin},
  date = {2012-06-17},
  series = {{{SoCG}} '12},
  pages = {41--50},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2261250.2261257},
  url = {https://dl.acm.org/doi/10.1145/2261250.2261257},
  urldate = {2024-02-22},
  abstract = {In this paper, we study the point-set-embeddability-problem, i.e., given a planar graph and a set of points, is there a mapping of the vertices to the points such that the resulting straight-line drawing is planar? It was known that this problem is NP-hard if the embedding can be chosen, but becomes polynomial for triangulated graphs of treewidth 3. We show here that in fact it can be answered for all planar graphs with a fixed combinatorial embedding that have constant treewidth and constant face-degree. We also prove that as soon as one of the conditions is dropped (i.e., either the treewidth is unbounded or some faces have large degrees), point-set-embeddability with a fixed embedding becomes NP-hard. The NP-hardness holds even for a 3-connected planar graph with constant treewidth, triangulated planar graphs, or 2-connected outer-planar graphs.},
  isbn = {978-1-4503-1299-8},
  keywords = {carving width,graph drawing,point-set embedding},
  file = {/home/krawczuk/Zotero/storage/GY5KXWMI/Biedl and Vatshelle - 2012 - The point-set embeddability problem for plane grap.pdf}
}

@inproceedings{biettiSampleComplexityLearning2021a,
  title = {On the {{Sample Complexity}} of {{Learning}} under {{Geometric Stability}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bietti, Alberto and Venturi, Luca and Bruna, Joan},
  date = {2021},
  volume = {34},
  pages = {18673--18684},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/9ac5a6d86e8924182271bd820acbce0e-Abstract.html},
  urldate = {2024-02-26},
  abstract = {Many supervised learning problems involve high-dimensional data such as images, text, or graphs. In order to make efficient use of data, it is often useful to leverage certain geometric priors in the problem at hand, such as invariance to translations, permutation subgroups, or stability to small deformations. We study the sample complexity of learning problems where the target function presents such invariance and stability properties, by considering spherical harmonic decompositions of such functions on the sphere. We provide non-parametric rates of convergence for kernel methods, and show improvements in sample complexity by a factor equal to the size of the group when using an invariant kernel over the group, compared to the corresponding non-invariant kernel. These improvements are valid when the sample size is large enough, with an asymptotic behavior that depends on spectral properties of the group. Finally, these gains are extended beyond invariance groups to also cover geometric stability to small deformations, modeled here as subsets (not necessarily subgroups) of permutations.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YMPSRJPM/Bietti et al. - 2021 - On the Sample Complexity of Learning under Geometr.pdf}
}

@online{blattmannStableVideoDiffusion2023,
  title = {Stable {{Video Diffusion}}: {{Scaling Latent Video Diffusion Models}} to {{Large Datasets}}},
  shorttitle = {Stable {{Video Diffusion}}},
  author = {Blattmann, Andreas and Dockhorn, Tim and Kulal, Sumith and Mendelevitch, Daniel and Kilian, Maciej and Lorenz, Dominik and Levi, Yam and English, Zion and Voleti, Vikram and Letts, Adam and Jampani, Varun and Rombach, Robin},
  date = {2023-11-25},
  eprint = {2311.15127},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.15127},
  url = {http://arxiv.org/abs/2311.15127},
  urldate = {2024-02-25},
  abstract = {We present Stable Video Diffusion - a latent video diffusion model for high-resolution, state-of-the-art text-to-video and image-to-video generation. Recently, latent diffusion models trained for 2D image synthesis have been turned into generative video models by inserting temporal layers and finetuning them on small, high-quality video datasets. However, training methods in the literature vary widely, and the field has yet to agree on a unified strategy for curating video data. In this paper, we identify and evaluate three different stages for successful training of video LDMs: text-to-image pretraining, video pretraining, and high-quality video finetuning. Furthermore, we demonstrate the necessity of a well-curated pretraining dataset for generating high-quality videos and present a systematic curation process to train a strong base model, including captioning and filtering strategies. We then explore the impact of finetuning our base model on high-quality data and train a text-to-video model that is competitive with closed-source video generation. We also show that our base model provides a powerful motion representation for downstream tasks such as image-to-video generation and adaptability to camera motion-specific LoRA modules. Finally, we demonstrate that our model provides a strong multi-view 3D-prior and can serve as a base to finetune a multi-view diffusion model that jointly generates multiple views of objects in a feedforward fashion, outperforming image-based methods at a fraction of their compute budget. We release code and model weights at https://github.com/Stability-AI/generative-models .},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/krawczuk/Zotero/storage/KSN8SULL/Blattmann et al. - 2023 - Stable Video Diffusion Scaling Latent Video Diffu.pdf;/home/krawczuk/Zotero/storage/5CW8EDKX/2311.html}
}

@article{blondelFastUnfoldingCommunities2008d,
  title = {Fast Unfolding of Communities in Large Networks},
  author = {Blondel, Vincent D. and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  date = {2008-10},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2008},
  number = {10},
  pages = {P10008},
  issn = {1742-5468},
  doi = {2008101003130400},
  url = {https://dx.doi.org/10.1088/1742-5468/2008/10/P10008},
  urldate = {2024-02-25},
  abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.},
  langid = {english}
}

@article{blondelFastUnfoldingCommunities2008e,
  title = {Fast Unfolding of Communities in Large Networks},
  author = {Blondel, Vincent D and Guillaume, Jean-Loup and Lambiotte, Renaud and Lefebvre, Etienne},
  date = {2008-10-09},
  journaltitle = {Journal of Statistical Mechanics: Theory and Experiment},
  shortjournal = {J. Stat. Mech.},
  volume = {2008},
  number = {10},
  pages = {P10008},
  issn = {1742-5468},
  doi = {2008101003130400},
  url = {https://iopscience.iop.org/article/10.1088/1742-5468/2008/10/P10008},
  urldate = {2024-02-25},
  abstract = {We propose a simple method to extract the community structure of large networks. Our method is a heuristic method that is based on modularity optimization. It is shown to outperform all other known community detection methods in terms of computation time. Moreover, the quality of the communities detected is very good, as measured by the so-called modularity. This is shown first by identifying language communities in a Belgian mobile phone network of 2 million customers and by analysing a web graph of 118 million nodes and more than one billion links. The accuracy of our algorithm is also verified on ad hoc modular networks.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/KSTM6M2A/Blondel et al. - 2008 - Fast unfolding of communities in large networks.pdf}
}

@article{bodnarNeuralSheafDiffusion2022a,
  title = {Neural {{Sheaf Diffusion}}: {{A Topological Perspective}} on {{Heterophily}} and {{Oversmoothing}} in {{GNNs}}},
  shorttitle = {Neural {{Sheaf Diffusion}}},
  author = {Bodnar, Cristian and Di Giovanni, Francesco and Chamberlain, Benjamin and Lió, Pietro and Bronstein, Michael},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {18527--18541},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/75c45fca2aa416ada062b26cc4fb7641-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/32H4LP5G/Bodnar et al. - 2022 - Neural Sheaf Diffusion A Topological Perspective .pdf}
}

@article{bokerFinegrainedExpressivityGraph2023,
  title = {Fine-Grained {{Expressivity}} of {{Graph Neural Networks}}},
  author = {Böker, Jan and Levie, Ron and Huang, Ningyuan and Villar, Soledad and Morris, Christopher},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9200d97ca2bf3a26db7b591844014f00-Abstract-Conference.html},
  urldate = {2024-02-24},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/MHZYYDT7/Böker et al. - 2023 - Fine-grained Expressivity of Graph Neural Networks.pdf}
}

@article{bollaSpectralPropertiesModularity2015a,
  title = {Spectral Properties of Modularity Matrices},
  author = {Bolla, Marianna and Bullins, Brian and Chaturapruek, Sorathan and Chen, Shiwen and Friedl, Katalin},
  date = {2015-05},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {473},
  pages = {359--376},
  issn = {00243795},
  doi = {10.1016/j.laa.2014.10.039},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379514007186},
  urldate = {2024-02-22},
  abstract = {There is an exact relation between the spectra of modularity matrices introduced in social network analysis and the χ2 statistic. We investigate a weighted graph with the main interest being when the hypothesis of independent attachment of the vertices is rejected, and we look for clusters of vertices with higher inter-cluster relations than expected under the hypothesis of independence. In this context, we give a sufficient condition for a weighted, and a sufficient and necessary condition for an unweighted graph to have at least one positive eigenvalue in its modularity or normalized modularity spectrum, which guarantees a community structure with more than one cluster. This property has important implications for the isoperimetric inequality, the symmetric maximal correlation, and the Newman–Girvan modularity.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/R8LT2LCT/Bolla et al. - 2015 - Spectral properties of modularity matrices.pdf}
}

@unpublished{bommasaniGeneralizedOptimalLinear2020,
  title = {Generalized {{Optimal Linear Orders}}},
  author = {Bommasani, Rishi},
  date = {2020},
  eprint = {2108.10692},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.7298/5x0j-me63},
  url = {http://arxiv.org/abs/2108.10692},
  urldate = {2024-02-22},
  abstract = {The sequential structure of language, and the order of words in a sentence specifically, plays a central role in human language processing. Consequently, in designing computational models of language, the de facto approach is to present sentences to machines with the words ordered in the same order as in the original human-authored sentence. The very essence of this work is to question the implicit assumption that this is desirable and inject theoretical soundness into the consideration of word order in natural language processing. In this thesis, we begin by uniting the disparate treatments of word order in cognitive science, psycholinguistics, computational linguistics, and natural language processing under a flexible algorithmic framework. We proceed to use this heterogeneous theoretical foundation as the basis for exploring new word orders with an undercurrent of psycholinguistic optimality. In particular, we focus on notions of dependency length minimization given the difficulties in human and computational language processing in handling long-distance dependencies. We then discuss algorithms for finding optimal word orders efficiently in spite of the combinatorial space of possibilities. We conclude by addressing the implications of these word orders on human language and their downstream impacts when integrated in computational models.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/Z7Q8JXYX/Bommasani - 2020 - Generalized Optimal Linear Orders.pdf;/home/krawczuk/Zotero/storage/D9M28Z5W/2108.html}
}

@article{bond-taylorDeepGenerativeModelling2022,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  date = {2022-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {7327--7347},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3116668},
  url = {https://ieeexplore.ieee.org/abstract/document/9555209},
  urldate = {2024-02-22},
  abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Analytical models,autoregressive models,Computational modeling,Data models,Deep learning,energy-based models,generative adversarial networks,Generative adversarial networks,generative models,Neurons,normalizing flows,Predictive models,Training,variational autoencoders},
  file = {/home/krawczuk/Zotero/storage/TC44WUWB/Bond-Taylor et al. - 2022 - Deep Generative Modelling A Comparative Review of.pdf;/home/krawczuk/Zotero/storage/TZWGDWKL/9555209.html}
}

@inproceedings{bouritsasPartitionCodeLearning2021c,
  title = {Partition and {{Code}}: Learning How to Compress Graphs},
  shorttitle = {Partition and {{Code}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bouritsas, Giorgos and Loukas, Andreas and Karalias, Nikolaos and Bronstein, Michael},
  date = {2021},
  volume = {34},
  pages = {18603--18619},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/9a4d6e8685bd057e4f68930bd7c8ecc0-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Can we use machine learning to compress graph data? The absence of ordering in graphs poses a significant challenge to conventional compression algorithms, limiting their attainable gains as well as their ability to discover relevant patterns. On the other hand, most graph compression approaches rely on domain-dependent handcrafted representations and cannot adapt to different underlying graph distributions. This work aims to establish the necessary principles a lossless graph compression method should follow to approach the entropy storage lower bound. Instead of making rigid assumptions about the graph distribution, we formulate the compressor as a probabilistic model that can be learned from data and generalise to unseen instances. Our “Partition and Code” framework entails three steps: first, a partitioning algorithm decomposes the graph into subgraphs, then these are mapped to the elements of a small dictionary on which we learn a probability distribution, and finally,  an entropy encoder translates the representation into bits.  All the components (partitioning, dictionary and distribution) are parametric and can be trained with gradient descent. We theoretically compare the compression quality of several graph encodings and prove, under mild conditions, that PnC achieves compression gains that grow either linearly or quadratically with the number of vertices. Empirically, PnC yields significant compression improvements on diverse real-world networks.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YYA7JZUA/Bouritsas et al. - 2021 - Partition and Code learning how to compress graph.pdf}
}

@online{bousquetOptimalTransportGenerative2017a,
  title = {From Optimal Transport to Generative Modeling: The {{VEGAN}} Cookbook},
  shorttitle = {From Optimal Transport to Generative Modeling},
  author = {Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Simon-Gabriel, Carl-Johann and Schoelkopf, Bernhard},
  date = {2017-05-22},
  eprint = {1705.07642},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1705.07642},
  url = {http://arxiv.org/abs/1705.07642},
  urldate = {2024-02-20},
  abstract = {We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution \$P\_X\$ and the latent variable model distribution \$P\_G\$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from \$P\_X\$ and \$P\_G\$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/ZXVBVCJ2/Bousquet et al. - 2017 - From optimal transport to generative modeling the.pdf;/home/krawczuk/Zotero/storage/GDK9IQDU/1705.html}
}

@inproceedings{boybatMultiReRAMSynapsesArtificial2019c,
  title = {Multi-{{ReRAM}} Synapses for Artificial Neural Network Training},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Boybat, Irem and Giovinazzo, Cecilia and Shahrabi, Elmira and Krawczuk, Igor and Giannopoulos, Iason and Piveteau, Christophe and Le Gallo, Manuel and Ricciardi, Carlo and Sebastian, Abu and Eleftheriou, Evangelos},
  date = {2019},
  pages = {1--5},
  publisher = {{IEEE}},
  doi = {10.1109/ISCAS.2019.8702714},
  url = {https://ieeexplore.ieee.org/abstract/document/8702714/},
  urldate = {2024-02-27}
}

@book{boydConvexOptimization2004a,
  title = {Convex Optimization},
  author = {Boyd, Stephen P. and Vandenberghe, Lieven},
  date = {2004},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, UK ; New York}},
  isbn = {978-0-521-83378-3},
  langid = {english},
  pagetotal = {716},
  keywords = {Convex functions,Mathematical optimization},
  file = {/home/krawczuk/Zotero/storage/GC34P7KC/Boyd and Vandenberghe - 2004 - Convex optimization.pdf}
}

@online{brucknerSPQRTreeLikeEmbeddingRepresentation2019,
  title = {An {{SPQR-Tree-Like Embedding Representation}} for {{Upward Planarity}}},
  author = {Brückner, Guido and Himmel, Markus and Rutter, Ignaz},
  date = {2019-08-01},
  eprint = {1908.00352},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.00352},
  url = {http://arxiv.org/abs/1908.00352},
  urldate = {2024-02-20},
  abstract = {The SPQR-tree is a data structure that compactly represents all planar embeddings of a biconnected planar graph. It plays a key role in constrained planarity testing. We develop a similar data structure, called the UP-tree, that compactly represents all upward planar embeddings of a biconnected single-source directed graph. We demonstrate the usefulness of the UP-tree by solving the upward planar embedding extension problem for biconnected single-source directed graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/krawczuk/Zotero/storage/82ZLH3BK/Brückner et al. - 2019 - An SPQR-Tree-Like Embedding Representation for Upw.pdf;/home/krawczuk/Zotero/storage/6Z42IVTC/1908.html}
}

@online{brundageTrustworthyAIDevelopment2020c,
  title = {Toward {{Trustworthy AI Development}}: {{Mechanisms}} for {{Supporting Verifiable Claims}}},
  shorttitle = {Toward {{Trustworthy AI Development}}},
  author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Théo and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and family=Haas, given=Sarah, prefix=de, useprefix=true and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and {hÉigeartaigh}, Seán Ó and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and Herbert-Voss, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
  date = {2020-04-20},
  eprint = {2004.07213},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2004.07213},
  urldate = {2024-02-27},
  abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
  pubstate = {preprint},
  keywords = {Computer Science - Computers and Society},
  file = {/home/krawczuk/Zotero/storage/SFZ9MSZ4/Brundage et al. - 2020 - Toward Trustworthy AI Development Mechanisms for .pdf;/home/krawczuk/Zotero/storage/6TCPZG6U/2004.html}
}

@unpublished{brundageTrustworthyAIDevelopment2020d,
  title = {Toward Trustworthy {{AI}} Development: Mechanisms for Supporting Verifiable Claims (2020)},
  shorttitle = {Toward Trustworthy {{AI}} Development},
  author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth},
  date = {2020},
  eprint = {2004.07213},
  eprinttype = {arxiv},
  url = {https://scholar.google.com/scholar?cluster=11170515934228921249&hl=en&oi=scholarr},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found}
}

@inproceedings{budakJointOptimizationSizing2023,
  title = {Joint {{Optimization}} of {{Sizing}} and {{Layout}} for {{AMS Designs}}: {{Challenges}} and {{Opportunities}}},
  shorttitle = {Joint {{Optimization}} of {{Sizing}} and {{Layout}} for {{AMS Designs}}},
  booktitle = {Proceedings of the 2023 {{International Symposium}} on {{Physical Design}}},
  author = {Budak, Ahmet F. and Zhu, Keren and Chen, Hao and Poddar, Souradip and Zhao, Linran and Jia, Yaoyao and Pan, David Z.},
  date = {2023-03-26},
  series = {{{ISPD}} '23},
  pages = {84--92},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3569052.3578929},
  url = {https://dl.acm.org/doi/10.1145/3569052.3578929},
  urldate = {2024-02-26},
  abstract = {Recent advances in analog device sizing algorithms show promising results on the automatic schematic design. However, the majority of the sizing algorithms are based on schematic-level simulations and layout-agnostic. The physical layout implementation brings extra parasitics to the analog circuits, leading to discrepancies between schematic and post-layout performance. This performance gap raises questions about the effectiveness of automatic analog device sizing tools. Prior work has leveraged procedural layout generation to account for layout-induced parasitics in the sizing process. However, the need for layout templates makes such methodology limited in application. In this paper, we propose to bridge automatic analog sizing with post-layout performance using state-of-the-art optimization-based analog layout generators. A quantitative study is conducted to measure the impact of layout awareness in state-of-the-art device sizing algorithms. Furthermore, we present our perspectives on the future directions in layout-aware analog circuit schematic design.},
  isbn = {978-1-4503-9978-4},
  keywords = {analog circuit synthesis,analog layout automation,analog sizing automation,electronic design automation (eda),layout-aware sizing},
  file = {/home/krawczuk/Zotero/storage/7SMKBIL5/Budak et al. - 2023 - Joint Optimization of Sizing and Layout for AMS De.pdf}
}

@online{buesingLearningQueryingFast2018,
  title = {Learning and {{Querying Fast Generative Models}} for {{Reinforcement Learning}}},
  author = {Buesing, Lars and Weber, Theophane and Racaniere, Sebastien and Eslami, S. M. Ali and Rezende, Danilo and Reichert, David P. and Viola, Fabio and Besse, Frederic and Gregor, Karol and Hassabis, Demis and Wierstra, Daan},
  date = {2018-02-08},
  eprint = {1802.03006},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1802.03006},
  url = {http://arxiv.org/abs/1802.03006},
  urldate = {2024-02-26},
  abstract = {A key challenge in model-based reinforcement learning (RL) is to synthesize computationally efficient and accurate environment models. We show that carefully designed generative models that learn and operate on compact state representations, so-called state-space models, substantially reduce the computational costs for predicting outcomes of sequences of actions. Extensive experiments establish that state-space models accurately capture the dynamics of Atari games from the Arcade Learning Environment from raw pixels. The computational speed-up of state-space models while maintaining high accuracy makes their application in RL feasible: We demonstrate that agents which query these models for decision making outperform strong model-free baselines on the game MSPACMAN, demonstrating the potential of using learned environment models for planning.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/ELBZGHN7/Buesing et al. - 2018 - Learning and Querying Fast Generative Models for R.pdf;/home/krawczuk/Zotero/storage/BI4EJBEW/1802.html}
}

@online{caiLatentGraphDiffusion2024a,
  title = {Latent {{Graph Diffusion}}: {{A Unified Framework}} for {{Generation}} and {{Prediction}} on {{Graphs}}},
  shorttitle = {Latent {{Graph Diffusion}}},
  author = {Cai, Zhou and Wang, Xiyuan and Zhang, Muhan},
  date = {2024-02-04},
  eprint = {2402.02518},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.02518},
  url = {http://arxiv.org/abs/2402.02518},
  urldate = {2024-02-20},
  abstract = {In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across generation and regression tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/LHD27RHX/Cai et al. - 2024 - Latent Graph Diffusion A Unified Framework for Ge.pdf;/home/krawczuk/Zotero/storage/TEG7YCQA/2402.html}
}

@inproceedings{campilho-gomesAutomaticFlatLevelCircuit2020,
  title = {Automatic {{Flat-Level Circuit Generation}} with {{Genetic Algorithms}}},
  booktitle = {Technological {{Innovation}} for {{Life Improvement}}},
  author = {Campilho-Gomes, Miguel and Tavares, Rui and Goes, João},
  editor = {Camarinha-Matos, Luis M. and Farhadi, Nastaran and Lopes, Fábio and Pereira, Helena},
  date = {2020},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  pages = {101--108},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-45124-0_9},
  abstract = {This paper describes a novel methodology to generate analog and digital circuits, autonomously, using the transistor (or other elementary device, e.g. resistor) as the basic elementary block – flat-level. A genetic algorithm is employed as the generation engine and variable length chromosomes are used to describe the circuit topology that evolves during the search. The circuit devices type and sizing are described by each gene of genetic algorithm. The automatic process starts with the circuit input and output specifications, and proceeds with the circuit topology and sizing evolution to meet those specifications, eventually, ending up with a novel topology. During the evolution, each generated circuit is electrically evaluated by a spice-like circuit simulator, i.e. Ngspice, using full model specifications - like BSIM3 for transistors - in a highly parallelized architecture built over a multi-thread model.},
  isbn = {978-3-030-45124-0},
  langid = {english},
  keywords = {Amplifier,Analog circuit,Automatic topology generation,Digital circuit,Genetic algorithm,Ngspice,Variable Length Chromosome},
  file = {/home/krawczuk/Zotero/storage/ED65I72T/Campilho-Gomes et al. - 2020 - Automatic Flat-Level Circuit Generation with Genet.pdf}
}

@article{chaiCircuitNetOpenSourceDataset2023b,
  title = {{{CircuitNet}}: {{An Open-Source Dataset}} for {{Machine Learning}} in {{VLSI CAD Applications With Improved Domain-Specific Evaluation Metric}} and {{Learning Strategies}}},
  shorttitle = {{{CircuitNet}}},
  author = {Chai, Zhuomin and Zhao, Yuxiang and Liu, Wei and Lin, Yibo and Wang, Runsheng and Huang, Ru},
  date = {2023-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {12},
  pages = {5034--5047},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2023.3287970},
  url = {https://ieeexplore.ieee.org/abstract/document/10158384?casa_token=5qpmtJtoYJIAAAAA:aDCl7xJR457l-Mw8opQAYhCed-lywODhgas0xX4ylhB1eY68GUD4QWzSWu5jZUkmrfVyEnbBzLSf},
  urldate = {2024-02-25},
  abstract = {The design automation community has been actively exploring machine learning (ML) for very-large-scale-integrated (VLSI) computer-aided design (CAD). Many studies have explored learning-based techniques for cross-stage prediction tasks in the design flow. Although building ML models usually requires a large amount of data, most studies can only generate small internal datasets for validation due to the lack of large public datasets. Such a situation challenges the research in this field and raises potential issues like difficulty in benchmarking and reproducing results, limited research scope on small internal datasets, and high bar for new researchers. Therefore, in this article, we present an open-source dataset called “CircuitNet” for ML tasks in VLSI CAD. The dataset consists of more than 10K samples extracted from versatile runs of commercial design tools based on six open-source RISC-V designs which support typical cross-stage prediction tasks, such as routability and IR drop prediction, with extensive benchmarking on recent models. With the dataset prepared, we identify two practical challenges, data imbalance and model transferability, for ML application in CAD. To overcome data imbalance, we propose a loss function, biased loss, to give more weight to the minority, leading to 2\% congestion reduction in routability-driven placement. We test the model transferability from RISC-V designs to ISPD 2015 contest designs in congestion prediction with several transfer learning methods and further proposed a knowledge distillation-based transfer learning framework with up to 20\% accuracy improvement. We believe this dataset can open up new opportunities for ML in CAD research and beyond.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Benchmark testing,integrated circuit modeling,Integrated circuit modeling,Machine learning,physical design,predictive models,Predictive models,Routing,Solid modeling,Task analysis,training,Very large scale integration},
  file = {/home/krawczuk/Zotero/storage/4GTHF8WV/Chai et al. - 2023 - CircuitNet An Open-Source Dataset for Machine Lea.pdf;/home/krawczuk/Zotero/storage/HMEVJTZ2/10158384.html}
}

@inproceedings{chenAutoCRAFTLayoutAutomation2022,
  title = {{{AutoCRAFT}}: {{Layout Automation}} for {{Custom Circuits}} in {{Advanced FinFET Technologies}}},
  shorttitle = {{{AutoCRAFT}}},
  booktitle = {Proceedings of the 2022 {{International Symposium}} on {{Physical Design}}},
  author = {Chen, Hao and Turner, Walker J. and Song, Sanquan and Zhu, Keren and Kokai, George F. and Zimmer, Brian and Gray, C. Thomas and Khailany, Brucek and Pan, David Z. and Ren, Haoxing},
  date = {2022-04-13},
  series = {{{ISPD}} '22},
  pages = {175--183},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3505170.3511044},
  url = {https://dl.acm.org/doi/10.1145/3505170.3511044},
  urldate = {2024-02-19},
  abstract = {Despite continuous efforts in layout automation for full-custom circuits, including analog/mixed-signal (AMS) designs, automated layout tools have not yet been widely adopted in current industrial full-custom design flows due to the high circuit complexity and sensitivity to layout parasitics. Nevertheless, the strict design rules and grid-based restrictions in nanometer-scale FinFET nodes limit the degree of freedom in full-custom layout design and thus reduce the gap between automation tools and human experts. This paper presents AutoCRAFT, an automatic layout generator targeting region-based layouts for advanced FinFET-based full-custom circuits. AutoCRAFT uses specialized place-and-route (P\&R) algorithms to handle various design constraints while adhering to typical FinFET layout styles. Verified by comprehensive post-layout analyses, AutoCRAFT has achieved promising preliminary results in generating sign-off quality layouts for industrial benchmarks.},
  isbn = {978-1-4503-9210-5},
  keywords = {analog/mixed-signal,finfet,full-custom layout,physical design},
  file = {/home/krawczuk/Zotero/storage/ZC4UHS3J/Chen et al. - 2022 - AutoCRAFT Layout Automation for Custom Circuits i.pdf}
}

@inproceedings{chenAutoCRAFTLayoutAutomation2022a,
  title = {{{AutoCRAFT}}: {{Layout Automation}} for {{Custom Circuits}} in {{Advanced FinFET Technologies}}},
  shorttitle = {{{AutoCRAFT}}},
  booktitle = {Proceedings of the 2022 {{International Symposium}} on {{Physical Design}}},
  author = {Chen, Hao and Turner, Walker J. and Song, Sanquan and Zhu, Keren and Kokai, George F. and Zimmer, Brian and Gray, C. Thomas and Khailany, Brucek and Pan, David Z. and Ren, Haoxing},
  date = {2022-04-13},
  series = {{{ISPD}} '22},
  pages = {175--183},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3505170.3511044},
  url = {https://dl.acm.org/doi/10.1145/3505170.3511044},
  urldate = {2024-02-26},
  abstract = {Despite continuous efforts in layout automation for full-custom circuits, including analog/mixed-signal (AMS) designs, automated layout tools have not yet been widely adopted in current industrial full-custom design flows due to the high circuit complexity and sensitivity to layout parasitics. Nevertheless, the strict design rules and grid-based restrictions in nanometer-scale FinFET nodes limit the degree of freedom in full-custom layout design and thus reduce the gap between automation tools and human experts. This paper presents AutoCRAFT, an automatic layout generator targeting region-based layouts for advanced FinFET-based full-custom circuits. AutoCRAFT uses specialized place-and-route (P\&R) algorithms to handle various design constraints while adhering to typical FinFET layout styles. Verified by comprehensive post-layout analyses, AutoCRAFT has achieved promising preliminary results in generating sign-off quality layouts for industrial benchmarks.},
  isbn = {978-1-4503-9210-5},
  keywords = {analog/mixed-signal,finfet,full-custom layout,physical design},
  file = {/home/krawczuk/Zotero/storage/JUUVSSC7/Chen et al. - 2022 - AutoCRAFT Layout Automation for Custom Circuits i.pdf}
}

@article{chenChallengesOpportunitiesFully2020d,
  title = {Challenges and Opportunities toward Fully Automated Analog Layout Design},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Sun, Nan and Pan, David Z.},
  date = {2020-11},
  journaltitle = {Journal of Semiconductors},
  shortjournal = {J. Semicond.},
  volume = {41},
  number = {11},
  pages = {111407},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1674-4926},
  doi = {10.1088/1674-4926/41/11/111407},
  url = {https://dx.doi.org/10.1088/1674-4926/41/11/111407},
  urldate = {2024-02-20},
  abstract = {Realizing the layouts of analog/mixed-signal (AMS) integrated circuits (ICs) is a complicated task due to the high design flexibility and sensitive circuit performance. Compared with the advancements of digital IC layout automation, analog IC layout design is still heavily manual, which leads to a more time-consuming and error-prone process. In recent years, significant progress has been made in automated analog layout design with emerging of several open-source frameworks. This paper firstly reviews the existing state-of-the art AMS layout synthesis frameworks with focus on the different approaches and their individual challenges. We then present recent research trends and opportunities in the field. Finally, we summaries the paper with open questions and future directions for fully-automating the analog IC layout.},
  langid = {english}
}

@article{chenChallengesOpportunitiesFully2020e,
  title = {Challenges and Opportunities toward Fully Automated Analog Layout Design},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Sun, Nan and Pan, David Z.},
  date = {2020-11},
  journaltitle = {Journal of Semiconductors},
  shortjournal = {J. Semicond.},
  volume = {41},
  number = {11},
  pages = {111407},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1674-4926},
  doi = {10.1088/1674-4926/41/11/111407},
  url = {https://dx.doi.org/10.1088/1674-4926/41/11/111407},
  urldate = {2024-02-20},
  abstract = {Realizing the layouts of analog/mixed-signal (AMS) integrated circuits (ICs) is a complicated task due to the high design flexibility and sensitive circuit performance. Compared with the advancements of digital IC layout automation, analog IC layout design is still heavily manual, which leads to a more time-consuming and error-prone process. In recent years, significant progress has been made in automated analog layout design with emerging of several open-source frameworks. This paper firstly reviews the existing state-of-the art AMS layout synthesis frameworks with focus on the different approaches and their individual challenges. We then present recent research trends and opportunities in the field. Finally, we summaries the paper with open questions and future directions for fully-automating the analog IC layout.},
  langid = {english}
}

@article{chenChallengesOpportunitiesFully2020f,
  title = {Challenges and Opportunities toward Fully Automated Analog Layout Design},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Sun, Nan and Pan, David Z.},
  date = {2020-11-01},
  journaltitle = {Journal of Semiconductors},
  shortjournal = {J. Semicond.},
  volume = {41},
  number = {11},
  pages = {111407},
  issn = {1674-4926, 2058-6140},
  doi = {10.1088/1674-4926/41/11/111407},
  url = {https://iopscience.iop.org/article/10.1088/1674-4926/41/11/111407},
  urldate = {2024-02-20},
  abstract = {Realizing the layouts of analog/mixed-signal (AMS) integrated circuits (ICs) is a complicated task due to the high design flexibility and sensitive circuit performance. Compared with the advancements of digital IC layout automation, analog IC layout design is still heavily manual, which leads to a more time-consuming and error-prone process. In recent years, significant progress has been made in automated analog layout design with emerging of several open-source frameworks. This paper firstly reviews the existing state-of-the art AMS layout synthesis frameworks with focus on the different approaches and their individual challenges. We then present recent research trends and opportunities in the field. Finally, we summaries the paper with open questions and future directions for fully-automating the analog IC layout.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/U2SIPBX4/Chen et al. - 2020 - Challenges and opportunities toward fully automate.pdf}
}

@article{chenComponentsConvertersFundamental2024,
  title = {From {{Components}} to {{Converters}}: {{A Fundamental Topology Derivation Method}} for {{Nonresonant DC}}–{{DC Converters Based}} on {{Graph Theory}}},
  shorttitle = {From {{Components}} to {{Converters}}},
  author = {Chen, Guipeng and Mo, Liping and Jiang, Chaoqiang and Qing, Xinlin},
  date = {2024-01},
  journaltitle = {IEEE Transactions on Power Electronics},
  volume = {39},
  number = {1},
  pages = {1028--1045},
  issn = {1941-0107},
  doi = {10.1109/TPEL.2023.3323597},
  url = {https://ieeexplore.ieee.org/abstract/document/10278481?casa_token=g7L2SJVcFBgAAAAA:GY1iv85vEejRHF_TB3RX-m9b38k1-ND2ZPOubf1iLvWlFzr7gfTM66Oe_qXEWhRtvNnyPoeCV4XU},
  urldate = {2024-02-19},
  abstract = {The past 20 years witnessed the invention of numerous converters by utilizing various topology derivation methods. Unfortunately, most of these methods are limited by pre-existing topologies or specific cells, causing the omission of some potentially valuable topologies. To break the limitations, a fundamental topology derivation method, namely components to converters (C2C), is proposed for nonresonant dc–dc converters. The basic idea of C2C is intuitively to derive topologies by combining separate components and filtering out valid combinations. Theoretically, C2C can derive converters more comprehensively since its results are not restricted by firm connections of the existing topologies or cells. However, C2C faces a heavy computing load caused by the massive combinations of components. Hence, a two-stage C2C topology derivation strategy is designed to alleviate the computing load. Furthermore, graph theory and dynamic programming are applied to computerize and optimize the above two-stage C2C. The two-stage C2C is utilized to derive single-switch two-port converters and single-inductor multiple-port converters. The derivation results show that all existing topologies with given components and numerous new topologies are derived automatically and simultaneously. Compared with the existing topology derivation methods, the proposed two-stage C2C is more thorough and automatic, facilitating more converters to meet various demands in practical applications.},
  eventtitle = {{{IEEE Transactions}} on {{Power Electronics}}},
  keywords = {Dynamic programming,Graph theory,Heuristic algorithms,Network topology,nonresonant dc–dc converter,Switches,Topology,topology derivation,Voltage},
  file = {/home/krawczuk/Zotero/storage/TFL7P93H/Chen et al. - 2024 - From Components to Converters A Fundamental Topol.pdf;/home/krawczuk/Zotero/storage/EUZ4WZL3/10278481.html}
}

@article{chenCompressGraphEfficientParallel2023,
  title = {{{CompressGraph}}: {{Efficient Parallel Graph Analytics}} with {{Rule-Based Compression}}},
  shorttitle = {{{CompressGraph}}},
  author = {Chen, Zheng and Zhang, Feng and Guan, JiaWei and Zhai, Jidong and Shen, Xipeng and Zhang, Huanchen and Shu, Wentong and Du, Xiaoyong},
  date = {2023-05-30},
  journaltitle = {Proceedings of the ACM on Management of Data},
  shortjournal = {Proc. ACM Manag. Data},
  volume = {1},
  number = {1},
  pages = {4:1--4:31},
  doi = {10.1145/3588684},
  url = {https://dl.acm.org/doi/10.1145/3588684},
  urldate = {2024-02-26},
  abstract = {Modern graphs exert colossal time and space pressure on graph analytics applications. In 2022, Facebook social graph reaches 2.91 billion users with trillions of edges. Many compression algorithms have been developed to support direct processing on compressed graphs to address this challenge. However, previous graph compression algorithms do not focus on leveraging redundancy in repeated neighbor sequences, so they do not save the amount of computation for graph analytics. We develop CompressGraph, an efficient rule-based graph analytics engine that leverages data redundancy in graphs to achieve both performance boost and space reduction for common graph applications. CompressGraph has three advantages over previous works. First, the rule-based abstraction of CompressGraph supports the reuse of intermediate results during graph traversal, thus saving time. Second, CompressGraph has intense expressiveness to support a wide range of graph applications. Third, CompressGraph scales well under high parallelism because the context-free rules have few dependencies. Experiments show that CompressGraph provides significant performance and space benefits on both CPUs and GPUs. On evaluating six typical graph applications, CompressGraph can achieve 1.97× speedup on the CPU, while 3.95× speedup on the GPU, compared to the state-of-the-art CPU and GPU methods, respectively. Moreover, CompressGraph can save an average of 71.27\% memory savings on CPU and 70.36 on GPU.},
  keywords = {compressed data direct processing,compression,graph analytic},
  file = {/home/krawczuk/Zotero/storage/LVRM7D4D/Chen et al. - 2023 - CompressGraph Efficient Parallel Graph Analytics .pdf}
}

@online{chenEfficientDegreeGuidedGraph2023g,
  title = {Efficient and {{Degree-Guided Graph Generation}} via {{Discrete Diffusion Modeling}}},
  author = {Chen, Xiaohui and He, Jiaxing and Han, Xu and Liu, Li-Ping},
  date = {2023-05-31},
  eprint = {2305.04111},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.04111},
  url = {http://arxiv.org/abs/2305.04111},
  urldate = {2024-02-25},
  abstract = {Diffusion-based generative graph models have been proven effective in generating high-quality small graphs. However, they need to be more scalable for generating large graphs containing thousands of nodes desiring graph statistics. In this work, we propose EDGE, a new diffusion-based generative graph model that addresses generative tasks with large graphs. To improve computation efficiency, we encourage graph sparsity by using a discrete diffusion process that randomly removes edges at each time step and finally obtains an empty graph. EDGE only focuses on a portion of nodes in the graph at each denoising step. It makes much fewer edge predictions than previous diffusion-based models. Moreover, EDGE admits explicitly modeling the node degrees of the graphs, further improving the model performance. The empirical study shows that EDGE is much more efficient than competing methods and can generate large graphs with thousands of nodes. It also outperforms baseline models in generation quality: graphs generated by our approach have more similar graph statistics to those of the training graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/LR6WTKK6/Chen et al. - 2023 - Efficient and Degree-Guided Graph Generation via D.pdf;/home/krawczuk/Zotero/storage/2P7YFNKB/2305.html}
}

@inproceedings{chenEquivalenceNeuralNetwork2021,
  title = {On the {{Equivalence}} between {{Neural Network}} and {{Support Vector Machine}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Yilan and Huang, Wei and Nguyen, Lam and Weng, Tsui-Wei},
  date = {2021},
  volume = {34},
  pages = {23478--23490},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/c559da2ba967eb820766939a658022c8-Abstract.html},
  urldate = {2024-02-26},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/PJYXAK9X/Chen et al. - 2021 - On the Equivalence between Neural Network and Supp.pdf}
}

@inproceedings{chenGeneratingCellLibrary2023,
  title = {On {{Generating Cell Library}} in {{Advanced Nodes}}: {{Efforts}} and {{Challenges}}},
  shorttitle = {On {{Generating Cell Library}} in {{Advanced Nodes}}},
  booktitle = {2023 {{International VLSI Symposium}} on {{Technology}}, {{Systems}} and {{Applications}} ({{VLSI-TSA}}/{{VLSI-DAT}})},
  author = {Chen, Hung-Ming and Hsiao, Cheng-Li and Chao, Wei-Tung and Hsieh, I-Chun},
  date = {2023-04-17},
  pages = {1--4},
  publisher = {{IEEE}},
  location = {{HsinChu, Taiwan}},
  doi = {10.1109/VLSI-TSA/VLSI-DAT57221.2023.10134126},
  url = {https://ieeexplore.ieee.org/document/10134126/},
  urldate = {2024-02-22},
  eventtitle = {2023 {{International VLSI Symposium}} on {{Technology}}, {{Systems}} and {{Applications}} ({{VLSI-TSA}}/{{VLSI-DAT}})},
  isbn = {9798350334166},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/YHY9SW5F/Chen et al. - 2023 - On Generating Cell Library in Advanced Nodes Effo.pdf}
}

@article{chenMachineLearningAdvanced2022,
  title = {Machine Learning in Advanced {{IC}} Design: {{A}} Methodological Survey},
  shorttitle = {Machine Learning in Advanced {{IC}} Design},
  author = {Chen, Tinghuan and Zhang, Grace Li and Yu, Bei and Li, Bing and Schlichtmann, Ulf},
  date = {2022},
  journaltitle = {IEEE Design \& Test},
  volume = {40},
  number = {1},
  pages = {17--33},
  publisher = {{IEEE}},
  doi = {10.1109/MDAT.2022.3216799},
  url = {https://ieeexplore.ieee.org/document/9927393/?casa_token=T9748pCZBRcAAAAA:GeXAatfUFxNH7kYG1jlXmgiTv_ivV3EukpRBXgMMdlsKVjxBNs42wtCSqLdruMqOK-u8FrB4KuIm},
  urldate = {2024-02-19}
}

@inproceedings{chenMAGICALOpenSourceFullyAutomated2021,
  title = {{{MAGICAL}} 1.0: {{An Open-Source Fully-Automated AMS Layout Synthesis Framework Verified With}} a 40-Nm {{1GS}}/s {{Δ}}∑ {{ADC}}},
  shorttitle = {{{MAGICAL}} 1.0},
  booktitle = {2021 {{IEEE Custom Integrated Circuits Conference}} ({{CICC}})},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Mukherjee, Abhishek and Sun, Nan and Pan, David Z.},
  date = {2021-04},
  pages = {1--2},
  issn = {2152-3630},
  doi = {10.1109/CICC51472.2021.9431521},
  url = {https://ieeexplore.ieee.org/abstract/document/9431521?casa_token=NCKWIPHtGY8AAAAA:cuJCl844fAGGBRpIb3TIkRD_5aMlh1c4yCyg3krCnDnOu08jgWUAOWdmyj8Q4L_FD10z4XU_dMCi},
  urldate = {2024-02-26},
  abstract = {The layout process is incredibly tedious as it is being exacerbated by technology scaling, where design rules become increasingly complicated in advanced nodes. While digital blocks' layout can be easily synthesized, analog/mixed-signal (AMS) layout designs are still heavily manual, setting bottlenecks for time-to-market. Though endeavored in research for decades, the automation of AMS circuit layouts has not been as successful as its digital counterpart. The reason is rooted in the sensitiveness and complexity of AMS circuits layouts [1]. Researchers have proposed specialized digital-like AMS circuit architectures [2],[3], which are robust against layout mismatches. Those circuit architectures can be synthesized using commercial digital place-and-route (P\&R) tools but fail to produce a general-purpose solution to other circuit architectures. On the other hand, procedure-based methodologies have been proposed to synthesize AMS circuits based on parameterized layout templates [4]. Procedure-based layout generators provide a standardized flow to migrate circuit layouts to different circuit sizings and manufacturing technologies. Still, they require a significant amount of manual effort to program the layout templates. While there are attempts to apply P\&R algorithms on AMS layouts [1], to the best of the authors' knowledge, no prior work has demonstrated a silicon-proven fully automated layout of a real-world mixed-signal system.},
  eventtitle = {2021 {{IEEE Custom Integrated Circuits Conference}} ({{CICC}})},
  keywords = {Application specific integrated circuits,Automation,Conferences,Generators,Layout,Manuals,Tools},
  file = {/home/krawczuk/Zotero/storage/FCE8M88I/Chen et al. - 2021 - MAGICAL 1.0 An Open-Source Fully-Automated AMS La.pdf;/home/krawczuk/Zotero/storage/FABAISXZ/9431521.html}
}

@inproceedings{chenMAGICALOpenSourceFullyAutomated2021a,
  title = {{{MAGICAL}} 1.0: {{An Open-Source Fully-Automated AMS Layout Synthesis Framework Verified With}} a 40-Nm {{1GS}}/s {{Δ}}∑ {{ADC}}},
  shorttitle = {{{MAGICAL}} 1.0},
  booktitle = {2021 {{IEEE Custom Integrated Circuits Conference}} ({{CICC}})},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Mukherjee, Abhishek and Sun, Nan and Pan, David Z.},
  date = {2021-04},
  pages = {1--2},
  issn = {2152-3630},
  doi = {10.1109/CICC51472.2021.9431521},
  url = {https://ieeexplore.ieee.org/abstract/document/9431521?casa_token=NCKWIPHtGY8AAAAA:cuJCl844fAGGBRpIb3TIkRD_5aMlh1c4yCyg3krCnDnOu08jgWUAOWdmyj8Q4L_FD10z4XU_dMCi},
  urldate = {2024-02-26},
  abstract = {The layout process is incredibly tedious as it is being exacerbated by technology scaling, where design rules become increasingly complicated in advanced nodes. While digital blocks' layout can be easily synthesized, analog/mixed-signal (AMS) layout designs are still heavily manual, setting bottlenecks for time-to-market. Though endeavored in research for decades, the automation of AMS circuit layouts has not been as successful as its digital counterpart. The reason is rooted in the sensitiveness and complexity of AMS circuits layouts [1]. Researchers have proposed specialized digital-like AMS circuit architectures [2],[3], which are robust against layout mismatches. Those circuit architectures can be synthesized using commercial digital place-and-route (P\&R) tools but fail to produce a general-purpose solution to other circuit architectures. On the other hand, procedure-based methodologies have been proposed to synthesize AMS circuits based on parameterized layout templates [4]. Procedure-based layout generators provide a standardized flow to migrate circuit layouts to different circuit sizings and manufacturing technologies. Still, they require a significant amount of manual effort to program the layout templates. While there are attempts to apply P\&R algorithms on AMS layouts [1], to the best of the authors' knowledge, no prior work has demonstrated a silicon-proven fully automated layout of a real-world mixed-signal system.},
  eventtitle = {2021 {{IEEE Custom Integrated Circuits Conference}} ({{CICC}})},
  keywords = {Application specific integrated circuits,Automation,Conferences,Generators,Layout,Manuals,Tools},
  file = {/home/krawczuk/Zotero/storage/IYI6UIME/Chen et al. - 2021 - MAGICAL 1.0 An Open-Source Fully-Automated AMS La.pdf;/home/krawczuk/Zotero/storage/D4VQTSBP/9431521.html}
}

@article{chenMultiLayerNeuralNetworks2023,
  title = {Multi-{{Layer Neural Networks}} as {{Trainable Ladders}} of {{Hilbert Spaces}}},
  author = {Chen, Zhengdao},
  date = {2023-06-15},
  url = {https://openreview.net/forum?id=ZMvv6laV5b},
  urldate = {2024-02-26},
  abstract = {To characterize the functions spaces explored by multi-layer neural networks (NNs), we introduce Neural Hilbert Ladders (NHLs), a collection of reproducing kernel Hilbert spaces (RKHSes) that are defined iteratively and adaptive to training. First, we prove a correspondence between functions expressed by L-layer NNs and those belonging to L-level NHLs. Second, we prove generalization guarantees for learning the NHL based on a new complexity measure. Third, corresponding to the training of multi-layer NNs in the infinite-width mean-field limit, we derive an evolution of the NHL characterized by the dynamics of multiple random fields. Finally, we examine linear and shallow NNs from the new perspective and complement the theory with numerical results.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/XDR9FBRZ/Chen - 2023 - Multi-Layer Neural Networks as Trainable Ladders o.pdf}
}

@online{chenOrderMattersProbabilistic2021c,
  title = {Order {{Matters}}: {{Probabilistic Modeling}} of {{Node Sequence}} for {{Graph Generation}}},
  shorttitle = {Order {{Matters}}},
  author = {Chen, Xiaohui and Han, Xu and Hu, Jiajing and Ruiz, Francisco J. R. and Liu, Liping},
  date = {2021-06-14},
  eprint = {2106.06189},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2106.06189},
  url = {http://arxiv.org/abs/2106.06189},
  urldate = {2024-02-25},
  abstract = {A graph generative model defines a distribution over graphs. One type of generative model is constructed by autoregressive neural networks, which sequentially add nodes and edges to generate a graph. However, the likelihood of a graph under the autoregressive model is intractable, as there are numerous sequences leading to the given graph; this makes maximum likelihood estimation challenging. Instead, in this work we derive the exact joint probability over the graph and the node ordering of the sequential process. From the joint, we approximately marginalize out the node orderings and compute a lower bound on the log-likelihood using variational inference. We train graph generative models by maximizing this bound, without using the ad-hoc node orderings of previous methods. Our experiments show that the log-likelihood bound is significantly tighter than the bound of previous schemes. Moreover, the models fitted with the proposed algorithm can generate high-quality graphs that match the structures of target graphs not seen during training. We have made our code publicly available at \textbackslash hyperref[https://github.com/tufts-ml/graph-generation-vi]\{https://github.com/tufts-ml/graph-generation-vi\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/EZJNGLAA/Chen et al. - 2021 - Order Matters Probabilistic Modeling of Node Sequ.pdf;/home/krawczuk/Zotero/storage/9M3ZGBQ5/2106.html}
}

@online{chenSimpleHierarchicalPlanning2024,
  title = {Simple {{Hierarchical Planning}} with {{Diffusion}}},
  author = {Chen, Chang and Deng, Fei and Kawaguchi, Kenji and Gulcehre, Caglar and Ahn, Sungjin},
  date = {2024-01-05},
  eprint = {2401.02644},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.02644},
  url = {http://arxiv.org/abs/2401.02644},
  urldate = {2024-02-26},
  abstract = {Diffusion-based generative methods have proven effective in modeling trajectories with offline datasets. However, they often face computational challenges and can falter in generalization, especially in capturing temporal abstractions for long-horizon tasks. To overcome this, we introduce the Hierarchical Diffuser, a simple, fast, yet surprisingly effective planning method combining the advantages of hierarchical and diffusion-based planning. Our model adopts a "jumpy" planning strategy at the higher level, which allows it to have a larger receptive field but at a lower computational cost -- a crucial factor for diffusion-based planning methods, as we have empirically verified. Additionally, the jumpy sub-goals guide our low-level planner, facilitating a fine-tuning stage and further improving our approach's effectiveness. We conducted empirical evaluations on standard offline reinforcement learning benchmarks, demonstrating our method's superior performance and efficiency in terms of training and planning speed compared to the non-hierarchical Diffuser as well as other hierarchical planning methods. Moreover, we explore our model's generalization capability, particularly on how our method improves generalization capabilities on compositional out-of-distribution tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/2HTGKEKE/Chen et al. - 2024 - Simple Hierarchical Planning with Diffusion.pdf;/home/krawczuk/Zotero/storage/SNJK6GRG/2401.html}
}

@inproceedings{chenTopologicalRelationalLearning2021,
  title = {Topological {{Relational Learning}} on {{Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Yuzhou and Coskunuzer, Baris and Gel, Yulia},
  date = {2021},
  volume = {34},
  pages = {27029--27042},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/e334fd9dac68f13fa1a57796148cf812-Abstract.html},
  urldate = {2024-02-20},
  abstract = {Graph neural networks (GNNs) have emerged as a powerful tool for graph classification and representation learning. However, GNNs tend to suffer from over-smoothing problems and are vulnerable to graph perturbations. To address these challenges, we propose a novel topological neural framework of topological relational inference (TRI) which allows for integrating higher-order graph information to GNNs and for systematically learning a local graph structure. The key idea is to rewire the original graph by using the persistent homology of the small neighborhoods of the nodes and then to incorporate the extracted topological summaries as the side information into the local algorithm. As a result, the new framework enables us to harness both the conventional information on the graph structure and information on higher order topological properties of the graph. We derive theoretical properties on stability of the new local topological representation of the graph and discuss its implications on the graph algebraic connectivity. The experimental results on node classification tasks demonstrate that the new TRI-GNN outperforms all 14 state-of-the-art baselines on 6 out 7 graphs and exhibit higher robustness to perturbations, yielding up to 10\textbackslash\% better performance under noisy scenarios.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/S7IN62SA/Chen et al. - 2021 - Topological Relational Learning on Graphs.pdf}
}

@inproceedings{chenTOTALTopologyOptimization2023,
  title = {{{TOTAL}}: {{Topology Optimization}} of {{Operational Amplifier}} via {{Reinforcement Learning}}},
  shorttitle = {{{TOTAL}}},
  booktitle = {2023 24th {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  author = {Chen, Zihao and Meng, Songlei and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2023},
  pages = {1--8},
  publisher = {{IEEE}},
  url = {https://ieeexplore.ieee.org/abstract/document/10129336/?casa_token=jWG_R5SWFfYAAAAA:IkVSx9R6WervPNZLg8YaLlvx4ZlZ7P1W5Xn20044DWb7EfxxaNU-BbI3MoVFl3SCurN1zezZPzjN},
  urldate = {2024-02-19},
  keywords = {❓ Multiple DOI}
}

@online{chienYouAreAllSet2022c,
  title = {You Are {{AllSet}}: {{A Multiset Function Framework}} for {{Hypergraph Neural Networks}}},
  shorttitle = {You Are {{AllSet}}},
  author = {Chien, Eli and Pan, Chao and Peng, Jianhao and Milenkovic, Olgica},
  date = {2022-03-28},
  eprint = {2106.13264},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.13264},
  url = {http://arxiv.org/abs/2106.13264},
  urldate = {2024-02-26},
  abstract = {Hypergraphs are used to model higher-order interactions amongst agents and there exist many practically relevant instances of hypergraph datasets. To enable efficient processing of hypergraph-structured data, several hypergraph neural network platforms have been proposed for learning hypergraph properties and structure, with a special focus on node classification. However, almost all existing methods use heuristic propagation rules and offer suboptimal performance on many datasets. We propose AllSet, a new hypergraph neural network paradigm that represents a highly general framework for (hyper)graph neural networks and for the first time implements hypergraph neural network layers as compositions of two multiset functions that can be efficiently learned for each task and each dataset. Furthermore, AllSet draws on new connections between hypergraph neural networks and recent advances in deep learning of multiset functions. In particular, the proposed architecture utilizes Deep Sets and Set Transformer architectures that allow for significant modeling flexibility and offer high expressive power. To evaluate the performance of AllSet, we conduct the most extensive experiments to date involving ten known benchmarking datasets and three newly curated datasets that represent significant challenges for hypergraph node classification. The results demonstrate that AllSet has the unique ability to consistently either match or outperform all other hypergraph neural networks across the tested datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/QKJZIGH9/Chien et al. - 2022 - You are AllSet A Multiset Function Framework for .pdf;/home/krawczuk/Zotero/storage/4S69XRDW/2106.html}
}

@article{choiMetropolisHastingsReversiblizations2020a,
  title = {Metropolis–{{Hastings}} Reversiblizations of Non-Reversible {{Markov}} Chains},
  author = {Choi, Michael C. H.},
  date = {2020-02-01},
  journaltitle = {Stochastic Processes and their Applications},
  shortjournal = {Stochastic Processes and their Applications},
  volume = {130},
  number = {2},
  pages = {1041--1073},
  issn = {0304-4149},
  doi = {10.1016/j.spa.2019.04.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0304414919302327},
  urldate = {2024-02-25},
  abstract = {We study two types of Metropolis–Hastings (MH) reversiblizations for non-reversible Markov chains with Markov kernel P. While the first type is the classical Metropolised version of P, we introduce a new self-adjoint kernel which captures the opposite transition effect of the first type, that we call the second MH kernel. We investigate the spectral relationship between P and the two MH kernels. Along the way, we state a version of Weyl’s inequality for the spectral gap of P (and hence its additive reversiblization), as well as an expansion of P. Both results are expressed in terms of the spectrum of the two MH kernels. In the spirit of Fill (1991) and Paulin (2015), we define a new pseudo-spectral gap based on the two MH kernels, and show that the total variation distance from stationarity can be bounded by this gap. We give variance bounds of the Markov chain in terms of the proposed gap, and offer spectral bounds in metastability and Cheeger’s inequality in terms of the two MH kernels by comparison of Dirichlet form and Peskun ordering.},
  keywords = {Metropolis–Hastings algorithm,Mixing time,Non-reversible Markov chain,Spectral gap,Variance bounds,Weyl’s inequality},
  file = {/home/krawczuk/Zotero/storage/74XECBHP/Choi - 2020 - Metropolis–Hastings reversiblizations of non-rever.pdf;/home/krawczuk/Zotero/storage/CJ8CLAKT/S0304414919302327.html}
}

@online{chowdhuryOpenABCDLargeScaleDataset2021e,
  title = {{{OpenABC-D}}: {{A Large-Scale Dataset For Machine Learning Guided Integrated Circuit Synthesis}}},
  shorttitle = {{{OpenABC-D}}},
  author = {Chowdhury, Animesh Basak and Tan, Benjamin and Karri, Ramesh and Garg, Siddharth},
  date = {2021-10-21},
  eprint = {2110.11292},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2110.11292},
  url = {http://arxiv.org/abs/2110.11292},
  urldate = {2024-02-25},
  abstract = {Logic synthesis is a challenging and widely-researched combinatorial optimization problem during integrated circuit (IC) design. It transforms a high-level description of hardware in a programming language like Verilog into an optimized digital circuit netlist, a network of interconnected Boolean logic gates, that implements the function. Spurred by the success of ML in solving combinatorial and graph problems in other domains, there is growing interest in the design of ML-guided logic synthesis tools. Yet, there are no standard datasets or prototypical learning tasks defined for this problem domain. Here, we describe OpenABC-D,a large-scale, labeled dataset produced by synthesizing open source designs with a leading open-source logic synthesis tool and illustrate its use in developing, evaluating and benchmarking ML-guided logic synthesis. OpenABC-D has intermediate and final outputs in the form of 870,000 And-Inverter-Graphs (AIGs) produced from 1500 synthesis runs plus labels such as the optimized node counts, and de-lay. We define a generic learning problem on this dataset and benchmark existing solutions for it. The codes related to dataset creation and benchmark models are available athttps://github.com/NYU-MLDA/OpenABC.git. The dataset generated is available athttps://archive.nyu.edu/handle/2451/63311},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  file = {/home/krawczuk/Zotero/storage/NBQF7P7S/Chowdhury et al. - 2021 - OpenABC-D A Large-Scale Dataset For Machine Learn.pdf;/home/krawczuk/Zotero/storage/8FJI3PEE/2110.html}
}

@article{chungEmbeddingGraphsBooks1987,
  title = {Embedding {{Graphs}} in {{Books}}: {{A Layout Problem}} with {{Applications}} to {{VLSI Design}}},
  shorttitle = {Embedding {{Graphs}} in {{Books}}},
  author = {Chung, Fan R. K. and Leighton, Frank Thomson and Rosenberg, Arnold L.},
  date = {1987-01},
  journaltitle = {SIAM Journal on Algebraic Discrete Methods},
  shortjournal = {SIAM. J. on Algebraic and Discrete Methods},
  volume = {8},
  number = {1},
  pages = {33--58},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0196-5212},
  doi = {10.1137/0608002},
  url = {https://epubs.siam.org/doi/abs/10.1137/0608002},
  urldate = {2024-02-22},
  abstract = {The relative powers of queues and stacks are compared as mechanisms for laying out the edges of a graph. In a k-queue layout, vertices of the graph are placed in some linear order (also called a linear arrangement), and each edge is assigned to exactly one of the k queues, so that the edges assigned to each queue obey a first-in/first-out (FIFO) discipline. As the vertices are scanned left to right, an edge is enqueued on its assigned queue when its left endpoint is encountered and is dequeued from its queue when its right endpoint is encountered. In a k-stack layout, vertices of the graph are placed in some linear order, and each edge is assigned to exactly one of the k stacks so that the edges assigned to each stack obey a last-in/first-out discipline. As the vertices are scanned left to right, an edge is pushed on its assigned stack when its left endpoint is encountered and is popped from its stack when its right endpoint is encountered. The paper has three main results. First, a tradeoff between queuenumber and stacknumber is shown for a fixed linear order of the vertices of G. In particular, for a fixed-order layout of a graph G, \textbackslash [ \{\textbackslash text\{queuenumber \}\} \textbackslash times \{\textbackslash text\{ stacknumber \}\} \textbackslash geq \{\textbackslash text\{ cutwidth/valence \}\}( G ).\textbackslash ] Second, it is shown that every 1-queue graph has a 2-stack layout and that every 1-stack graph has a 2-queue layout. Third, in a surprising display of the power of queues, it is shown that the ternary hypercube requires exponentially more stacks than queues. More precisely, an N-vertex ternary hypercube has a \$( 2\textbackslash log \_3 N )\$-queue layout but requires \$\textbackslash Omega ( N\^{}\{1/9 - \textbackslash epsilon \}  )\$ stacks, \$\textbackslash epsilon  {$>$} 0\$, in any stack layout. Also, some asymptotic bounds for the queuenumber of bounded-valence graphs are derived.},
  file = {/home/krawczuk/Zotero/storage/TNGH5ZYJ/Chung et al. - 1987 - Embedding Graphs in Books A Layout Problem with A.pdf}
}

@article{churchApplicationRecursiveArithmetic1963,
  title = {Application of {{Recursive Arithmetic}} to the {{Problem}} of {{Circuit Synthesis}}},
  author = {Church, Alonzo},
  date = {1963},
  journaltitle = {Journal of Symbolic Logic},
  volume = {28},
  number = {4},
  pages = {289--290},
  publisher = {{Association for Symbolic Logic}},
  doi = {10.2307/2271310},
  file = {/home/krawczuk/Zotero/storage/BXU6AFEL/Church - 1963 - Application of Recursive Arithmetic to the Problem.pdf}
}

@online{CircuitNetOpenSourceDataseta,
  title = {{{CircuitNet}}: {{An Open-Source Dataset}} for {{Machine Learning}} in {{VLSI CAD Applications With Improved Domain-Specific Evaluation Metric}} and {{Learning Strategies}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10158384?casa_token=5qpmtJtoYJIAAAAA:aDCl7xJR457l-Mw8opQAYhCed-lywODhgas0xX4ylhB1eY68GUD4QWzSWu5jZUkmrfVyEnbBzLSf},
  urldate = {2024-02-25},
  file = {/home/krawczuk/Zotero/storage/RAF83SM2/10158384.html}
}

@inproceedings{clarksonDAMNETSDeepAutoregressive2022,
  title = {{{DAMNETS}}: {{A Deep Autoregressive Model}} for {{Generating Markovian Network Time Series}}},
  shorttitle = {{{DAMNETS}}},
  booktitle = {Proceedings of the {{First Learning}} on {{Graphs Conference}}},
  author = {Clarkson, Jase and Cucuringu, Mihai and Elliott, Andrew and Reinert, Gesine},
  date = {2022-12-21},
  pages = {23:1-23:19},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v198/clarkson22a.html},
  urldate = {2024-02-25},
  abstract = {Generative models for network time series (also known as dynamic graphs) have tremendous potential in fields such as epidemiology, biology and economics, where complex graph-based dynamics are core objects of study. Designing flexible and scalable generative models is a very challenging task due to the high dimensionality of the data, as well as the need to represent temporal dependencies and marginal network structure. Here we introduce DAMNETS, a scalable deep generative model for network time series. DAMNETS outperforms competing methods on all of our measures of sample quality, over both real and synthetic data sets.},
  eventtitle = {Learning on {{Graphs Conference}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/I6VNYEQN/Clarkson et al. - 2022 - DAMNETS A Deep Autoregressive Model for Generatin.pdf}
}

@article{courtoisCanNeuralNetworks2023,
  title = {Can Neural Networks Extrapolate? {{Discussion}} of a Theorem by {{Pedro Domingos}}},
  shorttitle = {Can Neural Networks Extrapolate?},
  author = {Courtois, Adrien and Morel, Jean-Michel and Arias, Pablo},
  date = {2023-03-02},
  journaltitle = {Revista de la Real Academia de Ciencias Exactas, Físicas y Naturales. Serie A. Matemáticas},
  shortjournal = {Rev. Real Acad. Cienc. Exactas Fis. Nat. Ser. A-Mat.},
  volume = {117},
  number = {2},
  pages = {79},
  issn = {1579-1505},
  doi = {10.1007/s13398-023-01411-z},
  url = {https://doi.org/10.1007/s13398-023-01411-z},
  urldate = {2024-02-26},
  abstract = {Neural networks trained on large datasets by minimizing a loss have become the state-of-the-art approach for resolving data science problems, particularly in computer vision, image processing and natural language processing. In spite of their striking results, our theoretical understanding about how neural networks operate is limited. In particular, what are the extrapolation capabilities of trained neural networks if any? In this paper we discuss a theorem of Domingos stating that “every machine learned by continuous gradient descent is approximately a kernel machine”. According to Domingos, this fact leads to conclude that all machines trained on data are mere kernel machines. We first extend Domingo’s result in the discrete case and to networks with vector-valued output. We then study its relevance and significance on simple examples. We find that in simple cases, the “neural tangent kernel” arising in Domingos’ theorem does provide understanding of the networks’ predictions. When the task given to the network grows in complexity, the interpolation capability of the network can be effectively explained by Domingos’ theorem, and no extrapolation capability of the network beyond its learning domain is found, even when the network’s structure would allow for it. We illustrate this fact on a classic perception theory problem: recovering a shape from its boundary.},
  langid = {english},
  keywords = {68Q32,68T07,68T45,Gradient descent,Kernel machine,Machine learning,Neural networks,Neural tangent kernel,Planar topology},
  file = {/home/krawczuk/Zotero/storage/7W7C2K22/Courtois et al. - 2023 - Can neural networks extrapolate Discussion of a t.pdf}
}

@inproceedings{dablainEfficientAugmentationImbalanced2023,
  title = {Efficient {{Augmentation}} for {{Imbalanced Deep Learning}}},
  booktitle = {2023 {{IEEE}} 39th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  author = {Dablain, Damien A. and Bellinger, Colin and Krawczyk, Bartosz and Chawla, Nitesh V.},
  date = {2023-04},
  pages = {1433--1446},
  issn = {2375-026X},
  doi = {10.1109/ICDE55515.2023.00114},
  url = {https://ieeexplore.ieee.org/abstract/document/10184844},
  urldate = {2024-02-26},
  abstract = {Deep learning models may not effectively generalize across under-represented or minority classes. We empirically study a convolutional neural network’s (CNN) internal representation of imbalanced image data and measure the generalization gap between a model’s feature embeddings in the training and test sets, showing that the gap is wider for minority classes. This insight enables us to design an efficient three-phase CNN training framework for imbalanced data. The framework involves training the network end-to-end on imbalanced data to learn feature embeddings, performing data augmentation in the learned embedding space to balance the training data distribution, and fine-tuning the classifier head on the embedded balanced training data. We develop Expansive Over-Sampling (EOS) as a data augmentation technique to utilize in the training framework. EOS forms synthetic training instances as convex combinations between the minority class samples and their nearest adversaries in the embedding space to reduce the generalization gap. The proposed framework improves the accuracy over leading cost-sensitive and resampling methods commonly used in imbalanced learning. Moreover, it is more computationally efficient than standard data pre-processing methods, such as SMOTE and GAN-based over-sampling, as it requires fewer parameters and less training time. The source code for the proposed framework is available at: https://github.com/dd1github/EOS.},
  eventtitle = {2023 {{IEEE}} 39th {{International Conference}} on {{Data Engineering}} ({{ICDE}})},
  keywords = {class imbalance,Data augmentation,deep learning,Deep learning,Earth Observing System,machine learning,Neural networks,over-sampling,Source coding,Training,Training data},
  file = {/home/krawczuk/Zotero/storage/QVP7LPXY/Dablain et al. - 2023 - Efficient Augmentation for Imbalanced Deep Learnin.pdf;/home/krawczuk/Zotero/storage/C9CTEN3J/10184844.html}
}

@inproceedings{daiMarginalDistributionAdaptation2022b,
  title = {Marginal {{Distribution Adaptation}} for {{Discrete Sets}} via {{Module-Oriented Divergence Minimization}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Hanjun and Yang, Mengjiao and Xue, Yuan and Schuurmans, Dale and Dai, Bo},
  date = {2022-06-28},
  pages = {4605--4617},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/dai22c.html},
  urldate = {2024-02-20},
  abstract = {Distributions over discrete sets capture the essential statistics including the high-order correlation among elements. Such information provides powerful insight for decision making across various application domains, e.g., product assortment based on product distribution in shopping carts. While deep generative models trained on pre-collected data can capture existing distributions, such pre-trained models are usually not capable of aligning with a target domain in the presence of distribution shift due to reasons such as temporal shift or the change in the population mix. We develop a general framework to adapt a generative model subject to a (possibly counterfactual) target data distribution with both sampling and computation efficiency. Concretely, instead of re-training a full model from scratch, we reuse the learned modules to preserve the correlations between set elements, while only adjusting corresponding components to align with target marginal constraints. We instantiate the approach for three commonly used forms of discrete set distribution—latent variable, autoregressive, and energy based models—and provide efficient solutions for marginal-constrained optimization in either primal or dual forms. Experiments on both synthetic and real-world e-commerce and EHR datasets show that the proposed framework is able to practically align a generative model to match marginal constraints under distribution shift.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/HNPCWBKR/Dai et al. - 2022 - Marginal Distribution Adaptation for Discrete Sets.pdf}
}

@inproceedings{daiScalableDeepGenerative2020e,
  title = {Scalable {{Deep Generative Modeling}} for {{Sparse Graphs}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Hanjun and Nazi, Azade and Li, Yujia and Dai, Bo and Schuurmans, Dale},
  date = {2020-11-21},
  pages = {2302--2312},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v119/dai20b.html},
  urldate = {2024-02-25},
  abstract = {Learning graph generative models is a challenging task for deep learning and has wide applicability to a range of domains like chemistry, biology and social science. However current deep neural methods suffer from limited scalability: for a graph with n nodes and m edges, existing deep neural methods require Omega(n\^{}2) complexity by building up the adjacency matrix. On the other hand, many real world graphs are actually sparse in the sense that m {$<<$} n\^{}2. Based on this, we develop a novel autoregressive model, named BiGG, that utilizes this sparsity to avoid generating the full adjacency matrix, and importantly reduces the graph generation time complexity to O((n + m) log n). Furthermore, during training this autoregressive model can be parallelized with O(log n) synchronization stages, which makes it much more efficient than other autoregressive models that require Omega(n). Experiments on several benchmarks show that the proposed approach not only scales to orders of magnitude larger graphs than previously possible with deep autoregressive graph generative models, but also yields better graph generation quality.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/PCVU4XG6/Dai et al. - 2020 - Scalable Deep Generative Modeling for Sparse Graph.pdf;/home/krawczuk/Zotero/storage/QZ5JQ75V/Dai et al. - 2020 - Scalable Deep Generative Modeling for Sparse Graph.pdf}
}

@article{dana74AttorneyAgent,
  title = {(74) {{Attorney}}, {{Agent}},or {{Firm}} - {{Dana Legal Services}};},
  author = {Dana, Jubin},
  abstract = {In accordance with various embodiments and aspects of the invention, systems and methods are disclosed that can automatically find the best legal configuration that will be optimal with respect to a given set of requirements or metrics, such as: area, timing, and power. A designer defines the metrics or requirements, which represent the functional needs. A designer typically selects a set of parameters from a group of parameters available to user, which are user selectable parameters. The best parameters, from which the user can select parameters, are identified, and provided to the user. A constraint solver module ensures all rules are enforced and finds all legal parameters that fulfil the intent. The constraint solver module generates configura tions that meet the requirements and are legal configura tions .},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/M4Z7XGSE/Dana - (74) Attorney, Agent,or Firm - Dana Legal Services.pdf}
}

@article{dasilvaAutoTGReinforcementLearningBased2023,
  title = {{{AutoTG}}: {{Reinforcement Learning-Based Symbolic Optimization}} for {{AI-Assisted Power Converter Design}}},
  shorttitle = {{{AutoTG}}},
  author = {family=Silva, given=Felipe Leno, prefix=da, useprefix=true and Glatt, Ruben and Su, Wencong and Bui, Van-Hai and Chang, Fangyuan and Chaturvedi, Shivam and Wang, Mengqi and Murphey, Yi Lu and Huang, Can and Xue, Lingxiao and Zeng, Rong},
  date = {2023},
  journaltitle = {IEEE Journal of Emerging and Selected Topics in Industrial Electronics},
  pages = {1--10},
  issn = {2687-9743},
  doi = {10.1109/JESTIE.2023.3303836},
  url = {https://ieeexplore.ieee.org/abstract/document/10215513?casa_token=96l5LfY-AMcAAAAA:euKy9S6j6PHMeezQ52jY7vQGtR6vgoBqfb1p1bCTL5wXkgsNVQAsEIrnefOyEB9bRdyxHNXWCwyJ},
  urldate = {2024-02-19},
  abstract = {Power converters are pervasive in modern electronic component design. They can be found in all electronic devices from household appliances and cellphone chargers to vehicles. Currently, designing new circuit topologies is hard because it requires human expertise based on experience and is difficult to automate. However, artificial-intelligence-assisted design can significantly facilitate the development of new power converters and/or improve the final result. Intelligently designed highly efficient power converters can have a significant effect on many important attributes, such as power efficiency, layout size, cost, heat dissemination, energy requirements, etc. We propose Autonomous Topology Generator (AutoTG), a reinforcement-learning-based framework that generates power converter topology candidates based on user specifications, optimized for user preferences. By modeling power converter design as a symbolic optimization problem, we sequentially sample components in an autoregressive manner until new topologies are formed, providing both the topology specification and the sizing (magnitude of each component parameter) of the proposed power converter. We provide an empirical evaluation and show that AutoTG is able to generate varied high-efficiency topologies within component restrictions based on user input and show that previously unknown topologies can be found for further evaluation.},
  eventtitle = {{{IEEE Journal}} of {{Emerging}} and {{Selected Topics}} in {{Industrial Electronics}}},
  keywords = {(AI)-based design,Capacitors,Integrated circuit modeling,Mathematical models,Optimization,power converter design,Reinforcement learning,symbolic optimization applications,Topology,Voltage},
  file = {/home/krawczuk/Zotero/storage/NBNV78XX/da Silva et al. - 2023 - AutoTG Reinforcement Learning-Based Symbolic Opti.pdf;/home/krawczuk/Zotero/storage/U3WILZ5I/10215513.html}
}

@article{dastidarSynthesisSystemAnalog2005,
  title = {A Synthesis System for Analog Circuits Based on Evolutionary Search and Topological Reuse},
  author = {Dastidar, T.R. and Chakrabarti, P.P. and Ray, P.},
  date = {2005-04},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {9},
  number = {2},
  pages = {211--224},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2004.841308},
  url = {https://ieeexplore.ieee.org/abstract/document/1413261},
  urldate = {2024-02-20},
  abstract = {We present a method for automated synthesis of analog circuits using evolutionary search and a set of circuit design rules based on topological reuse. The system requires only moderate expert knowledge on part of the user. It allows circuit size, circuit topology, and device values to evolve. The circuit representation scheme employs a topological reuse-based approach-it uses commonly used subcircuits for analog design as inputs and utilizes these to create the final circuit. The connectivity between these blocks is governed by a well-defined set of rules and the scheme is capable of representing most standard analog circuit topologies. The system operation consists of two phases-in the first phase, the circuit size and topology are evolved. A limited amount of device sizing also occurs in this phase. The second phase consists entirely of device value optimization. The design of the evaluation function-which evaluates each generated circuit using SPICE simulations-has also been automated to a great extent. The evaluation function is generated automatically depending on a behavioral description of the circuit. We present several experimental results obtained using this scheme, including two types of comparators, two types of oscillators, and an XOR logic gate. The generated circuits closely resemble hand designed circuits. The computational needs of the system are modest.},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  keywords = {Analog circuits,Bioinformatics,circuit design,Circuit simulation,Circuit synthesis,Circuit topology,Design automation,evolutionary search,Genetic programming,Genomics,Process design,SPICE,SPICE simulation,topological reuse},
  file = {/home/krawczuk/Zotero/storage/EMHC2NGZ/1413261.html}
}

@inproceedings{dasTopologySynthesisAnalog2008,
  title = {Topology Synthesis of Analog Circuits Based on Adaptively Generated Building Blocks},
  booktitle = {Proceedings of the 45th Annual {{Design Automation Conference}}},
  author = {Das, Angan and Vemuri, Ranga},
  date = {2008-06-08},
  series = {{{DAC}} '08},
  pages = {44--49},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1391469.1391483},
  url = {https://dl.acm.org/doi/10.1145/1391469.1391483},
  urldate = {2024-02-19},
  abstract = {This paper presents an automated analog synthesis tool for topology generation and subsequent circuit sizing. Though sizing is indispensable, the paper mainly concentrates on topology generation. A new kind of GA is developed, where a fraction of the offsprings in each generation is built from building blocks or cells obtained from previous generations. The cells are stored in a hierarchically arranged library that also contains information on the preferred neighborhood of each cell. The adaptively formed cell library starts only with basic elements and gradually includes functionally useful and bigger blocks, pertinent to the design. The techniques have been applied to synthesize an operational amplifier and a ring oscillator design. Results show that with reasonable computational effort, topologies have evolved that are designer understandable.},
  isbn = {978-1-60558-115-6},
  keywords = {automated design,genetic algorithm,topology generation},
  file = {/home/krawczuk/Zotero/storage/N7PPL6U3/Das and Vemuri - 2008 - Topology synthesis of analog circuits based on ada.pdf}
}

@online{daviesSizeMattersLarge2023c,
  title = {Size {{Matters}}: {{Large Graph Generation}} with {{HiGGs}}},
  shorttitle = {Size {{Matters}}},
  author = {Davies, Alex O. and Ajmeri, Nirav S. and Filho, Telmo M. Silva},
  date = {2023-11-07},
  eprint = {2306.11412},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.11412},
  url = {http://arxiv.org/abs/2306.11412},
  urldate = {2024-02-25},
  abstract = {Large graphs are present in a variety of domains, including social networks, civil infrastructure, and the physical sciences to name a few. Graph generation is similarly widespread, with applications in drug discovery, network analysis and synthetic datasets among others. While GNN (Graph Neural Network) models have been applied in these domains their high in-memory costs restrict them to small graphs. Conversely less costly rule-based methods struggle to reproduce complex structures. We propose HIGGS (Hierarchical Generation of Graphs) as a model-agnostic framework of producing large graphs with realistic local structures. HIGGS uses GNN models with conditional generation capabilities to sample graphs in hierarchies of resolution. As a result HIGGS has the capacity to extend the scale of generated graphs from a given GNN model by quadratic order. As a demonstration we implement HIGGS using DiGress, a recent graph-diffusion model, including a novel edge-predictive-diffusion variant edge-DiGress. We use this implementation to generate categorically attributed graphs with tens of thousands of nodes. These HIGGS generated graphs are far larger than any previously produced using GNNs. Despite this jump in scale we demonstrate that the graphs produced by HIGGS are, on the local scale, more realistic than those from the rule-based model BTER.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks}
}

@online{decaoMolGANImplicitGenerative2022b,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  date = {2022-09-27},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1805.11973},
  url = {http://arxiv.org/abs/1805.11973},
  urldate = {2024-02-24},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse. Code at https://github.com/nicola-decao/MolGAN},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/EMX5CAXR/De Cao and Kipf - 2022 - MolGAN An implicit generative model for small mol.pdf;/home/krawczuk/Zotero/storage/RKRWRQZH/1805.html}
}

@online{DefiningCommandsOptional,
  title = {8.1 {{Defining Commands}} with an {{Optional Argument}}},
  url = {https://www.dickimaw-books.com/latex/novices/html/newcomopt.html},
  urldate = {2024-02-23},
  file = {/home/krawczuk/Zotero/storage/65EZM6L7/newcomopt.html}
}

@article{dibartolomeoSTRATISFIMALLAYOUTModular2022,
  title = {{{STRATISFIMAL LAYOUT}}: {{A}} Modular Optimization Model for Laying out Layered Node-Link Network Visualizations},
  shorttitle = {{{STRATISFIMAL LAYOUT}}},
  author = {family=Bartolomeo, given=Sara, prefix=di, useprefix=true and Riedewald, Mirek and Gatterbauer, Wolfgang and Dunne, Cody},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {1},
  pages = {324--334},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3114756},
  url = {https://ieeexplore.ieee.org/abstract/document/9556579?casa_token=rpAO5UT0J9oAAAAA:yZQZinEONYZWld1DiK49IFMVf3GhoRMvMvFiAX7HxH8w0USq5XcT6uMAWTOO6Z6SR-CKy-5s8HFT},
  urldate = {2024-02-22},
  abstract = {Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing layered layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a layout optimization model that prioritizes optimality - as compared to scalability - because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing Stratisfimal Layout, a modular integer-linear-programming formulation that can consider several important readability criteria simultaneously — crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that we believe cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at osf.io/qdyt9 with live examples at https://visdunneright.github.io/stratisfimal/.},
  eventtitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  keywords = {bendiness reduction,Computational modeling,crossing reduction,integer linear programming,Integer linear programming,Layered node-link visualization,Layout,nested groups,Optimization,Scalability,Structured Query Language,Visualization},
  file = {/home/krawczuk/Zotero/storage/FC2QISNY/di Bartolomeo et al. - 2022 - STRATISFIMAL LAYOUT A modular optimization model .pdf;/home/krawczuk/Zotero/storage/F6HYYYSY/9556579.html}
}

@online{dielemanContinuousDiffusionCategorical2022,
  title = {Continuous Diffusion for Categorical Data},
  author = {Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H. and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and Hawthorne, Curtis and Leblond, Rémi and Grathwohl, Will and Adler, Jonas},
  date = {2022-12-15},
  eprint = {2211.15089},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.15089},
  url = {http://arxiv.org/abs/2211.15089},
  urldate = {2024-02-25},
  abstract = {Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/6SVC9A4E/Dieleman et al. - 2022 - Continuous diffusion for categorical data.pdf;/home/krawczuk/Zotero/storage/CVLUB4T3/2211.html}
}

@online{DiffusionCNFLearningDenoising,
  title = {{{DiffusionCNF}}: {{Learning Denoising Diffusion Models}} via {{Conditional Normalizing Flows}} - {{University}} of {{Georgia}}},
  url = {https://esploro.libs.uga.edu/esploro/outputs/9949575129702959?institution=01GALI_UGA&skipUsageReporting=true&recordUsage=false},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/RNEJG3YT/9949575129702959.html}
}

@article{dingResultsTreeDecomposition1995,
  title = {Some Results on Tree Decomposition of Graphs},
  author = {Ding, Guoli and Oporowski, Bogdan},
  date = {1995},
  journaltitle = {Journal of Graph Theory},
  volume = {20},
  number = {4},
  pages = {481--499},
  issn = {1097-0118},
  doi = {10.1002/jgt.3190200412},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jgt.3190200412},
  urldate = {2024-02-20},
  abstract = {We investigate tree decompositions (T,(Xt)tϵV(T)) whose width is “close to optimal” and such that all the subtrees of T induced by the vertices of the graph are “small.” We prove the existence of such decompositions for various interpretations of “close to optimal” and “small.” As a corollary of these results, we prove that the dilation of a graph is bounded by a logarithmic function of the congestion of the graph thereby settling a generalization of a conjecture of Bienstock. © 1995 John Wiley \& Sons, Inc.},
  langid = {english}
}

@online{domingosEveryModelLearned2020b,
  title = {Every {{Model Learned}} by {{Gradient Descent Is Approximately}} a {{Kernel Machine}}},
  author = {Domingos, Pedro},
  date = {2020-11-30},
  eprint = {2012.00152},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2012.00152},
  url = {http://arxiv.org/abs/2012.00152},
  urldate = {2024-02-26},
  abstract = {Deep learning's successes are often attributed to its ability to automatically discover new representations of the data, rather than relying on handcrafted features like other learning methods. We show, however, that deep networks learned by the standard gradient descent algorithm are in fact mathematically approximately equivalent to kernel machines, a learning method that simply memorizes the data and uses it directly for prediction via a similarity function (the kernel). This greatly enhances the interpretability of deep network weights, by elucidating that they are effectively a superposition of the training examples. The network architecture incorporates knowledge of the target function into the kernel. This improved understanding should lead to better learning algorithms.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,I.5.1,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/Y2HH49QD/Domingos - 2020 - Every Model Learned by Gradient Descent Is Approxi.pdf;/home/krawczuk/Zotero/storage/MMMGI6NA/2012.html}
}

@online{dongCktGNNCircuitGraph2024,
  title = {{{CktGNN}}: {{Circuit Graph Neural Network}} for {{Electronic Design Automation}}},
  shorttitle = {{{CktGNN}}},
  author = {Dong, Zehao and Cao, Weidong and Zhang, Muhan and Tao, Dacheng and Chen, Yixin and Zhang, Xuan},
  date = {2024-02-09},
  eprint = {2308.16406},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.16406},
  url = {http://arxiv.org/abs/2308.16406},
  urldate = {2024-02-20},
  abstract = {The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains \$10\$K distinct operational amplifiers with carefully-extracted circuit specifications. OCB is also equipped with communicative circuit generation and evaluation capabilities such that it can help to generalize CktGNN to design various analog circuits by producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and human experts' manual designs. Our work paves the way toward a learning-based open-sourced design automation for analog circuits. Our source code is available at \textbackslash url\{https://github.com/zehao-dong/CktGNN\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/NALLKYJH/Dong et al. - 2024 - CktGNN Circuit Graph Neural Network for Electroni.pdf;/home/krawczuk/Zotero/storage/83W8FGXJ/2308.html}
}

@book{downeyParameterizedComplexity2012,
  title = {Parameterized {{Complexity}}},
  author = {Downey, Rodney G. and Fellows, M. R.},
  date = {2012-12-06},
  eprint = {HyTjBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The idea for this book was conceived over the second bottle of Villa Maria's Caber net Medot '89, at the dinner of the Australasian Combinatorics Conference held at Palmerston North, New Zealand in December 1990, where the authors first met and discovered they had a number of interests in common. Initially, we embarked on a small project to try to formulate reductions to address the apparent parame terized intractability of DOMINATING SET, and to introduce a structure in which to frame our answers. Having spent several months trying to get the definitions for the reductions right (they now seem so obvious), we turned to our tattered copies of Garey and Johnson's work [239]. We were stunned to find that virtually none of the classical reductions worked in the parameterized setting. We then wondered if we'd be able to find any interesting reductions. Several years, many more bottles, so many papers, and reductions later it [3] seemed that we had unwittingly stumbled upon what we believe is a truly central and new area of complexity theory. It seemed to us that the material would be of great interest to people working in areas where exact algorithms for a small range of parameters are natural and useful (e. g. , Molecular Biology, VLSI design). The tractability theory was rich with distinctive and powerful techniques. The intractability theory seemed to have a deep structure and techniques all of its own.},
  isbn = {978-1-4612-0515-9},
  langid = {english},
  pagetotal = {538},
  keywords = {Computers / Computer Science,Computers / Information Technology,Computers / Programming / Algorithms,Mathematics / Applied,Mathematics / Combinatorics,Mathematics / History \& Philosophy,Mathematics / Logic}
}

@online{duDeepReinforcementLearning2023a,
  title = {Beyond {{Deep Reinforcement Learning}}: {{A Tutorial}} on {{Generative Diffusion Models}} in {{Network Optimization}}},
  shorttitle = {Beyond {{Deep Reinforcement Learning}}},
  author = {Du, Hongyang and Zhang, Ruichen and Liu, Yinqiu and Wang, Jiacheng and Lin, Yijing and Li, Zonghang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Cui, Shuguang and Ai, Bo and Zhou, Haibo and Kim, Dong In},
  date = {2023-08-10},
  eprint = {2308.05384},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2308.05384},
  url = {http://arxiv.org/abs/2308.05384},
  urldate = {2024-02-20},
  abstract = {Generative Diffusion Models (GDMs) have emerged as a transformative force in the realm of Generative Artificial Intelligence (GAI), demonstrating their versatility and efficacy across a variety of applications. The ability to model complex data distributions and generate high-quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning. Furthermore, their iterative nature, which involves a series of noise addition and denoising steps, is a powerful and unique approach to learning and generating data. This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks. We delve into the strengths of GDMs, emphasizing their wide applicability across various domains, such as vision, text, and audio generation.We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks. The paper first provides a basic background of GDMs and their applications in network optimization. This is followed by a series of case studies, showcasing the integration of GDMs with Deep Reinforcement Learning (DRL), incentive mechanism design, Semantic Communications (SemCom), Internet of Vehicles (IoV) networks, etc. These case studies underscore the practicality and efficacy of GDMs in real-world scenarios, offering insights into network design. We conclude with a discussion on potential future directions for GDM research and applications, providing major insights into how they can continue to shape the future of network optimization.},
  pubstate = {preprint},
  keywords = {Computer Science - Networking and Internet Architecture,Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/krawczuk/Zotero/storage/35EQLGKK/Du et al. - 2023 - Beyond Deep Reinforcement Learning A Tutorial on .pdf;/home/krawczuk/Zotero/storage/ZMK6EKAR/2308.html}
}

@article{dudzikGraphNeuralNetworks2022a,
  title = {Graph {{Neural Networks}} Are {{Dynamic Programmers}}},
  author = {Dudzik, Andrew J. and Veličković, Petar},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {20635--20647},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/8248b1ded388fcdbbd121bcdfea3068c-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/42FLR4ZJ/Dudzik and Veličković - 2022 - Graph Neural Networks are Dynamic Programmers.pdf}
}

@inproceedings{duvenaudConvolutionalNetworksGraphs2015,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
  urldate = {2024-02-24},
  abstract = {We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/QC3NCDBB/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf}
}

@article{dwivediLogconcaveSamplingMetropolisHastings2019,
  title = {Log-Concave Sampling: {{Metropolis-Hastings}} Algorithms Are Fast},
  shorttitle = {Log-Concave Sampling},
  author = {Dwivedi, Raaz and Chen, Yuansi and Wainwright, Martin J. and Yu, Bin},
  date = {2019},
  journaltitle = {Journal of Machine Learning Research},
  volume = {20},
  number = {183},
  pages = {1--42},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v20/19-306.html},
  urldate = {2024-02-25},
  abstract = {We study the problem of sampling from a strongly log-concave density supported on  R d 𝑅 𝑑 , and prove a non-asymptotic upper bound on the mixing time of the Metropolis-adjusted Langevin algorithm (MALA). The method draws samples by simulating a Markov chain obtained from the discretization of an appropriate Langevin diffusion, combined with an accept-reject step. Relative to known guarantees for the unadjusted Langevin algorithm (ULA), our bounds show that the use of an accept-reject step in MALA leads to an exponentially improved dependence on the error-tolerance. Concretely, in order to obtain samples with TV error at most  δ 𝛿  for a density with condition number  κ 𝜅 , we show that MALA requires  O(κdlog(1/δ)) 𝑂 ( 𝜅 𝑑 log ⁡ ( 1 / 𝛿 ) )  steps from a warm start, as compared to the  O( κ 2 d/ δ 2 ) 𝑂 ( 𝜅 2 𝑑 / 𝛿 2 )  steps established in past work on ULA. We also demonstrate the gains of a modified version of MALA over ULA for weakly log-concave densities. Furthermore, we derive mixing time bounds for the Metropolized random walk (MRW) and obtain  O(κ) 𝑂 ( 𝜅 )  mixing time slower than MALA. We provide numerical examples that support our theoretical findings, and demonstrate the benefits of Metropolis-Hastings adjustment for Langevin-type sampling algorithms.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/LCEZUF5K/Dwivedi et al. - 2019 - Log-concave sampling Metropolis-Hastings algorith.pdf}
}

@online{edwardsMagicVLSI2024,
  title = {Magic {{VLSI}}},
  author = {Edwards, R Timothy},
  date = {2024},
  url = {http://opencircuitdesign.com/magic/},
  urldate = {2024-02-26},
  file = {/home/krawczuk/Zotero/storage/LIX96J4R/magic.html}
}

@online{EfficientArithmeticBlock,
  title = {Efficient {{Arithmetic Block Identification With Graph Learning}} and {{Network-Flow}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9975800?casa_token=dcI5WrXDmQkAAAAA:2HalhgJglK4bc3PJlTs7YnN_Ob_ZIiYPkdbTBPN3YUpPXs1Flyu9mRZpt3DZtkUvBE2dHJCe2TRV},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/A485QCGW/9975800.html}
}

@online{EfficientBatchConstrainedBayesian,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9336041?casa_token=fPG-7PuzrdgAAAAA:S9Y9hlsyrUarBMsLk8oVfvtisLBEAA9pVyN4KY3L9BNOOoDr4ZhBs5t5v0M1pw27rOSlhvXllF66},
  urldate = {2024-02-20}
}

@online{EfficientlyFindingBest,
  title = {Efficiently Finding the ‘Best’ Solution with Multi-Objectives from Multiple Topologies in Topology Library of Analog Circuit | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/4796529},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/D877Y7G4/4796529.html}
}

@online{EfficientPerformanceModelinga,
  title = {Efficient {{Performance Modeling}} for {{Automated CMOS Analog Circuit Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9528897?casa_token=ZBgdjluZh7sAAAAA:yF2LZzHwypFCVoDhIZjEr1XzBtaVFJMY4uvOcbPqNq0tQd21Igb4fmz-1MYaFQUWzeoOIMjCuIJY},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/AI476698/9528897.html}
}

@inproceedings{elesedyProvablyStrictGeneralisation2021b,
  title = {Provably {{Strict Generalisation Benefit}} for {{Equivariant Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Elesedy, Bryn and Zaidi, Sheheryar},
  date = {2021-07-01},
  pages = {2959--2969},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/elesedy21a.html},
  urldate = {2024-02-22},
  abstract = {It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/GXZ74F9P/Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf;/home/krawczuk/Zotero/storage/H5YXFAXX/Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf}
}

@inproceedings{elesedyProvablyStrictGeneralisation2021c,
  title = {Provably {{Strict Generalisation Benefit}} for {{Equivariant Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Elesedy, Bryn and Zaidi, Sheheryar},
  date = {2021-07-01},
  pages = {2959--2969},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/elesedy21a.html},
  urldate = {2024-02-26},
  abstract = {It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/62QY6MCU/Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf;/home/krawczuk/Zotero/storage/WJUN4KC4/Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf}
}

@inproceedings{eleyanSemicustomDesignFlow2009,
  title = {Semi-Custom Design Flow: {{Leveraging Place}} and Route Tools in {{Custom Circuit}} Design},
  shorttitle = {Semi-Custom Design Flow},
  booktitle = {2009 {{IEEE International Conference}} on {{IC Design}} and {{Technology}}},
  author = {Eleyan, Nadeem N. and Lin, Ken and Kamal, Masud and Mohammad, Baker and Bassett, Paul},
  date = {2009-05},
  pages = {143--147},
  issn = {2381-3555},
  doi = {10.1109/ICICDT.2009.5166283},
  url = {https://ieeexplore.ieee.org/document/5166283},
  urldate = {2024-02-19},
  abstract = {There are generally two options available to integrated circuit (IC) designers to physically implement their designs: synthesis/place and route design and custom circuit design. Each design approach has its advantages and draw backs. This paper will cover a hybrid design flow using concepts from both areas to give us a quick design turn around time while allowing control on custom placement and routing.},
  eventtitle = {2009 {{IEEE International Conference}} on {{IC Design}} and {{Technology}}},
  keywords = {Circuit synthesis,CMOS logic circuits,Design automation,Design optimization,Digital signal processing,Integrated circuit design,Integrated circuit noise,Integrated circuit synthesis,Logic design,Random access memory,Routing,Timing},
  file = {/home/krawczuk/Zotero/storage/8C2K4VZ8/5166283.html}
}

@online{EMGraphFastLearningBased,
  title = {{{EMGraph}}: {{Fast Learning-Based Electromigration Analysis}} for {{Multi-Segment Interconnect Using Graph Convolution Networks}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9586239?casa_token=aPtfgBTKzREAAAAA:dWCqyGFU9oJLp2WomDQDeX_SSivmmB0zTFY9nPzO2I7PjEGCKXEwvSh89lwPi4KBHbZauYItMAoi},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/DYYBZ2PE/9586239.html}
}

@online{engelmayerParallelAlgorithmsAlign2024,
  title = {Parallel {{Algorithms Align}} with {{Neural Execution}}},
  author = {Engelmayer, Valerie and Georgiev, Dobrik and Veličković, Petar},
  date = {2024-01-03},
  eprint = {2307.04049},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.04049},
  url = {http://arxiv.org/abs/2307.04049},
  urldate = {2024-02-26},
  abstract = {Neural algorithmic reasoners are parallel processors. Teaching them sequential algorithms contradicts this nature, rendering a significant share of their computations redundant. Parallel algorithms however may exploit their full computational power, therefore requiring fewer layers to be executed. This drastically reduces training times, as we observe when comparing parallel implementations of searching, sorting and finding strongly connected components to their sequential counterparts on the CLRS framework. Additionally, parallel versions achieve (often strongly) superior predictive performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/FQY5S2L3/Engelmayer et al. - 2024 - Parallel Algorithms Align with Neural Execution.pdf;/home/krawczuk/Zotero/storage/KCKHJFSZ/2307.html}
}

@article{erkolConsistencyPaysScience2023,
  title = {Consistency Pays off in Science},
  author = {Erkol, Sirag and Sikdar, Satyaki and Radicchi, Filippo and Fortunato, Santo},
  date = {2023-05-01},
  journaltitle = {Quantitative Science Studies},
  volume = {4},
  number = {2},
  eprint = {2210.08440},
  eprinttype = {arxiv},
  eprintclass = {physics},
  pages = {491--500},
  issn = {2641-3337},
  doi = {10.1162/qss_a_00252},
  url = {http://arxiv.org/abs/2210.08440},
  urldate = {2024-02-25},
  abstract = {The exponentially growing number of scientific papers stimulates a discussion on the interplay between quantity and quality in science. In particular, one may wonder which publication strategy may offer more chances of success: publishing lots of papers, producing a few hit papers, or something in between. Here we tackle this question by studying the scientific portfolios of Nobel Prize laureates. A comparative analysis of different citation-based indicators of individual impact suggests that the best path to success may rely on consistently producing high-quality work. Such a pattern is especially rewarded by a new metric, the \$E\$-index, which identifies excellence better than state-of-the-art measures.},
  keywords = {{Physics - Data Analysis, Statistics and Probability},Computer Science - Digital Libraries,Physics - Physics and Society},
  file = {/home/krawczuk/Zotero/storage/U6ATDYKC/Erkol et al. - 2023 - Consistency pays off in science.pdf;/home/krawczuk/Zotero/storage/G7TKDM2A/2210.html}
}

@article{fangPoincareKernelsHyperbolic2023,
  title = {Poincaré {{Kernels}} for {{Hyperbolic Representations}}},
  author = {Fang, Pengfei and Harandi, Mehrtash and Lan, Zhenzhong and Petersson, Lars},
  date = {2023-11-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {131},
  number = {11},
  pages = {2770--2792},
  issn = {1573-1405},
  doi = {10.1007/s11263-023-01834-6},
  url = {https://doi.org/10.1007/s11263-023-01834-6},
  urldate = {2024-02-26},
  abstract = {Embedding data in hyperbolic spaces has proven beneficial for many advanced machine learning applications. However, working in hyperbolic spaces is not without difficulties as a result of its curved geometry (e.g., computing the Fréchet mean of a set of points requires an iterative algorithm). In Euclidean spaces, one can resort to kernel machines that not only enjoy rich theoretical properties but that can also lead to superior representational power (e.g., infinite-width neural networks). In this paper, we introduce valid kernel functions for hyperbolic representations. This brings in two major advantages, 1. kernelization will pave the way to seamlessly benefit the representational power from kernel machines in conjunction with hyperbolic embeddings, and 2. the rich structure of the Hilbert spaces associated with kernel machines enables us to simplify various operations involving hyperbolic data. That said, identifying valid kernel functions on curved spaces is not straightforward and is indeed considered an open problem in the learning community. Our work addresses this gap and develops several positive definite kernels in hyperbolic spaces (modeled by a Poincaré ball), the proposed kernels include the rich universal ones (e.g., Poincaré RBF kernel), or realize the multiple kernel learning scheme (e.g., Poincaré radial kernel). We comprehensively study the proposed kernels on a variety of challenging tasks including few-shot learning, zero-shot learning, person re-identification, deep metric learning, knowledge distillation and self-supervised learning. The consistent performance gain over different tasks shows the benefits of the kernelization for hyperbolic representations.},
  langid = {english},
  keywords = {Deep metric learning,Few-shot learning,Hyperbolic spaces,Kernelization,Knowledge distillation,Person re-identification,Poincaré kernels,Self-supervised learning,Zero-shot learning},
  file = {/home/krawczuk/Zotero/storage/7LGJZ8TD/Fang et al. - 2023 - Poincaré Kernels for Hyperbolic Representations.pdf}
}

@article{fanPowerConverterCircuit2022a,
  title = {Power {{Converter Circuit Design Automation Using Parallel Monte Carlo Tree Search}}},
  author = {Fan, Shaoze and Zhang, Shun and Liu, Jianbo and Cao, Ningyuan and Guo, Xiaoxiao and Li, Jing and Zhang, Xin},
  date = {2022-12-24},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {28},
  number = {2},
  pages = {17:1--17:33},
  issn = {1084-4309},
  doi = {10.1145/3549538},
  url = {https://dl.acm.org/doi/10.1145/3549538},
  urldate = {2024-02-19},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using Monte Carlo Tree Search. Specifically, the framework embraces the upper-confidence-bound-tree (UCT), a variant of Monte Carlo Tree Search, to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy and can run in parallel to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduce design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient—the sequential version can generate topologies with the same quality while being up to 67\% faster. The parallelization schemes can further achieve high speedups compared to the sequential version.},
  keywords = {circuit synthesis,circuit topology design,Design automation,Monte Carlo Tree Search (MCTS),power converter,upper-confidence-bound tree (UCT)},
  file = {/home/krawczuk/Zotero/storage/3VAPTTCJ/Fan et al. - 2022 - Power Converter Circuit Design Automation Using Pa.pdf}
}

@inproceedings{fanSpecificationTopologyAutomatic2021d,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}}},
  shorttitle = {From {{Specification}} to {{Topology}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Fan, Shaoze and Cao, Ningyuan and Zhang, Shun and Li, Jing and Guo, Xiaoxiao and Zhang, Xin},
  date = {2021-11},
  pages = {1--9},
  issn = {1558-2434},
  doi = {10.1109/ICCAD51958.2021.9643552},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=a9WE7fzBkhUAAAAA:uF7KocWq5SpT4JvweG-3L54Dv12Mok8KiHNsHc5sdVKaVe45kdmEkH7hNIClQ6IMX6HGapaSvMdB},
  urldate = {2024-02-19},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient - it can generate topologies with the same quality while being up to 67\% faster. Additionally, we discussed some interesting circuits discovered by our framework.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  keywords = {Circuit topology,Computational efficiency,design automation,Energy efficiency,Manuals,power converter topology design,reinforcement learning,Reinforcement learning,Space exploration,Topology,upper-confidence-bound tree (UCT)},
  file = {/home/krawczuk/Zotero/storage/HQ7VFUGM/Fan et al. - 2021 - From Specification to Topology Automatic Power Co.pdf;/home/krawczuk/Zotero/storage/WS866L76/9643552.html}
}

@inproceedings{fanSpecificationTopologyAutomatic2021e,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}}},
  shorttitle = {From {{Specification}} to {{Topology}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Fan, Shaoze and Cao, Ningyuan and Zhang, Shun and Li, Jing and Guo, Xiaoxiao and Zhang, Xin},
  date = {2021-11-01},
  pages = {1--9},
  publisher = {{IEEE}},
  location = {{Munich, Germany}},
  doi = {10.1109/ICCAD51958.2021.9643552},
  url = {https://ieeexplore.ieee.org/document/9643552/},
  urldate = {2024-02-20},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous applicationspecific power converters. A conventional manual design procedure of such power converters is computation- and laborintensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient —it can generate topologies with the same quality while being up to 67\% faster. Additionally, we discussed some interesting circuits discovered by our framework.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  isbn = {978-1-66544-507-8},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/576QF55G/Fan et al. - 2021 - From Specification to Topology Automatic Power Co.pdf}
}

@inproceedings{fanSpecificationTopologyAutomatic2021f,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}}},
  shorttitle = {From {{Specification}} to {{Topology}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Fan, Shaoze and Cao, Ningyuan and Zhang, Shun and Li, Jing and Guo, Xiaoxiao and Zhang, Xin},
  date = {2021-11},
  pages = {1--9},
  issn = {1558-2434},
  doi = {10.1109/ICCAD51958.2021.9643552},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=OfMFhpoMuAQAAAAA:AfACosBN6oY2Vpsbk8_cjU86UjZRF_Ea-T_xy-llZuNF7C5ZRvRRToOmxHA4g0vG5tvB5wt09S5x},
  urldate = {2024-02-20},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient - it can generate topologies with the same quality while being up to 67\% faster. Additionally, we discussed some interesting circuits discovered by our framework.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  keywords = {Circuit topology,Computational efficiency,design automation,Energy efficiency,Manuals,power converter topology design,reinforcement learning,Reinforcement learning,Space exploration,Topology,upper-confidence-bound tree (UCT)},
  file = {/home/krawczuk/Zotero/storage/IP2RU9EW/Fan et al. - 2021 - From Specification to Topology Automatic Power Co.pdf}
}

@article{fayaziAnGeLFullyAutomatedAnalog2023a,
  title = {{{AnGeL}}: {{Fully-Automated Analog Circuit Generator Using}} a {{Neural Network Assisted Semi-Supervised Learning Approach}}},
  shorttitle = {{{AnGeL}}},
  author = {Fayazi, Morteza and Taba, Morteza Tavakoli and Afshari, Ehsan and Dreslinski, Ronald},
  date = {2023-11},
  journaltitle = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume = {70},
  number = {11},
  pages = {4516--4529},
  issn = {1558-0806},
  doi = {10.1109/TCSI.2023.3295737},
  url = {https://ieeexplore.ieee.org/abstract/document/10190116?casa_token=W4xAyjAsivMAAAAA:4e4PlPkYtzIr3vRUNkwyR78lbRvfkNBE1ywx3WA_MAYg-QtyL-XR9DIO20jdxR4HY6QqXKiPJD5E},
  urldate = {2024-02-19},
  abstract = {Machine Learning (ML) has shown promising results in predicting the behavior of analog circuits. However, in order to completely cover the design space for today’s complicated circuits, supervised ML requires a large number of labeled samples which is time-consuming to provide. Furthermore, a separate dataset must be collected for each circuit topology making all other previously gathered datasets useless. In this paper, we first present a database including labeled and unlabeled data. We use neural networks to determine the behavior of complicated topologies by combining the more simple ones. By generating such unlabeled data, the time for providing the training set is significantly reduced compared to the conventional approaches. Using this database, we propose a fully-automated analog circuit generator framework, AnGeL. AnGeL performs all the schematic circuit design steps from deciding the circuit topology to determining the circuit parameters i.e. sizing. Our results show that for multiple circuit topologies, in comparison to the state-of-the-art works while maintaining the same accuracy, the required labeled data is reduced by 4.7x - 1090x. Also, the runtime of AnGeL is 2.9x - 75x faster.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems I}}: {{Regular Papers}}},
  keywords = {Analog circuit design automation,Analog circuits,circuit sizing,Circuit synthesis,Circuit topology,Databases,Integrated circuit modeling,neural network,semi-supervised learning,Topology,topology selection,Training},
  file = {/home/krawczuk/Zotero/storage/EVNWLPZI/Fayazi et al. - 2023 - AnGeL Fully-Automated Analog Circuit Generator Us.pdf;/home/krawczuk/Zotero/storage/RELTEWK9/10190116.html}
}

@online{FEATSFrameworkExplorativea,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=Q7MqnOUlxuQAAAAA:AzXEteK3Y7AbcL5SV3ko3-aD8fUBJCvXV75FkslpJrfLg2RAseaiNNRII611beTwmAL2HDP_30I9},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/M6U2YDWA/6980087.html}
}

@online{FEATSFrameworkExplorativeb,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=7W00nHRy4joAAAAA:-NpwTCtaXxFtwCYJSnhNC1ZDlMRv_6NxpQFZLRE9OlqvgScAq5z60SejgPlV3fs_nRW4pm7RSSuT},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/ILE25UIW/6980087.html}
}

@online{FEATSFrameworkExplorativec,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=AupKaZ1Ni9MAAAAA:s6GSVExX1r_PejIk3CYI_X64YVSG4ls8AcDoa9F7QO33jCx7hbSkZYJ2-2dI5vbLj0_VZTKZerY},
  urldate = {2024-02-24},
  file = {/home/krawczuk/Zotero/storage/QUEYYDCM/6980087.html}
}

@online{fichteDynASP2DynamicProgramming2017,
  title = {{{DynASP2}}.5: {{Dynamic Programming}} on {{Tree Decompositions}} in {{Action}}},
  shorttitle = {{{DynASP2}}.5},
  author = {Fichte, Johannes K. and Hecher, Markus and Morak, Michael and Woltran, Stefan},
  date = {2017-06-28},
  eprint = {1706.09370},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.09370},
  url = {http://arxiv.org/abs/1706.09370},
  urldate = {2024-02-20},
  abstract = {A vibrant theoretical research area are efficient exact parameterized algorithms. Very recent solving competitions such as the PACE challenge show that there is also increasing practical interest in the parameterized algorithms community. An important research question is whether dedicated parameterized exact algorithms exhibit certain practical relevance and one can even beat well-established problem solvers. We consider the logic-based declarative modeling language and problem solving framework Answer Set Programming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based algorithms. An ASP solver (DynASP2), which is based on a classical dynamic programming on tree decompositions, has been published very recently. Unfortunately, DynASP2 can outperform modern ASP solvers on programs of small treewidth only if the question of interest is to count the number of solutions. In this paper, we describe underlying concepts of our new implementation (DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers even for finding just one solution when solving problems as the Steiner tree problem that have been modeled in ASP on graphs with low treewidth. Our implementation is based on a novel approach that we call multi-pass dynamic programming (M-DPSINC).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Logic in Computer Science},
  file = {/home/krawczuk/Zotero/storage/9P4WHLHN/Fichte et al. - 2017 - DynASP2.5 Dynamic Programming on Tree Decompositi.pdf;/home/krawczuk/Zotero/storage/826FFN38/1706.html}
}

@online{franklEmbeddingGraphsEuclidean,
  title = {Embedding Graphs in {{Euclidean}} Space},
  author = {Frankl, Nóra and Kupavskii, Andrey and Swanepoel, Konrad J.},
  eprint = {1802.03092},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.1016/j.jcta.2019.105146},
  url = {http://arxiv.org/abs/1802.03092},
  urldate = {2024-02-22},
  abstract = {The dimension of a graph \$G\$ is the smallest \$d\$ for which its vertices can be embedded in \$d\$-dimensional Euclidean space in the sense that the distances between endpoints of edges equal \$1\$ (but there may be other unit distances). Answering a question of Erd\textbackslash H\{o\}s and Simonovits [Ars Combin. 9 (1980) 229--246], we show that any graph with less than \$\textbackslash binom\{d+2\}\{2\}\$ edges has dimension at most \$d\$. Improving their result, we prove that that the dimension of a graph with maximum degree \$d\$ is at most \$d\$. We show the following Ramsey result: if each edge of the complete graph on \$2d\$ vertices is coloured red or blue, then either the red graph or the blue graph can be embedded in Euclidean \$d\$-space. We also derive analogous results for embeddings of graphs into the \$(d-1)\$-dimensional sphere of radius \$1/\textbackslash sqrt\{2\}\$.},
  pubstate = {preprint},
  keywords = {52C10,Mathematics - Combinatorics,Mathematics - Metric Geometry},
  file = {/home/krawczuk/Zotero/storage/L7DMQ4DE/Frankl et al. - Embedding graphs in Euclidean space.pdf;/home/krawczuk/Zotero/storage/W9SYS3E8/1802.html}
}

@online{fulber-garciaHowDrawFlowcharts2022,
  title = {How to {{Draw Flowcharts With LaTeX}} | {{Baeldung}} on {{Computer Science}}},
  author = {Fulber-Garcia, Vinicius},
  date = {2022-01-19T08:05:14+00:00},
  url = {https://www.baeldung.com/cs/latex-flowcharts},
  urldate = {2024-02-19},
  abstract = {Learn how to draw flowcharts with LaTeX/TikZ.},
  langid = {american},
  file = {/home/krawczuk/Zotero/storage/5MPRAJFQ/latex-flowcharts.html}
}

@online{FundamentalsLayoutDesign,
  title = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}} | {{SpringerLink}}},
  url = {https://link.springer.com/book/10.1007/978-3-030-39284-0},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/9PWSQLIF/978-3-030-39284-0.html}
}

@online{GANAGraphConvolutional,
  title = {{{GANA}}: {{Graph Convolutional Network Based Automated Netlist Annotation}} for {{Analog Circuits}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9116329?casa_token=pmDFJicmP08AAAAA:1rCal6CTB7-Eamcy3AvSogNocH1GGEo4qlxNM7-egouIg952ujAHMwdEnWLA-IPDRraV4uaxiUeh},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/IJYBLPVS/9116329.html}
}

@online{gaoOptimizationGeneralizationOverparameterized2022,
  title = {On the Optimization and Generalization of Overparameterized Implicit Neural Networks},
  author = {Gao, Tianxiang and Gao, Hongyang},
  date = {2022-09-30},
  eprint = {2209.15562},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2209.15562},
  url = {http://arxiv.org/abs/2209.15562},
  urldate = {2024-02-26},
  abstract = {Implicit neural networks have become increasingly attractive in the machine learning community since they can achieve competitive performance but use much less computational resources. Recently, a line of theoretical works established the global convergences for first-order methods such as gradient descent if the implicit networks are over-parameterized. However, as they train all layers together, their analyses are equivalent to only studying the evolution of the output layer. It is unclear how the implicit layer contributes to the training. Thus, in this paper, we restrict ourselves to only training the implicit layer. We show that global convergence is guaranteed, even if only the implicit layer is trained. On the other hand, the theoretical understanding of when and how the training performance of an implicit neural network can be generalized to unseen data is still under-explored. Although this problem has been studied in standard feed-forward networks, the case of implicit neural networks is still intriguing since implicit networks theoretically have infinitely many layers. Therefore, this paper investigates the generalization error for implicit neural networks. Specifically, we study the generalization of an implicit network activated by the ReLU function over random initialization. We provide a generalization bound that is initialization sensitive. As a result, we show that gradient flow with proper random initialization can train a sufficient over-parameterized implicit network to achieve arbitrarily small generalization errors.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/8S5JZM4G/Gao and Gao - 2022 - On the optimization and generalization of overpara.pdf;/home/krawczuk/Zotero/storage/LZEEF9KN/2209.html}
}

@book{gebotysOptimalVLSIArchitectural2012,
  title = {Optimal {{VLSI Architectural Synthesis}}: {{Area}}, {{Performance}} and {{Testability}}},
  shorttitle = {Optimal {{VLSI Architectural Synthesis}}},
  author = {Gebotys, Catherine H. and Elmasry, Mohamed I.},
  date = {2012-12-06},
  eprint = {njrUBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Although research in architectural synthesis has been conducted for over ten years it has had very little impact on industry. This in our view is due to the inability of current architectural synthesizers to provide area-delay competitive (or "optimal") architectures, that will support interfaces to analog, asynchronous, and other complex processes. They also fail to incorporate testability. The OASIC (optimal architectural synthesis with interface constraints) architectural synthesizer and the CATREE (computer aided trees) synthesizer demonstrate how these problems can be solved. Traditionally architectural synthesis is viewed as NP hard and there fore most research has involved heuristics. OASIC demonstrates by using an IP approach (using polyhedral analysis), that most input algo rithms can be synthesized very fast into globally optimal architectures. Since a mathematical model is used, complex interface constraints can easily be incorporated and solved. Research in test incorporation has in general been separate from syn thesis research. This is due to the fact that traditional test research has been at the gate or lower level of design representation. Nevertheless as technologies scale down, and complexity of design scales up, the push for reducing testing times is increased. On way to deal with this is to incorporate test strategies early in the design process. The second half of this text examines an approach for integrating architectural synthesis with test incorporation. Research showed that test must be considered during synthesis to provide good architectural solutions which minimize Xlll area delay cost functions.},
  isbn = {978-1-4615-4018-2},
  langid = {english},
  pagetotal = {293},
  keywords = {{Computers / Design, Graphics \& Media / CAD-CAM},Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Circuits / General}
}

@online{GenericTopologySelection,
  title = {A Generic Topology Selection Method for Analog Circuits with Embedded Circuit Sizing Demonstrated on the {{OTA}} Example | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/7927115?casa_token=ICCzPz3hLuEAAAAA:VeKxL84Rvqjsw44k7Y-0Wr-wLzB9ObE8Xn84V4EOfdSpxj6zjwrGnL2JhOFRAt7O6gQUnlQWJ2C3},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/IJVE8Z6S/7927115.html}
}

@online{genevayGANVAEOptimal2017e,
  title = {{{GAN}} and {{VAE}} from an {{Optimal Transport Point}} of {{View}}},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  date = {2017-06-06},
  eprint = {1706.01807},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1706.01807},
  url = {http://arxiv.org/abs/1706.01807},
  urldate = {2024-02-20},
  abstract = {This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE).},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/QWXUSBCN/Genevay et al. - 2017 - GAN and VAE from an Optimal Transport Point of Vie.pdf;/home/krawczuk/Zotero/storage/Y72YXKWA/1706.html}
}

@online{gengTorchDEQLibraryDeep2023,
  title = {{{TorchDEQ}}: {{A Library}} for {{Deep Equilibrium Models}}},
  shorttitle = {{{TorchDEQ}}},
  author = {Geng, Zhengyang and Kolter, J. Zico},
  date = {2023-10-28},
  eprint = {2310.18605},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2310.18605},
  urldate = {2024-02-26},
  abstract = {Deep Equilibrium (DEQ) Models, an emerging class of implicit models that maps inputs to fixed points of neural networks, are of growing interest in the deep learning community. However, training and applying DEQ models is currently done in an ad-hoc fashion, with various techniques spread across the literature. In this work, we systematically revisit DEQs and present TorchDEQ, an out-of-the-box PyTorch-based library that allows users to define, train, and infer using DEQs over multiple domains with minimal code and best practices. Using TorchDEQ, we build a ``DEQ Zoo'' that supports six published implicit models across different domains. By developing a joint framework that incorporates the best practices across all models, we have substantially improved the performance, training stability, and efficiency of DEQs on ten datasets across all six projects in the DEQ Zoo. TorchDEQ and DEQ Zoo are released as \textbackslash href\{https://github.com/locuslab/torchdeq\}\{open source\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/KC3XNGJC/Geng and Kolter - 2023 - TorchDEQ A Library for Deep Equilibrium Models.pdf;/home/krawczuk/Zotero/storage/PIIQ3Y7X/2310.html}
}

@inreference{GeometricGraphTheory2024,
  title = {Geometric Graph Theory},
  booktitle = {Wikipedia},
  date = {2024-01-16T22:33:01Z},
  url = {https://en.wikipedia.org/w/index.php?title=Geometric_graph_theory&oldid=1196243069},
  urldate = {2024-02-22},
  abstract = {Geometric graph theory in the broader sense is a large and amorphous subfield of graph theory, concerned with graphs defined by geometric means. In a stricter sense, geometric graph theory studies combinatorial and geometric properties of geometric graphs, meaning graphs drawn in the Euclidean plane with possibly intersecting straight-line edges, and topological graphs, where the edges are allowed to be arbitrary continuous curves connecting the vertices; thus, it can be described as "the theory of geometric and topological graphs" (Pach 2013). Geometric graphs are also known as spatial networks.},
  langid = {english},
  annotation = {Page Version ID: 1196243069},
  file = {/home/krawczuk/Zotero/storage/7XVHPA7E/Geometric_graph_theory.html}
}

@online{GeometricRepresentationsGraphs,
  title = {Geometric {{Representations}} of {{Graphs}} - {{Chair}} of {{Computer Science I}} - {{Algorithms}} and {{Complexity}}},
  url = {https://www.informatik.uni-wuerzburg.de/en/algo/projects/geometric-representations-of-graphs/},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/KJFZQ2Z6/geometric-representations-of-graphs.html}
}

@online{georgievDeepEquilibriumAlgorithmic2024,
  title = {The {{Deep Equilibrium Algorithmic Reasoner}}},
  author = {Georgiev, Dobrik and Liò, Pietro and Buffelli, Davide},
  date = {2024-02-09},
  eprint = {2402.06445},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.06445},
  url = {http://arxiv.org/abs/2402.06445},
  urldate = {2024-02-26},
  abstract = {Recent work on neural algorithmic reasoning has demonstrated that graph neural networks (GNNs) could learn to execute classical algorithms. Doing so, however, has always used a recurrent architecture, where each iteration of the GNN aligns with an algorithm's iteration. Since an algorithm's solution is often an equilibrium, we conjecture and empirically validate that one can train a network to solve algorithmic problems by directly finding the equilibrium. Note that this does not require matching each GNN iteration with a step of the algorithm.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/6JGRFAES/Georgiev et al. - 2024 - The Deep Equilibrium Algorithmic Reasoner.pdf;/home/krawczuk/Zotero/storage/YPKZFDDN/2402.html}
}

@online{georgievNeuralAlgorithmicReasoning2024,
  title = {Neural {{Algorithmic Reasoning}} for {{Combinatorial Optimisation}}},
  author = {Georgiev, Dobrik and Numeroso, Danilo and Bacciu, Davide and Liò, Pietro},
  date = {2024-02-13},
  eprint = {2306.06064},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.06064},
  url = {http://arxiv.org/abs/2306.06064},
  urldate = {2024-02-26},
  abstract = {Solving NP-hard/complete combinatorial problems with neural networks is a challenging research area that aims to surpass classical approximate algorithms. The long-term objective is to outperform hand-designed heuristics for NP-hard/complete problems by learning to generate superior solutions solely from training data. Current neural-based methods for solving CO problems often overlook the inherent "algorithmic" nature of the problems. In contrast, heuristics designed for CO problems, e.g. TSP, frequently leverage well-established algorithms, such as those for finding the minimum spanning tree. In this paper, we propose leveraging recent advancements in neural algorithmic reasoning to improve the learning of CO problems. Specifically, we suggest pre-training our neural model on relevant algorithms before training it on CO instances. Our results demonstrate that by using this learning setup, we achieve superior performance compared to non-algorithmically informed deep learning models.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/krawczuk/Zotero/storage/HMIRH4V5/Georgiev et al. - 2024 - Neural Algorithmic Reasoning for Combinatorial Opt.pdf;/home/krawczuk/Zotero/storage/9HS8DMZQ/2306.html}
}

@incollection{gilmerMessagePassingNeural2020,
  title = {Message {{Passing Neural Networks}}},
  booktitle = {Machine {{Learning Meets Quantum Physics}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  editor = {Schütt, Kristof T. and Chmiela, Stefan and family=Lilienfeld, given=O. Anatole, prefix=von, useprefix=true and Tkatchenko, Alexandre and Tsuda, Koji and Müller, Klaus-Robert},
  date = {2020},
  series = {Lecture {{Notes}} in {{Physics}}},
  pages = {199--214},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-40245-7_10},
  url = {https://doi.org/10.1007/978-3-030-40245-7_10},
  urldate = {2024-02-24},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. In this chapter, we describe a general common framework for learning representations on graph data called message passing neural networks (MPNNs) and show how several prior neural network models for graph data fit into this framework. This chapter contains large overlap with Gilmer et al. (International Conference on Machine Learning, pp. 1263–1272, 2017), and has been modified to highlight more recent extensions to the MPNN framework.},
  isbn = {978-3-030-40245-7},
  langid = {english}
}

@inproceedings{gilmerNeuralMessagePassing2017a,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017-07-17},
  pages = {1263--1272},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/gilmer17a.html},
  urldate = {2024-02-24},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/3USUCFH4/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf}
}

@article{glatt-holtzAcceptRejectMechanism2023,
  title = {On the Accept–Reject Mechanism for {{Metropolis}}–{{Hastings}} Algorithms},
  author = {Glatt-Holtz, Nathan and Krometis, Justin and Mondaini, Cecilia},
  date = {2023-12},
  journaltitle = {The Annals of Applied Probability},
  volume = {33},
  pages = {5279--5333},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {1050-5164, 2168-8737},
  doi = {10.1214/23-AAP1948},
  url = {https://projecteuclid.org/journals/annals-of-applied-probability/volume-33/issue-6B/On-the-acceptreject-mechanism-for-MetropolisHastings-algorithms/10.1214/23-AAP1948.full},
  urldate = {2024-02-25},
  abstract = {This work develops a powerful and versatile framework for determining acceptance ratios in Metropolis–Hastings-type Markov kernels widely used in statistical sampling problems. Our approach allows us to derive new classes of kernels which unify random walk or diffusion-type sampling methods with more complicated “extended phase space” algorithms based around ideas from Hamiltonian dynamics. Our starting point is an abstract result developed in the generality of measurable state spaces that addresses proposal kernels that possess a certain involution structure. Note that, while this underlying proposal structure suggests a scope which includes Hamiltonian-type kernels, we demonstrate that our abstract result is, in an appropriate sense, equivalent to an earlier general state space setting developed in (Ann. Appl. Probab. 8 (1998) 1–9) where the connection to Hamiltonian methods was more obscure. On the basis of our abstract results we develop several new classes of extended phase space, HMC-like algorithms. First we tackle the classical finite-dimensional setting of a continuously distributed target measure. We then consider an infinite-dimensional framework for targets which are absolutely continuous with respect to a Gaussian measure with a trace-class covariance. Each of these algorithm classes can be viewed as “surrogate-trajectory” methods, providing a versatile methodology to bypass expensive gradient computations through skillful reduced order modeling and/or data driven approaches as we begin to explore in a forthcoming companion work (Glatt-Holtz et al. (2023)). On the other hand, along with the connection of our main abstract result to the framework in (Ann. Appl. Probab. 8 (1998) 1–9), these algorithm classes provide a unifying picture connecting together a number of popular existing algorithms which arise as special cases of our general frameworks under suitable parameter choices. In particular we show that, in the finite-dimensional setting, we can produce an algorithm class which includes the Metropolis adjusted Langevin algorithm (MALA) and random walk Metropolis method (RWMC) alongside a number of variants of the HMC algorithm including the geometric approach introduced in (J. R. Stat. Soc. Ser. B. Stat. Methodol. 73 (2011) 123–214). In the infinite-dimensional situation, we show that the algorithm class we derive includes the preconditioned Crank–Nicolson (pCN), ∞MALA and ∞HMC methods considered in (Stoch. Dyn. 8 (2008) 319–350; Stochastic Process. Appl. 121 (2011) 2201–2230; Statist. Sci. 28 (2013) 424–446) as special cases.},
  issue = {6B},
  keywords = {65C05,65P10,Hamiltonian Monte Carlo,Markov chain Monte Carlo (MCMC) algorithms,Metropolis–Hastings algorithms,sampling on abstract state spaces,surrogate trajectory methods},
  file = {/home/krawczuk/Zotero/storage/FVZPBIG2/Glatt-Holtz et al. - 2023 - On the accept–reject mechanism for Metropolis–Hast.pdf}
}

@online{GNNCapChipScaleInterconnect,
  title = {{{GNN-Cap}}: {{Chip-Scale Interconnect Capacitance Extraction Using Graph Neural Network}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10314730?casa_token=b_LSbuO_O6cAAAAA:IqWNu4n9DoqhTPEMEIX9Ruj7du-yaEuWAjJnMwHPiqQ-FbY4QIp4XXm3WD9iinBJl6flelYhshzt},
  urldate = {2024-02-22}
}

@inproceedings{graebAnalogSynthesisDeterministic2022,
  title = {Analog {{Synthesis}} - {{The Deterministic Way}}},
  booktitle = {Proceedings of the 2022 {{International Symposium}} on {{Physical Design}}},
  author = {Graeb, Helmut},
  date = {2022-04-13},
  series = {{{ISPD}} '22},
  pages = {167--174},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3505170.3511043},
  url = {https://dl.acm.org/doi/10.1145/3505170.3511043},
  urldate = {2024-02-20},
  abstract = {While the majority of research in design automation for analog circuits has been relying on statistical solution approaches, deterministic approaches are an attractive alternative. This paper gives a few examples of deterministic methods for sizing, structural synthesis and layout synthesis of analog circuits, which have been developed over the past decades. It starts from the so-called characteristic boundary curve for interactive parameter optimization, and ends at recent approaches for structural synthesis of operational amplifiers based on functional block composition. A deterministic approach to analog placement and to yield optimization will also be described. The central role of structural analysis of circuit netlists in these approaches will be explained. A summary of the underlying mindset of analog design automation and an outlook on future opportunities for deterministic sizing and layout synthesis concludes the paper.},
  isbn = {978-1-4503-9210-5},
  keywords = {analog,circuit,design centering,layout,optimization,sizing,synthesis},
  file = {/home/krawczuk/Zotero/storage/AVWMFWHZ/Graeb - 2022 - Analog Synthesis - The Deterministic Way.pdf}
}

@inproceedings{graebLearningImplicitFunctional2023,
  title = {Learning from the {{Implicit Functional Hierarchy}} in an {{Analog Netlist}}},
  booktitle = {Proceedings of the 2023 {{International Symposium}} on {{Physical Design}}},
  author = {Graeb, Helmut and Leibl, Markus},
  date = {2023-03-26},
  series = {{{ISPD}} '23},
  pages = {93--100},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3569052.3578921},
  url = {https://dl.acm.org/doi/10.1145/3569052.3578921},
  urldate = {2024-02-19},
  abstract = {Analog circuit design is characterized by a plethora of implicit design and technology aspects available to the experienced designer. In order to create useful computer-aided design methods, this implicit knowledge has to be captured in a systematic and hierarchical way. A key approach to this goal is to "learn" the knowledge from the netlist of an analog circuit. This requires a library of structural and functional blocks for analog circuits together with their individual constraints and performance equations, graph homomorphism techniques to recognize blocks that can have different structural implementations and I/O pins, as well as synthesis methods that exploit the learned knowledge. In this contribution, we will present how to make use of the functional and structural hierarchy of operational amplifiers. As an application, we explore the capabilities of machine learning in the context of structural and functional properties and show that the results can be substantially improved by pre-processing data with traditional methods for functional block analysis. This claim is validated on a data set of roughly 100,000 readily sized and simulated operational amplifiers.},
  isbn = {978-1-4503-9978-4},
  keywords = {analog circuits,cad,design automation,neural networks,operational amplifier},
  file = {/home/krawczuk/Zotero/storage/XKU5CZMK/Graeb and Leibl - 2023 - Learning from the Implicit Functional Hierarchy in.pdf}
}

@inproceedings{grusAutomaticPlacerAnalog2023,
  title = {Automatic {{Placer}} for {{Analog Circuits Using Integer Linear Programming Warm Started}} by {{Graph Drawing}}:},
  shorttitle = {Automatic {{Placer}} for {{Analog Circuits Using Integer Linear Programming Warm Started}} by {{Graph Drawing}}},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Operations Research}} and {{Enterprise Systems}}},
  author = {Grus, Josef and Hanzálek, Zdeněk and Barri, Dalibor and Vacula, Patrik},
  date = {2023},
  pages = {106--116},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {{Lisbon, Portugal}},
  doi = {10.5220/0011789300003396},
  url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0011789300003396},
  urldate = {2024-02-20},
  eventtitle = {12th {{International Conference}} on {{Operations Research}} and {{Enterprise Systems}}},
  isbn = {978-989-758-627-9},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/L9VSMMIP/Grus et al. - 2023 - Automatic Placer for Analog Circuits Using Integer.pdf}
}

@article{gruverProteinDesignGuided2024,
  title = {Protein {{Design}} with {{Guided Discrete Diffusion}}},
  author = {Gruver, Nate and Stanton, Samuel and Frey, Nathan and Rudner, Tim G. J. and Hotzel, Isidro and Lafrance-Vanasse, Julien and Rajpal, Arvind and Cho, Kyunghyun and Wilson, Andrew G.},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/29591f355702c3f4436991335784b503-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/9D4HD67M/Gruver et al. - 2024 - Protein Design with Guided Discrete Diffusion.pdf}
}

@online{GtGeometricTopology,
  title = {Gt.Geometric Topology - {{Graph}} Embedding in {{3D}} Grid Minimizing Edge Length - {{MathOverflow}}},
  url = {https://mathoverflow.net/questions/158460/graph-embedding-in-3d-grid-minimizing-edge-length},
  urldate = {2024-02-22}
}

@article{gulcehreKnowledgeMattersImportance2016,
  title = {Knowledge {{Matters}}: {{Importance}} of {{Prior Information}} for {{Optimization}}},
  shorttitle = {Knowledge {{Matters}}},
  author = {Gülçehre, Çağlar and Bengio, Yoshua},
  date = {2016},
  journaltitle = {Journal of Machine Learning Research},
  volume = {17},
  number = {8},
  pages = {1--32},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v17/gulchere16a.html},
  urldate = {2024-02-26},
  abstract = {We explored the effect of introducing prior knowledge into the intermediate level of deep supervised neural networks on two tasks. On a task we designed, all black-box state-of-the-art machine learning algorithms which we tested, failed to generalize well. We motivate our work from the hypothesis that, there is a training barrier involved in the nature of such tasks, and that humans learn useful intermediate concepts from other individuals by using a form of supervision or guidance using a curriculum. Our results provide a positive evidence in favor of this hypothesis. In our experiments, we trained a two- tiered MLP architecture on a dataset for which each input image contains three sprites, and the binary target class is  1 1  if all of three shapes belong to the same category and otherwise the class is  0 0 . In terms of generalization, black-box machine learning algorithms could not perform better than chance on this task. Standard deep supervised neural networks also failed to generalize. However, using a particular structure and guiding the learner by providing intermediate targets in the form of intermediate concepts (the presence of each object) allowed us to solve the task efficiently. We obtained much better than chance, but imperfect results by exploring different architectures and optimization variants. This observation might be an indication of optimization difficulty when the neural network trained without hints on this task. We hypothesize that the learning difficulty is due to the composition of two highly non-linear tasks. Our findings are also consistent with the hypotheses on cultural learning inspired by the observations of training of neural networks sometimes getting stuck, even though good solutions exist, both in terms of training and generalization error.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/NGXVUR46/Gülçehre and Bengio - 2016 - Knowledge Matters Importance of Prior Information.pdf}
}

@inproceedings{gulrajaniImprovedTrainingWasserstein2017d,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/HHKIRMR4/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf}
}

@online{gulrajaniLikelihoodBasedDiffusionLanguage2023,
  title = {Likelihood-{{Based Diffusion Language Models}}},
  author = {Gulrajani, Ishaan and Hashimoto, Tatsunori B.},
  date = {2023-05-30},
  eprint = {2305.18619},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.18619},
  urldate = {2024-02-22},
  abstract = {Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GZZD4AKY/Gulrajani and Hashimoto - 2023 - Likelihood-Based Diffusion Language Models.pdf;/home/krawczuk/Zotero/storage/P75JPSPM/2305.html}
}

@inproceedings{guoCETransformerCasualEffect2021,
  title = {{{CETransformer}}: {{Casual Effect Estimation}} via {{Transformer Based Representation Learning}}},
  shorttitle = {{{CETransformer}}},
  booktitle = {Pattern {{Recognition}} and {{Computer Vision}}},
  author = {Guo, Zhenyu and Zheng, Shuai and Liu, Zhizhe and Yan, Kun and Zhu, Zhenfeng},
  editor = {Ma, Huimin and Wang, Liang and Zhang, Changshui and Wu, Fei and Tan, Tieniu and Wang, Yaonan and Lai, Jianhuang and Zhao, Yao},
  date = {2021},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {524--535},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-88013-2_43},
  abstract = {Treatment effect estimation, which refers to the estimation of causal effects and aims to measure the strength of the causal relationship, is of great importance in many fields but is a challenging problem in practice. As present, data-driven causal effect estimation faces two main challenges, i.e., selection bias and the missing of counterfactual. To address these two issues, most of the existing approaches tend to reduce the selection bias by learning a balanced representation, and then to estimate the counterfactual through the representation. However, they heavily rely on the finely hand-crafted metric functions when learning balanced representations, which generally doesn’t work well for the situations where the original distribution is complicated. In this paper, we propose a CETransformer model for casual effect estimation via transformer based representation learning. To learn the representation of covariates (features) robustly, a self-supervised transformer is proposed, by which the correlation between covariates can be well exploited through self-attention mechanism. In addition, an adversarial network is adopted to balance the distribution of the treated and control groups in the representation space. Experimental results on three real-world datasets demonstrate the advantages of the proposed CETransformer, compared with the state-of-the-art treatment effect estimation methods.},
  isbn = {978-3-030-88013-2},
  langid = {english},
  keywords = {Adversarial learning,Casual effect estimation,Transformer},
  file = {/home/krawczuk/Zotero/storage/947A7X89/Guo et al. - 2021 - CETransformer Casual Effect Estimation via Transf.pdf}
}

@inproceedings{guptaGRAFENNELearningGraphs2023,
  title = {{{GRAFENNE}}: {{Learning}} on {{Graphs}} with {{Heterogeneous}} and {{Dynamic Feature Sets}}},
  shorttitle = {{{GRAFENNE}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Gupta, Shubham and Manchanda, Sahil and Ranu, Sayan and Bedathur, Srikanta J.},
  date = {2023-07-03},
  pages = {12165--12181},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/gupta23b.html},
  urldate = {2024-02-20},
  abstract = {Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeiler-Leman tests, and therefore, the additional inductivity to unseen features does not come at the cost of expressivity. In addition, as demonstrated over four real-world graphs, GRAFENNE empowers the underlying GNN with high empirical efficacy and the ability to learn in continual fashion over streaming feature sets.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/76HYCMSE/Gupta et al. - 2023 - GRAFENNE Learning on Graphs with Heterogeneous an.pdf}
}

@online{guptaStructuringRepresentationGeometry2023,
  title = {Structuring {{Representation Geometry}} with {{Rotationally Equivariant Contrastive Learning}}},
  author = {Gupta, Sharut and Robinson, Joshua and Lim, Derek and Villar, Soledad and Jegelka, Stefanie},
  date = {2023-06-24},
  eprint = {2306.13924},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.13924},
  url = {http://arxiv.org/abs/2306.13924},
  urldate = {2024-02-20},
  abstract = {Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to improved performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color) that standard contrastive methods do not achieve. Code is available at https://github.com/Sharut/CARE.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/54EQDGJM/Gupta et al. - 2023 - Structuring Representation Geometry with Rotationa.pdf;/home/krawczuk/Zotero/storage/BLRMI2GL/2306.html}
}

@inproceedings{gusmaoDeepLearningToolbox2021,
  title = {A {{Deep Learning Toolbox}} for {{Analog Integrated Circuit Placement}}},
  booktitle = {{{SMACD}} / {{PRIME}} 2021; {{International Conference}} on {{SMACD}} and 16th {{Conference}} on {{PRIME}}},
  author = {Gusmao, Antonio and Canelas, Antonio and Horta, Nuno and Lourenco, Nuno and Martins, Ricardo},
  date = {2021-07},
  pages = {1--4},
  url = {https://ieeexplore.ieee.org/abstract/document/9547914},
  urldate = {2024-02-20},
  abstract = {This paper presents a deep learning toolbox, DEEPPLACER, to assist designers during the layout design of analog integrated circuits. DEEPPLACER relies on a simple pair-wise device interaction circuit description, i.e., the circuits’ topological constraints, to propose valid floorplan solutions for block-level structures, including topologies and deep technology nodes not used for its training, at push-button speed. Despite its automatic functionalities, the toolbox is focused on explainable artificial intelligence, involving the designer in the synthesis flow via filtering and editing options over the candidate floorplan solutions. This constant state of human-machine feedback environment turns the designer aware of the impact of each device’s position change and inherent tradeoffs while suggesting subsequent moves, ultimately increasing the designers’ productivity in this time-consuming and iterative task. Finally, DEEPPLACER is shown to instantly generate a floorplan with 61\% better constraint fulfilment than a human designed solution.},
  eventtitle = {{{SMACD}} / {{PRIME}} 2021; {{International Conference}} on {{SMACD}} and 16th {{Conference}} on {{PRIME}}},
  file = {/home/krawczuk/Zotero/storage/TESS7RSF/Gusmao et al. - 2021 - A Deep Learning Toolbox for Analog Integrated Circ.pdf;/home/krawczuk/Zotero/storage/4R3YSPCR/9547914.html}
}

@article{gusmaoDifferentiableConstraintsEncoding2023,
  title = {Differentiable {{Constraints}}’ {{Encoding}} for {{Gradient-Based Analog Integrated Circuit Placement Optimization}}},
  author = {Gusmão, António and Alves, Pedro and Horta, Nuno and Lourenço, Nuno and Martins, Ricardo},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {1},
  pages = {110},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics12010110},
  url = {https://www.mdpi.com/2079-9292/12/1/110},
  urldate = {2024-02-20},
  abstract = {Analog IC design is characterized by non-systematic re-design iterations, often requiring partial or complete layout re-design. The layout task usually starts with device placement, where the several performance figures and constraints to be met escalate its complexity immensely, and, due to the inherent tradeoffs, an “optimal” floorplan solution does not usually exist. Deep learning models are now establishing for the automation of the placement task of analog integrated circuit layout design, promising to bypass the limitations of existing approaches based on: time-consuming optimization processes with several constraints; or placement retargeting from legacy designs/templates, which rely heavily on legacy layout data. However, as the complexity of analog design cases tackled by these methodologies increases, a broader set of topological constraints must be supported to cover the different layout styles and circuit classes. Here, model-independent differentiable encodings for regularity, boundary, proximity, and symmetry island constraints are formulated for the first time in the literature, and an unsupervised loss function is used for the artificial neural network model to learn how to generate placements that follow them. The use of a deep learning model makes push-button speed placement generation possible, additionally, as only sizing data are required for its training, it discards the need to acquire legacy layouts containing insights into this vast set of, often neglected, constraints. The model is ultimately used to produce floorplans from scratch at push-button speed for real state-of-the-art analog structures, including technology nodes not used for training. A case-study comparison with a floorplan design made by a human-expert presents improvements in the fulfillment of every constraint, reaching an overall improvement of around 70\%, demonstrating the approach’s value in placement design.},
  issue = {1},
  langid = {english},
  keywords = {automatic placement,deep learning,electronic design automation,physical synthesis,topological constraints},
  file = {/home/krawczuk/Zotero/storage/TUG7TDXT/Gusmão et al. - 2023 - Differentiable Constraints’ Encoding for Gradient-.pdf}
}

@inproceedings{gusmaoLateBreakingResults2021,
  title = {Late {{Breaking Results}}: {{Attention}} in {{Graph2Seq Neural Networks}} towards {{Push-Button Analog IC Placement}}},
  shorttitle = {Late {{Breaking Results}}},
  booktitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Gusmao, Antonio and Horta, Nuno and Lourenco, Nuno and Martins, Ricardo},
  date = {2021-12-05},
  pages = {1360--1361},
  publisher = {{IEEE}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/DAC18074.2021.9586177},
  url = {https://ieeexplore.ieee.org/document/9586177/},
  urldate = {2024-02-20},
  abstract = {In this paper, disruptive research using modern embedding techniques and an attention-based encoder-decoder deep learning (DL) model is conducted to automate analog layout synthesis. Unlike previous legacy-based placement automation mechanisms, the attention-based Graph2Seq model is inherently independent of the number of devices within a circuit topology and their order. Moreover, its unsupervised training does not rely on expensive legacy layout data but only on sizing solutions. Experimental results show that the proposed model generates placement solutions at push-button speed and can generalize to circuit topologies and technological nodes not used in training. Moreover, while being scalable, the model produces placement solutions that compete with highly optimized analog placements and other, order-dependent and non-scalable, DL models.},
  eventtitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  isbn = {978-1-66543-274-0},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/WIZ4XQXT/Gusmao et al. - 2021 - Late Breaking Results Attention in Graph2Seq Neur.pdf}
}

@online{haefeliDiffusionModelsGraphs2023a,
  title = {Diffusion {{Models}} for {{Graphs Benefit From Discrete State Spaces}}},
  author = {Haefeli, Kilian Konstantin and Martinkus, Karolis and Perraudin, Nathanaël and Wattenhofer, Roger},
  date = {2023-08-15},
  eprint = {2210.01549},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2210.01549},
  url = {http://arxiv.org/abs/2210.01549},
  urldate = {2024-02-25},
  abstract = {Denoising diffusion probabilistic models and score-matching models have proven to be very powerful for generative tasks. While these approaches have also been applied to the generation of discrete graphs, they have, so far, relied on continuous Gaussian perturbations. Instead, in this work, we suggest using discrete noise for the forward Markov process. This ensures that in every intermediate step the graph remains discrete. Compared to the previous approach, our experimental results on four datasets and multiple architectures show that using a discrete noising process results in higher quality generated samples indicated with an average MMDs reduced by a factor of 1.5. Furthermore, the number of denoising steps is reduced from 1000 to 32 steps, leading to a 30 times faster sampling procedure.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/3BI6QX46/Haefeli et al. - 2023 - Diffusion Models for Graphs Benefit From Discrete .pdf;/home/krawczuk/Zotero/storage/KWZIZH4I/2210.html}
}

@inproceedings{hakhamaneshiBagNetBerkeleyAnalog2019d,
  title = {{{BagNet}}: {{Berkeley Analog Generator}} with {{Layout Optimizer Boosted}} with {{Deep Neural Networks}}},
  shorttitle = {{{BagNet}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}} ({{ICCAD}})},
  author = {Hakhamaneshi, Kourosh and Werblun, Nick and Abbeel, Pieter and Stojanović, Vladimir},
  date = {2019-11},
  pages = {1--8},
  issn = {1558-2434},
  doi = {10.1109/ICCAD45719.2019.8942062},
  url = {https://ieeexplore.ieee.org/abstract/document/8942062?casa_token=MmLlHOUsuEcAAAAA:PE70fQr7gDO_ggoIy96mlHokcVIK6qh4jdCCEAsJAudbyJpck8DrLxbSOuEbOda4xQU4DaWeJAk},
  urldate = {2024-02-24},
  abstract = {The discrepancy between post-layout and schematic simulation results continues to widen in analog design due in part to the domination of layout parasitics. This paradigm shift is forcing designers to adopt design methodologies that seamlessly integrate layout effects into the standard design flow. Hence, any simulation-based optimization framework should take into account time-consuming post-layout simulation results. This work presents a learning framework that learns to reduce the number of simulations of evolutionary-based combinatorial optimizers, using a DNN that discriminates against generated samples, before running simulations. Using this approach, the discriminator achieves at least two orders of magnitude improvement on sample efficiency for several large circuit examples including an optical link receiver layout.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}} ({{ICCAD}})},
  keywords = {Integrated circuit modeling,Layout,Measurement,Optimization,Sociology,Solid modeling,Statistics},
  file = {/home/krawczuk/Zotero/storage/NWYCDHQ2/Hakhamaneshi et al. - 2019 - BagNet Berkeley Analog Generator with Layout Opti.pdf;/home/krawczuk/Zotero/storage/QJL9Z99Q/8942062.html}
}

@article{hakhamaneshiPretrainingGraphNeural2023a,
  title = {Pretraining {{Graph Neural Networks}} for {{Few-Shot Analog Circuit Modeling}} and {{Design}}},
  author = {Hakhamaneshi, Kourosh and Nassar, Marcel and Phielipp, Mariano and Abbeel, Pieter and Stojanovic, Vladimir},
  date = {2023-07},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {7},
  pages = {2163--2173},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2022.3217421},
  url = {https://ieeexplore.ieee.org/abstract/document/9930855?casa_token=oEJI3VPE4T4AAAAA:_-NFX46KfVpWBpHgIGlFh450V-DbZxLtt0ImmW0KM2Yp6p459WUxUp8_u3mLpPoF68Yj1zhNNbs-},
  urldate = {2024-02-20},
  abstract = {Being able to predict the performance of circuits without running expensive simulations is a desired capability that can catalyze automated design. In this article, we present a supervised pretraining approach to learn circuit representations that can be adapted to new circuit topologies or unseen prediction tasks. We hypothesize that if we train a neural network (NN) that can predict the output direct current (dc) voltages of a wide range of circuit instances it will be forced to learn generalizable knowledge about the role of each circuit element and how they interact with each other. The dataset for this supervised learning objective can be easily collected at scale since the required dc simulation to get ground truth labels is relatively cheap. This representation would then be helpful for few-shot generalization to unseen circuit metrics that require more time-consuming simulations for obtaining the ground-truth labels. To cope with the variable topological structure of different circuits we describe each circuit as a graph and use graph NNs (GNNs) to learn node embeddings. We show that pretraining GNNs on prediction of output node voltages can encourage learning representations that can be adapted to new unseen topologies or prediction of new circuit-level properties with up to 10x more sample efficiency compared to a randomly initialized model. We further show that we can improve the sample efficiency of prior SoTA model-based optimization methods by 2\textbackslash times (almost as good as using an oracle model) via fintuning pretrained GNNs as the feature extractor of the learned models.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Adaptation models,Analog circuits,Circuit design,Feature extraction,graph neural networks (GNNs),Integrated circuit modeling,knowledge transfer,machine learning,Predictive models,pretraining,Task analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/6PE4IZT5/Hakhamaneshi et al. - 2023 - Pretraining Graph Neural Networks for Few-Shot Ana.pdf}
}

@article{hakhamaneshiPretrainingGraphNeural2023b,
  title = {Pretraining {{Graph Neural Networks}} for {{Few-Shot Analog Circuit Modeling}} and {{Design}}},
  author = {Hakhamaneshi, Kourosh and Nassar, Marcel and Phielipp, Mariano and Abbeel, Pieter and Stojanovic, Vladimir},
  date = {2023-07},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  volume = {42},
  number = {7},
  pages = {2163--2173},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2022.3217421},
  url = {https://ieeexplore.ieee.org/document/9930855/},
  urldate = {2024-02-20},
  abstract = {Being able to predict the performance of circuits without running expensive simulations is a desired capability that can catalyze automated design. In this article, we present a supervised pretraining approach to learn circuit representations that can be adapted to new circuit topologies or unseen prediction tasks. We hypothesize that if we train a neural network (NN) that can predict the output direct current (dc) voltages of a wide range of circuit instances it will be forced to learn generalizable knowledge about the role of each circuit element and how they interact with each other. The dataset for this supervised learning objective can be easily collected at scale since the required dc simulation to get ground truth labels is relatively cheap. This representation would then be helpful for few-shot generalization to unseen circuit metrics that require more time-consuming simulations for obtaining the ground-truth labels. To cope with the variable topological structure of different circuits we describe each circuit as a graph and use graph NNs (GNNs) to learn node embeddings. We show that pretraining GNNs on prediction of output node voltages can encourage learning representations that can be adapted to new unseen topologies or prediction of new circuit-level properties with up to 10x more sample efficiency compared to a randomly initialized model. We further show that we can improve the sample efficiency of prior SoTA model-based optimization methods by 2× (almost as good as using an oracle model) via fintuning pretrained GNNs as the feature extractor of the learned models.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/HKLTFE5U/Hakhamaneshi et al. - 2023 - Pretraining Graph Neural Networks for Few-Shot Ana.pdf}
}

@online{hanEquivalenceGraphConvolution2023,
  title = {On the {{Equivalence}} of {{Graph Convolution}} and {{Mixup}}},
  author = {Han, Xiaotian and Zeng, Hanqing and Chen, Yu and Nie, Shaoliang and Liu, Jingzhou and Narang, Kanika and Shakeri, Zahra and Sankararaman, Karthik Abinav and Jiang, Song and Khabsa, Madian and Wang, Qifan and Hu, Xia},
  date = {2023-09-29},
  eprint = {2310.00183},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.00183},
  url = {http://arxiv.org/abs/2310.00183},
  urldate = {2024-02-26},
  abstract = {This paper investigates the relationship between graph convolution and Mixup techniques. Graph convolution in a graph neural network involves aggregating features from neighboring samples to learn representative features for a specific node or sample. On the other hand, Mixup is a data augmentation technique that generates new examples by averaging features and one-hot labels from multiple samples. One commonality between these techniques is their utilization of information from multiple samples to derive feature representation. This study aims to explore whether a connection exists between these two approaches. Our investigation reveals that, under two mild conditions, graph convolution can be viewed as a specialized form of Mixup that is applied during both the training and testing phases. The two conditions are: 1) \textbackslash textit\{Homophily Relabel\} - assigning the target node's label to all its neighbors, and 2) \textbackslash textit\{Test-Time Mixup\} - Mixup the feature during the test time. We establish this equivalence mathematically by demonstrating that graph convolution networks (GCN) and simplified graph convolution (SGC) can be expressed as a form of Mixup. We also empirically verify the equivalence by training an MLP using the two conditions to achieve comparable performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/8VPIZYIG/Han et al. - 2023 - On the Equivalence of Graph Convolution and Mixup.pdf;/home/krawczuk/Zotero/storage/VFNWPPYL/2310.html}
}

@online{hanGeometricGraphRepresentation2022,
  title = {Geometric {{Graph Representation Learning}} via {{Maximizing Rate Reduction}}},
  author = {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Song, Qingquan and Li, Jundong and Hu, Xia},
  date = {2022-02-13},
  eprint = {2202.06241},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.06241},
  url = {http://arxiv.org/abs/2202.06241},
  urldate = {2024-02-22},
  abstract = {Learning discriminative node representations benefits various downstream tasks in graph analysis such as community detection and node classification. Existing graph representation learning methods (e.g., based on random walk and contrastive learning) are limited to maximizing the local similarity of connected nodes. Such pair-wise learning schemes could fail to capture the global distribution of representations, since it has no explicit constraints on the global geometric properties of representation space. To this end, we propose Geometric Graph Representation Learning (G2R) to learn node representations in an unsupervised manner via maximizing rate reduction. In this way, G2R maps nodes in distinct groups (implicitly stored in the adjacency matrix) into different subspaces, while each subspace is compact and different subspaces are dispersedly distributed. G2R adopts a graph neural network as the encoder and maximizes the rate reduction with the adjacency matrix. Furthermore, we theoretically and empirically demonstrate that rate reduction maximization is equivalent to maximizing the principal angles between different subspaces. Experiments on real-world datasets show that G2R outperforms various baselines on node classification and community detection tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/YFN2KS5N/Han et al. - 2022 - Geometric Graph Representation Learning via Maximi.pdf;/home/krawczuk/Zotero/storage/XYCSEUM5/2202.html}
}

@thesis{hanICPhysicalDesign2018,
  title = {{{IC Physical Design Methodologies}} for {{Advanced Process Nodes}}},
  author = {Han, Kwangsoo},
  date = {2018},
  institution = {{UC San Diego}},
  url = {https://escholarship.org/uc/item/5rn717w9},
  urldate = {2024-02-22},
  abstract = {Next-generation applications in mobile, automotive, internet of things, robotic, artificial intelligence, etc. domains require the development and integration of advanced systems-on-chip (SOCs) that deliver ever-higher performance with much lower power. Thus, Moore's Law continues to be necessary, and innovations are needed beyond this law to help manage performance, power, area and cost (PPAC) for integrated-circuit (IC) design. Among the steps in the typical IC design flow, physical design implementation critically impacts PPAC. However, in concert with continuation of the Moore's Law trajectory, IC physical design encounters new challenges such as patterning restrictions due to manufacturing limits, severe process variation, and escalating interconnect RC delay. This thesis presents techniques to mitigate these challenges, grouped into three main thrusts: (i) manufacturing-aware design methodologies, (ii) process-aware design methodologies, and (iii) interconnect-aware design methodologies.Multiple-patterning techniques play a key role in the quest to print ever-smaller features for continued technology scaling in advanced nodes. However, the use of multiple-patterning significantly raises the number of extra steps for patterning as well as layout constraints needed for patternability; this causes an explosion of design rules and a loss of achievable layout density. To manage the onslaught of complex design rules arising from multiple-patterning, the manufacturing-aware design methodologies thrust of this thesis proposes approaches to optimize 2D block mask layout for minimum timing degradation, perform detailed placement to fix complex front-end-of-line (FEOL) design rule violations, and evaluate complex back-end-of-line (BEOL) design rule impact.Design variability due to manufacturing process variations has significant impact on the quality and yield of modern IC designs. Escalating process variation with new device architectures and manufacturing techniques (e.g., FinFET, multiple-patterning, etc.) required for node scaling results in the rapid increase of pessimism and overdesign. To mitigate the impact of severe process variation, the process-aware design methodologies thrust of this thesis presents approaches to optimize top-level clock tree for OCV minimization, reduce skew variation in the clock network, and perform partitioning in 3DIC that leverages a priori knowledge of inter-die variation.In advanced technology nodes, interconnect RC delay becomes more and more dominant. The continuous rapid increase of interconnect RC leads to not only performance loss from interconnect delay increase, but circuit power and area degradation as well, due to exponential increase in the number of buffers and drivers. To mitigate the escalating interconnect RC delay, the interconnect-aware design methodologies thrust of this thesis proposes approaches to co-optimize wirelength and pathlength in routing, studies optimal wirelength-skew tradeoff and remaining suboptimality in interconnect tree constructions, and performs optimal generalized H-tree construction with buffering.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CFBEU759/Han - 2018 - IC Physical Design Methodologies for Advanced Proc.pdf}
}

@article{harjaniOASYSFrameworkAnalog1989a,
  title = {{{OASYS}}: A Framework for Analog Circuit Synthesis},
  shorttitle = {{{OASYS}}},
  author = {Harjani, R. and Rutenbar, R.A. and Carley, L.R.},
  date = {1989-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {8},
  number = {12},
  pages = {1247--1266},
  issn = {1937-4151},
  doi = {10.1109/43.44506},
  url = {https://ieeexplore.ieee.org/abstract/document/44506?casa_token=lTwqQXBnIH0AAAAA:0FaLAkXT-oJOA1nktVcV9mJRtTxomg5HGxBugj6Pym51WPrJbu6lyi5C0v8YgfNBHKO5LQ_vHQx7},
  urldate = {2024-02-20},
  abstract = {A hierarchically structured framework for analog circuit synthesis is described. This hierarchical structure has two important features: it decomposes the design task into a sequence of smaller tasks with uniform structure, and it simplifies the reuse of design knowledge. Mechanisms are described that select from among alternate design styles and translate performance specifications from one level in the hierarchy to the next lower, more concrete level. A prototype implementation, OASYS, synthesizes sized transistor schematics for CMOS operational amplifiers from performance specifications and process parameters. Measurements from detailed circuit simulation and from actual fabricated analog ICs based on OASYS-synthesized designs demonstrate that OASYS is capable of synthesizing functional circuits.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Analog integrated circuits,Circuit synthesis,Circuit topology,CMOS process,Design automation,Digital signal processing,Process design,Prototypes,Software libraries},
  file = {/home/krawczuk/Zotero/storage/YEUNP8FI/Harjani et al. - 1989 - OASYS a framework for analog circuit synthesis.pdf;/home/krawczuk/Zotero/storage/EPW4MXHN/44506.html}
}

@article{haslerProgrammableAnalogSystem2024a,
  title = {Programmable {{Analog System Benchmarks Leading}} to {{Efficient Analog Computation Synthesis}}},
  author = {Hasler, Jennifer and Hao, Cong},
  date = {2024-01-27},
  journaltitle = {ACM Transactions on Reconfigurable Technology and Systems},
  shortjournal = {ACM Trans. Reconfigurable Technol. Syst.},
  volume = {17},
  number = {1},
  pages = {12:1--12:25},
  issn = {1936-7406},
  doi = {10.1145/3625298},
  url = {https://dl.acm.org/doi/10.1145/3625298},
  urldate = {2024-02-20},
  abstract = {This effort develops the first rich suite of analog and mixed-signal benchmark of various sizes and domains, intended for use with contemporary analog and mixed-signal designs and synthesis tools. Benchmarking enables analog-digital co-design exploration as well as extensive evaluation of analog synthesis tools and the generated analog/mixed-signal circuit or device. The goals of this effort are defining analog computation system benchmarks, developing the required concepts for higher-level analog and mixed-signal tools to utilize these benchmarks, and enabling future automated architectural design space exploration (DSE) to determine the best configurable architecture (e.g., a new FPAA) for a certain family of applications. The benchmarks comprise multiple levels of an acoustic, a vision, a communications, and an analog filter system that must be simultaneously satisfied for a complete system.},
  keywords = {Analog benchmarks,analog computing,analog system synthesis,mixed-signal HLS},
  file = {/home/krawczuk/Zotero/storage/K3AV5RYC/Hasler and Hao - 2024 - Programmable Analog System Benchmarks Leading to E.pdf}
}

@article{hawashReversibleCircuitSynthesis2020,
  title = {Reversible {{Circuit Synthesis Time Reduction Based}} on {{Subtree-Circuit Mapping}}},
  author = {Hawash, Amjad and Awad, Ahmed and Abdalhaq, Baker},
  date = {2020-01},
  journaltitle = {Applied Sciences},
  volume = {10},
  number = {12},
  pages = {4147},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app10124147},
  url = {https://www.mdpi.com/2076-3417/10/12/4147},
  urldate = {2024-02-20},
  abstract = {Several works have been conducted regarding the reduction of the energy consumption in electrical circuits. Reversible circuit synthesis is considered to be one of the major efforts at reducing the amount of power consumption. The field of reversible circuit synthesis uses a large number of proposed algorithms to minimize the overall cost of circuits synthesis (represented in the line number and quantum cost), with minimal concern paid for synthesis time. However, because of the iterative nature of the synthesis optimization algorithms, synthesis time cannot be neglected as a parameter which needs to be tackled, especially for large-scale circuits which need to be realized by cascades of reversible gates. Reducing the synthesis cost can be achieved by Binary Decision Diagrams (BDDs), which are considered to be a step forward in this field. Nevertheless, the mapping of each BDD node into a cascade of reversible gates during the synthesis process is time-consuming. In this work, we implement the idea of the subtree-based mapping of BDD nodes to reversible gates instead of the classical nodal-based algorithm to effectively reduce the entire reversible circuit synthesis time. Considering Depth-First Search (DFS), we convert an entire BDD subtree in one step into a cascade of reversible gates. A look-up table for all possible combinations of subtrees and their corresponding reversible gates has been constructed, in which a hash key is used to directly access subtrees during the mapping process. This table is constructed as a result of a comprehensive study of all possible BDD subtrees and considered as a reference during the conversion process. The conducted experimental tests show a significant synthesis time reduction (around 95\% on average), preserving the correctness of the algorithm in generating a circuit realizing the required Boolean function.},
  issue = {12},
  langid = {english},
  keywords = {binary decision diagram (BDD),depth-first search (DFS),reversible circuits,subtrees,synthesis time reduction},
  file = {/home/krawczuk/Zotero/storage/TLJGLFCY/Hawash et al. - 2020 - Reversible Circuit Synthesis Time Reduction Based .pdf}
}

@article{hecherAdvancedToolsMethods2023,
  title = {Advanced Tools and Methods for Treewidth-Based Problem Solving},
  author = {Hecher, Markus},
  date = {2023-05-01},
  journaltitle = {it - Information Technology},
  volume = {65},
  number = {1-2},
  pages = {65--74},
  publisher = {{De Gruyter Oldenbourg}},
  issn = {2196-7032},
  doi = {10.1515/itit-2023-0004},
  url = {https://www.degruyter.com/document/doi/10.1515/itit-2023-0004/html},
  urldate = {2024-02-20},
  abstract = {Computer programs, so-called solvers, for solving the well-known Boolean satisfiability problem (S at ) have been improving for decades. Among the reasons, why these solvers are so fast, is the implicit usage of the formula’s structural properties during solving. One of such structural indicators is the so-called treewidth, which tries to measure how close a formula instance is to being easy (tree-like). This work focuses on logic-based problems and treewidth-based methods and tools for solving them. Many of these problems are also relevant for knowledge representation and reasoning (KR) as well as artificial intelligence (AI) in general. We present a new type of problem reduction, which is referred to by decomposition-guided ( DG ). This reduction type forms the basis to solve a problem for quantified Boolean formulas (QBFs) of bounded treewidth that has been open since 2004. The solution of this problem then gives rise to a new methodology for proving precise lower bounds for a range of further formalisms in logic, KR, and AI. Despite the established lower bounds, we implement an algorithm for solving extensions of S at efficiently, by directly using treewidth. Our implementation is based on finding abstractions of instances, which are then incrementally refined in the process. Thereby, our observations confirm that treewidth is an important measure that should be considered in the design of modern solvers.},
  langid = {english},
  keywords = {AI,ETH lower bounds,logic,parameterized complexity,quantitative reasoning,treewidth},
  file = {/home/krawczuk/Zotero/storage/5KBWR6BQ/Hecher - 2023 - Advanced tools and methods for treewidth-based pro.pdf}
}

@incollection{hicksBranchTreeDecomposition2005,
  title = {Branch and {{Tree Decomposition Techniques}} for {{Discrete Optimization}}},
  booktitle = {Emerging {{Theory}}, {{Methods}}, and {{Applications}}},
  author = {Hicks, Illya V. and Koster, Arie M. C. A. and Koloto?lu, Elif},
  date = {2005-09},
  series = {{{INFORMS TutORials}} in {{Operations Research}}},
  pages = {1--29},
  publisher = {{INFORMS}},
  doi = {10.1287/educ.1053.0017},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/educ.1053.0017},
  urldate = {2024-02-20},
  isbn = {978-1-877640-21-6},
  keywords = {branchwidth,combinatorial optimization,graph algorithms,treewidth},
  file = {/home/krawczuk/Zotero/storage/8YXIPCY7/Hicks et al. - 2005 - Branch and Tree Decomposition Techniques for Discr.pdf}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020f,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2024-02-25},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/4VZV9L5I/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@inproceedings{hodgkinsonStochasticContinuousNormalizing2021,
  title = {Stochastic Continuous Normalizing Flows: Training {{SDEs}} as {{ODEs}}},
  shorttitle = {Stochastic Continuous Normalizing Flows},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Hodgkinson, Liam and family=Heide, given=Chris, prefix=van der, useprefix=false and Roosta, Fred and Mahoney, Michael W.},
  date = {2021-12-01},
  pages = {1130--1140},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v161/hodgkinson21a.html},
  urldate = {2024-02-22},
  abstract = {We provide a general theoretical framework for stochastic continuous normalizing flows, an extension of continuous normalizing flows for density estimation of stochastic differential equations (SDEs). Using the theory of rough paths, the underlying Brownian motion is treated as a latent variable and approximated. Doing so enables the treatment of SDEs as random ordinary differential equations, which can be trained using existing techniques. For scalar loss functions, this approach naturally recovers the stochastic adjoint method of Li et al. [2020] for training neural SDEs, while supporting a more flexible class of approximations.},
  eventtitle = {Uncertainty in {{Artificial Intelligence}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CZW8X9GU/Hodgkinson et al. - 2021 - Stochastic continuous normalizing flows training .pdf}
}

@online{hodgkinsonStochasticNormalizingFlows2020,
  title = {Stochastic {{Normalizing Flows}}},
  author = {Hodgkinson, Liam and family=Heide, given=Chris, prefix=van der, useprefix=true and Roosta, Fred and Mahoney, Michael W.},
  date = {2020-02-25},
  eprint = {2002.09547},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.09547},
  url = {http://arxiv.org/abs/2002.09547},
  urldate = {2024-02-22},
  abstract = {We introduce stochastic normalizing flows, an extension of continuous normalizing flows for maximum likelihood estimation and variational inference (VI) using stochastic differential equations (SDEs). Using the theory of rough paths, the underlying Brownian motion is treated as a latent variable and approximated, enabling efficient training of neural SDEs as random neural ordinary differential equations. These SDEs can be used for constructing efficient Markov chains to sample from the underlying distribution of a given dataset. Furthermore, by considering families of targeted SDEs with prescribed stationary distribution, we can apply VI to the optimization of hyperparameters in stochastic MCMC.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/2SQHKHFX/Hodgkinson et al. - 2020 - Stochastic Normalizing Flows.pdf;/home/krawczuk/Zotero/storage/CWJGEQ4G/2002.html}
}

@inproceedings{hoogeboomArgmaxFlowsMultinomial2021d,
  title = {Argmax {{Flows}} and {{Multinomial Diffusion}}: {{Learning Categorical Distributions}}},
  shorttitle = {Argmax {{Flows}} and {{Multinomial Diffusion}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forré, Patrick and Welling, Max},
  date = {2021},
  volume = {34},
  pages = {12454--12465},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/67d96d458abdef21792e6d8e590244e7-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Generative flows and diffusion models have been predominantly trained on ordinal data, for example natural images. This paper introduces two extensions of flows and diffusion for categorical data such as language or image segmentation: Argmax Flows and Multinomial Diffusion. Argmax Flows are defined by a composition of a continuous distribution (such as a normalizing flow), and an argmax function. To optimize this model, we learn a probabilistic inverse for the argmax that lifts the categorical data to a continuous space. Multinomial Diffusion gradually adds categorical noise in a diffusion process, for which the generative denoising process is learned. We demonstrate that our method outperforms existing dequantization approaches on text modelling and modelling on image segmentation maps in log-likelihood.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/I4N2DREE/Hoogeboom et al. - 2021 - Argmax Flows and Multinomial Diffusion Learning C.pdf}
}

@online{hoogeboomAutoregressiveDiffusionModels2022,
  title = {Autoregressive {{Diffusion Models}}},
  author = {Hoogeboom, Emiel and Gritsenko, Alexey A. and Bastings, Jasmijn and Poole, Ben and family=Berg, given=Rianne, prefix=van den, useprefix=false and Salimans, Tim},
  date = {2022-02-01},
  eprint = {2110.02037},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2110.02037},
  url = {http://arxiv.org/abs/2110.02037},
  urldate = {2024-02-25},
  abstract = {We introduce Autoregressive Diffusion Models (ARDMs), a model class encompassing and generalizing order-agnostic autoregressive models (Uria et al., 2014) and absorbing discrete diffusion (Austin et al., 2021), which we show are special cases of ARDMs under mild assumptions. ARDMs are simple to implement and easy to train. Unlike standard ARMs, they do not require causal masking of model representations, and can be trained using an efficient objective similar to modern probabilistic diffusion models that scales favourably to highly-dimensional data. At test time, ARDMs support parallel generation which can be adapted to fit any given generation budget. We find that ARDMs require significantly fewer steps than discrete diffusion models to attain the same performance. Finally, we apply ARDMs to lossless compression, and show that they are uniquely suited to this task. Contrary to existing approaches based on bits-back coding, ARDMs obtain compelling results not only on complete datasets, but also on compressing single data points. Moreover, this can be done using a modest number of network calls for (de)compression due to the model's adaptable parallel generation.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/NUH2GQQU/Hoogeboom et al. - 2022 - Autoregressive Diffusion Models.pdf;/home/krawczuk/Zotero/storage/9Y33ZZMV/2110.html}
}

@inproceedings{hoogeboomEquivariantDiffusionMolecule2022d,
  title = {Equivariant {{Diffusion}} for {{Molecule Generation}} in {{3D}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Hoogeboom, Emiel and Satorras, Vı́ctor Garcia and Vignac, Clément and Welling, Max},
  date = {2022-06-28},
  pages = {8867--8887},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/hoogeboom22a.html},
  urldate = {2024-02-22},
  abstract = {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and the efficiency at training time.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/58APPAE8/Hoogeboom et al. - 2022 - Equivariant Diffusion for Molecule Generation in 3.pdf}
}

@online{hornTopologicalGraphNeural2021,
  title = {Topological {{Graph Neural Networks}}},
  author = {Horn, Max and De Brouwer, Edward and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
  date = {2021-02-15},
  url = {https://arxiv.org/abs/2102.07835v4},
  urldate = {2024-02-20},
  abstract = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {/home/krawczuk/Zotero/storage/M3Q6YB6N/Horn et al. - 2021 - Topological Graph Neural Networks.pdf}
}

@patent{hoyerNeuralReparameterizationOptimization2023,
  type = {patentus},
  title = {Neural Reparameterization for Optimization of Physical Designs},
  author = {Hoyer, Stephan Owen Steele and Sohl-Dickstein, Jascha Narain and Greydanus, Samuel James},
  holder = {{Google LLC}},
  date = {2023-02-07},
  number = {11574093B2},
  url = {https://patents.google.com/patent/US11574093B2/en},
  urldate = {2024-02-22},
  langid = {english},
  keywords = {design,learned model,machine,physical,solution},
  file = {/home/krawczuk/Zotero/storage/TC7WLA4R/Hoyer et al. - 2023 - Neural reparameterization for optimization of phys.pdf}
}

@article{huangBayesianOptimizationApproach2022,
  title = {Bayesian {{Optimization Approach}} for {{RF Circuit Synthesis}} via {{Multitask Neural Network Enhanced Gaussian Process}}},
  author = {Huang, Jiangli and Tao, Cong and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-11},
  journaltitle = {IEEE Transactions on Microwave Theory and Techniques},
  shortjournal = {IEEE Trans. Microwave Theory Techn.},
  volume = {70},
  number = {11},
  pages = {4787--4795},
  issn = {0018-9480, 1557-9670},
  doi = {10.1109/TMTT.2022.3194980},
  url = {https://ieeexplore.ieee.org/document/9852008/},
  urldate = {2024-02-20},
  abstract = {An RF integrated circuit design heavily relies upon experienced experts to iteratively tune the circuit parameters. A Bayesian optimization (BO) method is explored in existing works for automated analog and RF circuit synthesis. The overall performance can be further improved by constructing a model to exploit the correlation among different circuit specifications. In this article, we propose a BO approach for RF circuit synthesis via a multitask neural network enhanced Gaussian process (MTNN-GP). We present a novel multioutput GP model, in which the kernel functions of multiple outputs are constructed from a multitask neural network with shared hidden layers and taskspecific layers. Therefore, the correlation between the outputs can be captured by the shared hidden layers. Our proposed MTNN-GP-based BO method is compared with several stateof-the-art BO methods on three real word RF circuits and achieves best performance. The experimental results demonstrate the effectiveness and efficiency of our proposed method.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/G4EQSFQD/Huang et al. - 2022 - Bayesian Optimization Approach for RF Circuit Synt.pdf}
}

@article{huangMachineLearningElectronic2021g,
  title = {Machine {{Learning}} for {{Electronic Design Automation}}: {{A Survey}}},
  shorttitle = {Machine {{Learning}} for {{Electronic Design Automation}}},
  author = {Huang, Guyue and Hu, Jingbo and He, Yifan and Liu, Jialong and Ma, Mingyuan and Shen, Zhaoyang and Wu, Juejian and Xu, Yuanfan and Zhang, Hengrui and Zhong, Kai and Ning, Xuefei and Ma, Yuzhe and Yang, Haoyu and Yu, Bei and Yang, Huazhong and Wang, Yu},
  date = {2021-06-05},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {26},
  number = {5},
  pages = {40:1--40:46},
  issn = {1084-4309},
  doi = {10.1145/3451179},
  url = {https://dl.acm.org/doi/10.1145/3451179},
  urldate = {2024-02-24},
  abstract = {With the down-scaling of CMOS technology, the design complexity of very large-scale integrated is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 1990s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interest in incorporating ML to solve EDA tasks. In this article, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.},
  keywords = {Electronic design automation,machine learning,neural networks},
  file = {/home/krawczuk/Zotero/storage/SYNNP5JH/Huang et al. - 2021 - Machine Learning for Electronic Design Automation.pdf}
}

@article{huttonAutomaticGenerationSynthetic2002,
  title = {Automatic Generation of Synthetic Sequential Benchmark Circuits},
  author = {Hutton, M.D. and Rose, J.S. and Corneil, D.G.},
  date = {2002-08},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {21},
  number = {8},
  pages = {928--940},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2002.800456},
  url = {https://ieeexplore.ieee.org/abstract/document/1020350?casa_token=_xZfijPDv2kAAAAA:AzZcTFlVzzkSVUCb-P77q6Jg_VA3leMyffgFkCSWXmLugdY5bzZ1soIq4-9C-AGvLCwBl2XVajPl},
  urldate = {2024-02-19},
  abstract = {The design of programmable logic architectures and supporting computer-aided design tools fundamentally requires both a good understanding of the combinatorial nature of netlist graphs and sufficient quantities of realistic examples to evaluate or benchmark the results. In this paper, the authors investigate these two issues. They introduce an abstract model for describing sequential circuits and a collection of statistical parameters for better understanding the nature of circuits. Based upon this model they introduce and formally define the signature of a circuit netlist and the signature equivalence of netlists. They give an algorithm (GEN) for generating sequential benchmark netlists, significantly expanding previous work (Hutton et al, 1998) which generated purely combinational circuits. By comparing synthetic circuits to existing benchmarks and random graphs they show that GEN circuits are significantly more realistic than random graphs. The authors further illustrate the viabilty of the methodology by applying GEN to a case study comparing two partitioning algorithms.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Benchmark testing,Character generation,Circuit testing,Design automation,Logic circuits,Logic design,Partitioning algorithms,Programmable logic arrays,Programmable logic devices,Routing},
  file = {/home/krawczuk/Zotero/storage/DETIVN87/Hutton et al. - 2002 - Automatic generation of synthetic sequential bench.pdf;/home/krawczuk/Zotero/storage/QBVJA95Z/1020350.html}
}

@online{ImprovedWassersteinConditional,
  title = {Improved {{Wasserstein}} Conditional Generative Adversarial Network Speech Enhancement | {{EURASIP Journal}} on {{Wireless Communications}} and {{Networking}}},
  url = {https://link.springer.com/article/10.1186/s13638-018-1196-0#Sec3},
  urldate = {2024-02-26}
}

@online{InfinityMirrorTest,
  title = {The {{Infinity Mirror Test}} for {{Graph Models}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9669099?casa_token=zvheUYoBQFcAAAAA:efB15J4Xcm9eZ578T-eadl_dIolU167RDWuLBzUtOBW42odP8k7-GBjRr7b7-1SP7J1T-lLjuk2P},
  urldate = {2024-02-25},
  file = {/home/krawczuk/Zotero/storage/E9ULBI5Y/9669099.html}
}

@inproceedings{irieDualFormNeural2022,
  title = {The {{Dual Form}} of {{Neural Networks Revisited}}: {{Connecting Test Time Predictions}} to {{Training Patterns}} via {{Spotlights}} of {{Attention}}},
  shorttitle = {The {{Dual Form}} of {{Neural Networks Revisited}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Irie, Kazuki and Csordás, Róbert and Schmidhuber, Jürgen},
  date = {2022-06-28},
  pages = {9639--9659},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/irie22a.html},
  urldate = {2024-02-26},
  abstract = {Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/TZS86GND/Irie et al. - 2022 - The Dual Form of Neural Networks Revisited Connec.pdf}
}

@inproceedings{ishihataBagBasedSearchMetaAlgorithm2024,
  title = {The {{Bag-Based Search}}: {{A Meta-Algorithm}} to~{{Construct Tractable Logical Circuits}} for~{{Graphs Based}} on~{{Tree Decomposition}}},
  shorttitle = {The {{Bag-Based Search}}},
  booktitle = {Combinatorial {{Optimization}} and {{Applications}}},
  author = {Ishihata, Masakazu},
  editor = {Wu, Weili and Guo, Jianxiong},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {337--350},
  publisher = {{Springer Nature Switzerland}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-49614-1_25},
  abstract = {Tractable logical circuits (TLCs) have attracted more attention in the AI field as bases of knowledge representation and tractable probabilistic modeling. We propose the bag-based search (BBS), a new meta-algorithm for constructing a TLC that accepts all subgraphs of a given input graph that satisfies a target graph property. We implemented BBS examples for various graph properties, including independent set, k-edgeset, dominating set, k-matchings, and spanning trees, and applied them to artificial and real-world graphs. The experimental results showed that BBS generated significantly smaller circuits than ZDDs obtained by the frontier-based search (FBS).},
  isbn = {978-3-031-49614-1},
  langid = {english}
}

@inproceedings{ivanovaReinforcementLearningDesign2022,
  title = {Reinforcement {{Learning}} at {{Design}} of {{Electronic Circuits}}: {{Review}} and {{Analysis}}},
  shorttitle = {Reinforcement {{Learning}} at {{Design}} of {{Electronic Circuits}}},
  booktitle = {Proceedings of the 2022 5th {{Artificial Intelligence}} and {{Cloud Computing Conference}}},
  author = {Ivanova, Malinka and Rozeva, Anna and Ninov, Angel and Stosovic, Miona Andrejevic},
  date = {2022-12-17},
  pages = {275--284},
  publisher = {{ACM}},
  location = {{Osaka Japan}},
  doi = {10.1145/3582099.3582140},
  url = {https://dl.acm.org/doi/10.1145/3582099.3582140},
  urldate = {2024-02-19},
  eventtitle = {{{AICCC}} 2022: 2022 5th {{Artificial Intelligence}} and {{Cloud Computing Conference}}},
  isbn = {978-1-4503-9874-9},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CM5RLSQJ/Ivanova et al. - 2022 - Reinforcement Learning at Design of Electronic Cir.pdf}
}

@inproceedings{jaegerSimpleLatentVariable2023,
  title = {A {{Simple Latent Variable Model}} for {{Graph Learning}} and {{Inference}}},
  author = {Jaeger, Manfred and Longa, Antonio and Azzolin, Steve and Schulte, Oliver and Passerini, Andrea},
  date = {2023-11-25},
  url = {https://openreview.net/forum?id=S9jem2KZVr},
  urldate = {2024-02-20},
  abstract = {We introduce a probabilistic latent variable model for graphs that generalizes both the established graphon and stochastic block models. This naive histogram AHK model is simple and versatile, and we demonstrate its use for disparate tasks including complex predictive inference usually not supported by other approaches, and graph generation. We analyze the tradeoffs entailed by the simplicity of the model, which imposes certain limitations on expressivity on the one hand, but on the other hand leads to robust generalization capabilities to graph sizes different from what was seen in the training data.},
  eventtitle = {The {{Second Learning}} on {{Graphs Conference}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/2UKK7NJ4/Jaeger et al. - 2023 - A Simple Latent Variable Model for Graph Learning .pdf}
}

@inproceedings{jangSimpleScalableRepresentation2023a,
  title = {A {{Simple}} and {{Scalable Representation}} for {{Graph Generation}}},
  author = {Jang, Yunhui and Lee, Seul and Ahn, Sungsoo},
  date = {2023-12-21},
  url = {https://openreview.net/forum?id=wjVKHEPoU2},
  urldate = {2024-02-20},
  abstract = {Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar. Our findings reveal that the adoption of this compact representation not only enhances scalability but also bolsters performance by simplifying the graph generation process. We conduct a comprehensive evaluation across ten non-attributed and two molecular graph generation tasks, demonstrating the effectiveness of GEEL.},
  eventtitle = {{{NeurIPS}} 2023 {{Workshop}}: {{New Frontiers}} in {{Graph Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/Y6V429HW/Jang et al. - 2023 - A Simple and Scalable Representation for Graph Gen.pdf}
}

@inproceedings{jinEMGraphFastLearningBased2021a,
  title = {{{EMGraph}}: {{Fast Learning-Based Electromigration Analysis}} for {{Multi-Segment Interconnect Using Graph Convolution Networks}}},
  shorttitle = {{{EMGraph}}},
  booktitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Jin, Wentian and Chen, Liang and Sadiqbatcha, Sheriff and Peng, Shaoyi and Tan, Sheldon X.-D.},
  date = {2021-12},
  pages = {919--924},
  issn = {0738-100X},
  doi = {10.1109/DAC18074.2021.9586239},
  url = {https://ieeexplore.ieee.org/abstract/document/9586239?casa_token=aPtfgBTKzREAAAAA:dWCqyGFU9oJLp2WomDQDeX_SSivmmB0zTFY9nPzO2I7PjEGCKXEwvSh89lwPi4KBHbZauYItMAoi},
  urldate = {2024-02-22},
  abstract = {Electromigration (EM) becomes a major concern for VLSI circuits as the technology advances in the nanometer regime. With Korhonen equations, EM assessment for VLSI circuits remains challenged due to the increasing integrated density. VLSI multisegment interconnect trees can be naturally viewed as graphs. Based on this observation, we propose a new graph convolution network (GCN) model, which is called EMGraph considering both node and edge embedding features, to estimate the transient EM stress of interconnect trees. Compared with recently proposed generative adversarial network (GAN) based stress image-generation method, EMGraph model can learn more transferable knowledge to predict stress distributions on new graphs without retraining via inductive learning. Trained on the large dataset, the model shows less than 1.5\% averaged error compared to the ground truth results and is orders of magnitude faster than both COMSOL and state-of-the-art method. It also achieves smaller model size, 4\textbackslash times accuracy and 14\textbackslash times speedup over the GAN-based method.},
  eventtitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  keywords = {Convolution,Electromigration,Electromigration (EM),Generative adversarial networks,graph convolution network (GCN),hydrostatic stress assessment,Image edge detection,Integrated circuit interconnections,multisegment interconnect,Predictive models,Very large scale integration},
  file = {/home/krawczuk/Zotero/storage/HCNGD9JY/Jin et al. - 2021 - EMGraph Fast Learning-Based Electromigration Anal.pdf;/home/krawczuk/Zotero/storage/T5GR7I2E/9586239.html}
}

@inproceedings{jingHDMIHighorderDeep2021,
  title = {{{HDMI}}: {{High-order Deep Multiplex Infomax}}},
  shorttitle = {{{HDMI}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Jing, Baoyu and Park, Chanyoung and Tong, Hanghang},
  date = {2021-06-03},
  series = {{{WWW}} '21},
  pages = {2414--2424},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3442381.3449971},
  url = {https://dl.acm.org/doi/10.1145/3442381.3449971},
  urldate = {2024-02-22},
  abstract = {Networks have been widely used to represent the relations between objects such as academic networks and social networks, and learning embedding for networks has thus garnered plenty of research attention. Self-supervised network representation learning aims at extracting node embedding without external supervision. Recently, maximizing the mutual information between the local node embedding and the global summary (e.g. Deep Graph Infomax, or DGI for short) has shown promising results on many downstream tasks such as node classification. However, there are two major limitations of DGI. Firstly, DGI merely considers the extrinsic supervision signal (i.e., the mutual information between node embedding and global summary) while ignores the intrinsic signal (i.e., the mutual dependence between node embedding and node attributes). Secondly, nodes in a real-world network are usually connected by multiple edges with different relations, while DGI does not fully explore the various relations among nodes. To address the above-mentioned problems, we propose a novel framework, called High-order Deep Multiplex Infomax (HDMI), for learning node embedding on multiplex networks in a self-supervised way. To be more specific, we first design a joint supervision signal containing both extrinsic and intrinsic mutual information by high-order mutual information, and we propose a High-order Deep Infomax (HDI) to optimize the proposed supervision signal. Then we propose an attention based fusion module to combine node embedding from different layers of the multiplex network. Finally, we evaluate the proposed HDMI on various downstream tasks such as unsupervised clustering and supervised classification. The experimental results show that HDMI achieves state-of-the-art performance on these tasks.},
  isbn = {978-1-4503-8312-7},
  keywords = {High-order Mutual Information,Multiplex Networks,Network Representation Learning},
  file = {/home/krawczuk/Zotero/storage/N624ARPD/Jing et al. - 2021 - HDMI High-order Deep Multiplex Infomax.pdf}
}

@inproceedings{joEdgeRepresentationLearning2021,
  title = {Edge {{Representation Learning}} with {{Hypergraphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Jo, Jaehyeong and Baek, Jinheon and Lee, Seul and Kim, Dongki and Kang, Minki and Hwang, Sung Ju},
  date = {2021},
  volume = {34},
  pages = {7534--7546},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/3def184ad8f4755ff269862ea77393dd-Abstract.html},
  urldate = {2024-02-26},
  abstract = {Graph neural networks have recently achieved remarkable success in representing graph-structured data, with rapid progress in both the node embedding and graph pooling methods. Yet, they mostly focus on capturing information from the nodes considering their connectivity, and not much work has been done in representing the edges, which are essential components of a graph. However, for tasks such as graph reconstruction and generation, as well as graph classification tasks for which the edges are important for discrimination, accurately representing edges of a given graph is crucial to the success of the graph representation learning. To this end, we propose a novel edge representation learning framework based on Dual Hypergraph Transformation (DHT), which transforms the edges of a graph into the nodes of a hypergraph. This dual hypergraph construction allows us to apply message-passing techniques for node representations to edges. After obtaining edge representations from the hypergraphs, we then cluster or drop edges to obtain holistic graph-level edge representations. We validate our edge representation learning method with hypergraphs on diverse graph datasets for graph representation and generation performance, on which our method largely outperforms existing graph representation learning methods. Moreover, our edge representation learning and pooling method also largely outperforms state-of-the-art graph pooling methods on graph classification, not only because of its accurate edge representation learning, but also due to its lossless compression of the nodes and removal of irrelevant edges for effective message-passing.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/6JAHP4CY/Jo et al. - 2021 - Edge Representation Learning with Hypergraphs.pdf}
}

@article{jonesRecipesEconomicGrowth,
  title = {Recipes and {{Economic Growth}}: {{A Combinatorial March Down}} an {{Exponential Tail}}},
  author = {Jones, Charles I},
  abstract = {New ideas are often combinations of existing goods or ideas, a point emphasized by Romer (1993) and Weitzman (1998). A separate literature highlights the links between exponential growth and Pareto distributions: Gabaix (1999) shows how exponential growth generates Pareto distributions, while Kortum (1997) shows how Pareto distributions generate exponential growth. But this raises a “chicken and egg” problem: which came first, the exponential growth or the Pareto distribution? And regardless, what happened to the Romer and Weitzman insight that combinatorics should be important? This paper answers these questions by demonstrating that combinatorial growth in the number of draws from standard thin-tailed distributions leads to exponential economic growth; no Pareto assumption is required. More generally, it provides a theorem linking the behavior of the max extreme value to the number of draws and the shape of the tail for continuous probability distributions.},
  langid = {english},
  keywords = {❓ Multiple DOI},
  file = {/home/krawczuk/Zotero/storage/7J3MS35M/Jones - Recipes and Economic Growth A Combinatorial March.pdf}
}

@inproceedings{joScorebasedGenerativeModeling2022g,
  title = {Score-Based {{Generative Modeling}} of {{Graphs}} via the {{System}} of {{Stochastic Differential Equations}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Jo, Jaehyeong and Lee, Seul and Hwang, Sung Ju},
  date = {2022-06-28},
  pages = {10362--10383},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/jo22a.html},
  urldate = {2024-02-25},
  abstract = {Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/I2RVC8UK/Jo et al. - 2022 - Score-based Generative Modeling of Graphs via the .pdf}
}

@inproceedings{kabaEquivarianceLearnedCanonicalization2023b,
  title = {Equivariance with {{Learned Canonicalization Functions}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Kaba, Sékou-Oumar and Mondal, Arnab Kumar and Zhang, Yan and Bengio, Yoshua and Ravanbakhsh, Siamak},
  date = {2023-07-03},
  pages = {15546--15566},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/kaba23a.html},
  urldate = {2024-02-20},
  abstract = {Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, NNN-body dynamics prediction, point cloud classification and part segmentation, while being faster across the board.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/9AWGFSE5/Kaba et al. - 2023 - Equivariance with Learned Canonicalization Functio.pdf}
}

@online{kabaSymmetryBreakingEquivariant2023,
  title = {Symmetry {{Breaking}} and {{Equivariant Neural Networks}}},
  author = {Kaba, Sékou-Oumar and Ravanbakhsh, Siamak},
  date = {2023-12-14},
  eprint = {2312.09016},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2312.09016},
  url = {http://arxiv.org/abs/2312.09016},
  urldate = {2024-02-20},
  abstract = {Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/RNVHCACP/Kaba and Ravanbakhsh - 2023 - Symmetry Breaking and Equivariant Neural Networks.pdf;/home/krawczuk/Zotero/storage/89E3Q7W5/2312.html}
}

@inproceedings{karakidaUnderstandingApproximateFisher2020,
  title = {Understanding {{Approximate Fisher Information}} for {{Fast Convergence}} of {{Natural Gradient Descent}} in {{Wide Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Karakida, Ryo and Osawa, Kazuki},
  date = {2020},
  volume = {33},
  pages = {10891--10901},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/7b41bfa5085806dfa24b8c9de0ce567f-Abstract.html},
  urldate = {2024-02-26},
  abstract = {Natural Gradient Descent (NGD) helps to accelerate the convergence of gradient descent dynamics, but it requires approximations in large-scale deep neural networks because of its high computational cost. Empirical studies have confirmed that some NGD methods with approximate Fisher information converge sufficiently fast in practice. Nevertheless, it remains unclear from the theoretical perspective why and under what conditions such heuristic approximations work well. In this work, we reveal that, under specific conditions, NGD with approximate Fisher information achieves the same fast convergence to global minima as exact NGD. We consider deep neural networks in the infinite-width limit, and analyze the asymptotic training dynamics of NGD in function space via the neural tangent kernel. In the function space, the training dynamics with the approximate Fisher information are identical to those with the exact Fisher information, and they converge quickly. The fast convergence holds in layer-wise approximations; for instance, in block diagonal approximation where each block corresponds to a layer as well as in block tri-diagonal and K-FAC approximations. We also find that a unit-wise approximation achieves the same fast convergence under some assumptions. All of these different approximations have an isotropic gradient in the function space, and this plays a fundamental role in achieving the same convergence properties in training. Thus, the current study gives a novel and unified theoretical foundation with which to understand NGD methods in deep learning.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/7AK37UK4/Karakida and Osawa - 2020 - Understanding Approximate Fisher Information for F.pdf}
}

@thesis{karaliasProbabilisticMethodsNeural2023,
  title = {Probabilistic Methods for Neural Combinatorial Optimization},
  author = {Karalias, Nikolaos},
  date = {2023},
  institution = {{EPFL}},
  location = {{Lausanne}},
  doi = {10.5075/epfl-thesis-10221},
  abstract = {The monumental progress in the development of machine learning models has led to a plethora of applications with transformative effects in engineering and science. This has also turned the attention of the research community towards the pursuit of constructing artificial intelligence (AI) models with general reasoning capabilities. Yet, despite the staggering success of artificial neural networks in a variety of tasks that involve language and image generation or object detection and recognition, tasks that involve discrete and combinatorial problem-solving are still a fundamental blind spot of those models and present a longstanding obstacle in the road to general-purpose AI systems.  Combinatorial optimization problems are prominent representatives in that category as they present fundamental challenges that are hard to tackle within the standard machine learning paradigm. Two fundamental obstacles in this pursuit are i) the difficulty of navigating exponentially large discrete configuration spaces using continuous gradient-based optimization,  ii) our inability to procure large amounts of labeled data due to the high computational budget that this requires.  The subject of this thesis will be to develop a coherent approach to combinatorial optimization with neural networks that focuses on directly tackling those challenges.  In the first half of the thesis, we will present our proposal for neural combinatorial optimization without supervision. We demonstrate how it is possible to design continuous loss functions for constrained optimization problems in a way that enables training without access to labeled data. We leverage the celebrated probabilistic method from the field of combinatorics to argue about the existence of high-quality solutions within the learned representations of a neural network that has been trained with our approach. We also show how to deterministically recover those solutions using derandomization techniques from the literature.  In the second half, we expand the scope of our inquiry and design a general framework for continuous extensions of set functions. This approach enables training neural networks for discrete problems even when the objective and the constraints of the problem are given as a black box. We develop extensions for domains like the hypercube but also higher-dimensional domains like the cone of positive semi-definite matrices.  This framework enables us to efficiently incorporate problem-specific priors in the pipeline which leads to improved empirical results. Finally, we show that the versatility of this approach extends beyond combinatorial optimization as it can be used to define a novel continuous surrogate of the discrete training error for classification problems. Overall, our proposed methods make progress in advancing the state of the art for neural combinatorial optimization through principled loss function design. Furthermore, by enabling the use of discrete functions in end-to-end differentiable models they pave the way for improved combinatorial and reasoning capabilities for machine learning algorithms},
  langid = {english},
  pagetotal = {126},
  keywords = {Combinatorial optimization,learning in higher dimensions,probabilistic method,set function extensions,unsupervised learning}
}

@online{karamiHiGenHierarchicalGraph2023a,
  title = {{{HiGen}}: {{Hierarchical Graph Generative Networks}}},
  shorttitle = {{{HiGen}}},
  author = {Karami, Mahdi},
  date = {2023-10-02},
  eprint = {2305.19337},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.19337},
  url = {http://arxiv.org/abs/2305.19337},
  urldate = {2024-02-25},
  abstract = {Most real-world graphs exhibit a hierarchical structure, which is often overlooked by existing graph generation methods. To address this limitation, we propose a novel graph generative network that captures the hierarchical nature of graphs and successively generates the graph sub-structures in a coarse-to-fine fashion. At each level of hierarchy, this model generates communities in parallel, followed by the prediction of cross-edges between communities using separate neural networks. This modular approach enables scalable graph generation for large and complex graphs. Moreover, we model the output distribution of edges in the hierarchical graph with a multinomial distribution and derive a recursive factorization for this distribution. This enables us to generate community graphs with integer-valued edge weights in an autoregressive manner. Empirical studies demonstrate the effectiveness and scalability of our proposed generative model, achieving state-of-the-art performance in terms of graph quality across various benchmark datasets. The code is available at https://github.com/Karami-m/HiGen\_main.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/IDWJXH3H/Karami - 2023 - HiGen Hierarchical Graph Generative Networks.pdf;/home/krawczuk/Zotero/storage/R4VBJYGW/2305.html}
}

@online{katakkarWhatIntegratedCircuit,
  title = {What Is an {{Integrated Circuit}}? {{Specifications}} to Tapeout},
  shorttitle = {What Is an {{Integrated Circuit}}?},
  author = {Katakkar, Tanaya},
  url = {https://www.engineersgarage.com/what-is-an-integrated-circuit-specifications-to-tapeout/},
  urldate = {2024-02-26},
  abstract = {An integrated circuit, or an IC, is a miniature circuit made up of thousands or even billions of transistors. It can be described as a set of electronic circuits fabricated on a semiconductor materia},
  langid = {american},
  organization = {{Engineers Garage}},
  file = {/home/krawczuk/Zotero/storage/L7DSYY7B/what-is-an-integrated-circuit-specifications-to-tapeout.html}
}

@inproceedings{kenarangiPySynRapidSynthesis2021,
  title = {{{PySyn}}: {{A Rapid Synthesis}} for {{Mixed-Signal Machine Learning Classification}}},
  shorttitle = {{{PySyn}}},
  booktitle = {2021 {{IEEE International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  author = {Kenarangi, Farid and Partin-Vaisband, Inna},
  date = {2021-08},
  pages = {712--717},
  issn = {1558-3899},
  doi = {10.1109/MWSCAS47672.2021.9531745},
  url = {https://ieeexplore.ieee.org/abstract/document/9531745?casa_token=haHEH4bc_YQAAAAA:EziHYGmuknS-P2eqVUGSo7yum2miETOxw1IAkiTB8ATNWw9LXSZ-9xYg4xzqyJ6FoS-IEDzb2u7u},
  urldate = {2024-02-20},
  abstract = {Mixed-signal integrated circuits (ICs) for machine learning (ML) have been demonstrated as a powerful tool for efficient and accurate classification of large volumes of complex data. Despite the growing interest in ML ICs, the design process of mixed-signal ML classifiers is dominated by ad hoc approaches. In this paper, a rapid synthesizer is developed in Python (PySyn) for designing compact power-efficient high-performance ML classifiers. Circuit-level ML library is designed and leveraged within the flow. System-level tradeoffs are generated with PySyn and utilized to iteratively adjust the ML performance. PySyn is demonstrated with a state-of-the-art classifier, generating optimized netlists under input constraints.},
  eventtitle = {2021 {{IEEE International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  keywords = {Circuit topology,Circuits and systems,classification,Integrated circuits,machine learning,Machine learning,mixed-signal,synthesis,Synthesizers,System performance,Tools},
  file = {/home/krawczuk/Zotero/storage/NCICRTH8/Kenarangi and Partin-Vaisband - 2021 - PySyn A Rapid Synthesis for Mixed-Signal Machine .pdf}
}

@online{khajenezhadGransformerTransformerbasedGraph2022a,
  title = {Gransformer: {{Transformer-based Graph Generation}}},
  shorttitle = {Gransformer},
  author = {Khajenezhad, Ahmad and Osia, Seyed Ali and Karimian, Mahmood and Beigy, Hamid},
  date = {2022-05-30},
  eprint = {2203.13655},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2203.13655},
  url = {http://arxiv.org/abs/2203.13655},
  urldate = {2024-02-25},
  abstract = {Transformers have become widely used in modern models for various tasks such as natural language processing and machine vision. This paper proposes Gransformer, an algorithm for generating graphs based on the Transformer. We extend a simple autoregressive Transformer encoder to exploit the structural information of the given graph through efficient modifications. The attention mechanism is modified to consider the presence or absence of edges between each pair of nodes. We also introduce a graph-based familiarity measure between node pairs that applies to both the attention and the positional encoding. This measure of familiarity is based on message passing algorithms and contains structural information about the graph. Furthermore, the proposed measure is autoregressive, which allows our mode to acquire the necessary conditional probabilities in a single forward pass. In the output layer, we also use a masked autoencoder for density estimation to efficiently model the sequential generation of dependent edges. Moreover, since we use BFS node orderings, we propose a technique to prevent the model from generating isolated nodes without connection to preceding nodes. We evaluate this method on two real-world datasets and compare it with other state-of-the-art autoregressive graph generation methods. Experimental results have shown that the proposed method performs comparatively to these methods, including recurrent models and graph convolutional networks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/JGM6IHCG/Khajenezhad et al. - 2022 - Gransformer Transformer-based Graph Generation.pdf;/home/krawczuk/Zotero/storage/RGAQRT88/2203.html}
}

@online{kianiHardnessLearningSymmetries2024b,
  title = {On the Hardness of Learning under Symmetries},
  author = {Kiani, Bobak T. and Le, Thien and Lawrence, Hannah and Jegelka, Stefanie and Weber, Melanie},
  date = {2024-01-03},
  eprint = {2401.01869},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2401.01869},
  url = {http://arxiv.org/abs/2401.01869},
  urldate = {2024-02-24},
  abstract = {We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries ("equivariance") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/VLEHCD5U/Kiani et al. - 2024 - On the hardness of learning under symmetries.pdf;/home/krawczuk/Zotero/storage/8Q27DSLZ/2401.html}
}

@inproceedings{kimMachineLearningFramework2021a,
  title = {Machine {{Learning Framework}} for {{Early Routability Prediction}} with {{Artificial Netlist Generator}}},
  booktitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Kim, Daeyeon and Kwon, Hyunjeong and Lee, Sung-Yun and Kim, Seungwon and Woo, Mingyu and Kang, Seokhyeong},
  date = {2021-02},
  pages = {1809--1814},
  issn = {1558-1101},
  doi = {10.23919/DATE51398.2021.9473966},
  url = {https://ieeexplore.ieee.org/abstract/document/9473966?casa_token=t0B7SSVXd0IAAAAA:1EhnWWeR6DNGZj4w3PZz2Ft2nnlfnW083G8QYqSq3uXiEVNRj0mqWOHkE1aUtKTQ5-6prWfdTrz4},
  urldate = {2024-02-19},
  abstract = {Recent routability research has exploited a machine learning (ML)-based modeling methodologies to consider various routability factors that are derived from placement solution. These factors are very related to the circuit characteristics (e.g., pin density, routing congestion, demand of routing resources, etc), and lack of circuit benchmarks in training can lead to poor predictability for ‘unseen’ circuit designs. In this paper, we propose a machine learning (ML) framework for early routability prediction modeling. The method includes a new artificial netlist generator (ANG) that generates an artificial gate-level netlist from the user-specified topology characteristics of synthetic circuit, even with real world circuit-like. In this framework, we exploit that ANG that supports obtaining ground truths for use in training ML-based model, the training dataset that have a wide range of topological characteristics provides strong ability to inference noisy, previous-unseen data. Compared to a design-specific training dataset [4] that is used for routability prediction modeling, we increase the test accuracy of binary classification (‘pass' or ‘fail’) on timing, DRC and routability by 6.3\%, 8.6\% and 6.6\%, and reduce the generalization error [12] by as much as 87\% compared to design-specific training dataset [4].},
  eventtitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {Benchmark testing,Generators,Machine learning,Predictive models,Routing,Topology,Training},
  file = {/home/krawczuk/Zotero/storage/9FCLBQTI/Kim et al. - 2021 - Machine Learning Framework for Early Routability P.pdf;/home/krawczuk/Zotero/storage/7JJNE8G4/9473966.html}
}

@online{kipfSemiSupervisedClassificationGraph2017b,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-22},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1609.02907},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2024-02-24},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/TNIH769F/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/home/krawczuk/Zotero/storage/QTLHGVRV/1609.html}
}

@online{kipfVariationalGraphAutoEncoders2016b,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-21},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1611.07308},
  url = {http://arxiv.org/abs/1611.07308},
  urldate = {2024-02-24},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/RV4ZX2MH/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf;/home/krawczuk/Zotero/storage/JCTFT3HQ/1611.html}
}

@online{KLayoutKlayoutKLayout,
  title = {{{KLayout}}/Klayout: {{KLayout Main Sources}}},
  url = {https://github.com/KLayout/klayout},
  urldate = {2024-02-26},
  file = {/home/krawczuk/Zotero/storage/TJUA7QQ3/klayout.html}
}

@online{KLayoutLayoutViewera,
  title = {{{KLayout Layout Viewer And Editor}}},
  url = {https://www.klayout.de/intro.html},
  urldate = {2024-02-26},
  file = {/home/krawczuk/Zotero/storage/F9VL6696/intro.html}
}

@article{koblahSurveyPerspectiveArtificial2023,
  title = {A {{Survey}} and {{Perspective}} on {{Artificial Intelligence}} for {{Security-Aware Electronic Design Automation}}},
  author = {Koblah, David and Acharya, Rabin and Capecci, Daniel and Dizon-Paradis, Olivia and Tajik, Shahin and Ganji, Fatemeh and Woodard, Damon and Forte, Domenic},
  date = {2023-03-06},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {28},
  number = {2},
  pages = {16:1--16:57},
  issn = {1084-4309},
  doi = {10.1145/3563391},
  url = {https://doi.org/10.1145/3563391},
  urldate = {2024-02-19},
  abstract = {Artificial intelligence (AI) and machine learning (ML) techniques have been increasingly used in several fields to improve performance and the level of automation. In recent years, this use has exponentially increased due to the advancement of high-performance computing and the ever increasing size of data. One of such fields is that of hardware design—specifically the design of digital and analog integrated circuits, where AI/ ML techniques have been extensively used to address ever-increasing design complexity, aggressive time to market, and the growing number of ubiquitous interconnected devices. However, the security concerns and issues related to integrated circuit design have been highly overlooked. In this article, we summarize the state-of-the-art in AI/ML for circuit design/optimization, security and engineering challenges, research in security-aware computer-aided design/electronic design automation, and future research directions and needs for using AI/ML for security-aware circuit design.},
  keywords = {deep learning,Integrated circuit,reinforcement learning,security primitive},
  file = {/home/krawczuk/Zotero/storage/MP4XKSSE/Koblah et al. - 2023 - A Survey and Perspective on Artificial Intelligenc.pdf}
}

@article{kofinasLatentFieldDiscovery2024,
  title = {Latent {{Field Discovery}} in {{Interacting Dynamical Systems}} with {{Neural Fields}}},
  author = {Kofinas, Miltiadis (Miltos) and Bekkers, Erik and Nagaraja, Naveen and Gavves, Efstratios},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/6521bd47ebaa28228cd6c74cb85afb65-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/GD4FZSWM/Kofinas et al. - 2024 - Latent Field Discovery in Interacting Dynamical Sy.pdf}
}

@inproceedings{kohlerSmoothNormalizingFlows2021,
  title = {Smooth {{Normalizing Flows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Köhler, Jonas and Krämer, Andreas and Noe, Frank},
  date = {2021},
  volume = {34},
  pages = {2796--2809},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/167434fa6219316417cd4160c0c5e7d2-Abstract.html},
  urldate = {2024-02-22},
  abstract = {Normalizing flows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art flows accurately approximate distributions and energies, applications in physics additionally require smooth energies to compute forces and higher-order derivatives. Furthermore, such densities are often defined on non-trivial topologies. A recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. In this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori.Mixture transformations employ root-finding methods to invert them in practice, which has so far prevented bi-directional flow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem.We demonstrate two advantages of such smooth flows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YDAA8FTN/Köhler et al. - 2021 - Smooth Normalizing Flows.pdf}
}

@software{kolheBhushanKolhe1920Project_Automated_Circuit_To_VLSI_Layout2023,
  title = {{{BhushanKolhe1920}}/project\_{{Automated}}\_{{Circuit}}\_{{To}}\_{{VLSI}}\_{{Layout}}},
  author = {Kolhe, Bhushan},
  date = {2023-12-27T09:54:56Z},
  origdate = {2020-06-10T14:20:07Z},
  url = {https://github.com/BhushanKolhe1920/project_Automated_Circuit_To_VLSI_Layout},
  urldate = {2024-02-26},
  abstract = {Project Name: Automated Circuit To MAGIC VLSI layout Using Open Source EDA Tools}
}

@inproceedings{korattikaraAusterityMCMCLand2014,
  title = {Austerity in {{MCMC Land}}: {{Cutting}} the {{Metropolis-Hastings Budget}}},
  shorttitle = {Austerity in {{MCMC Land}}},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Korattikara, Anoop and Chen, Yutian and Welling, Max},
  date = {2014-01-27},
  pages = {181--189},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v32/korattikara14.html},
  urldate = {2024-02-25},
  abstract = {Can we make Bayesian posterior MCMC sampling more efficient when faced with very large datasets? We argue that computing the likelihood for N datapoints in the Metropolis-Hastings (MH) test to reach a single binary decision is computationally inefficient. We introduce an approximate MH rule based on a sequential hypothesis test that allows us to accept or reject samples with high confidence using only a fraction of the data required for the exact MH rule. While this method introduces an asymptotic bias, we show that this bias can be controlled and is more than offset by a decrease in variance due to our ability to draw more samples per unit of time.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/9KHH4JE7/Korattikara et al. - 2014 - Austerity in MCMC Land Cutting the Metropolis-Has.pdf}
}

@article{korhonenSingleExponentialTime2Approximation2023,
  title = {A {{Single-Exponential Time}} 2-{{Approximation Algorithm}} for {{Treewidth}}},
  author = {Korhonen, Tuukka},
  date = {2023-11-14},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  pages = {FOCS21-174},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/22M147551X},
  url = {https://epubs.siam.org/doi/full/10.1137/22M147551X},
  urldate = {2024-02-20},
  abstract = {We give an algorithm that for an input \$n\$-vertex graph \$G\$ and integer \$k{$>$}0\$, in time \$2\^{}\{O(k)\} n\$, either outputs that the treewidth of \$G\$ is larger than \$k\$, or gives a tree decomposition of \$G\$ of width at most \$5k+4\$.  This is the first algorithm providing a constant factor approximation for treewidth which runs in time single exponential in \$k\$ and linear in \$n\$. Treewidth-based computations are subroutines of numerous algorithms.  Our algorithm can be used to speed up many such algorithms to work in time which is single exponential in the treewidth and linear in the input size.},
  pagetotal = {FOCS21-194},
  file = {/home/krawczuk/Zotero/storage/ZUNNL6SA/Korhonen - 2023 - A Single-Exponential Time 2-Approximation Algorith.pdf}
}

@article{kostyukhinImprovingPrimaryvertexReconstruction2023,
  title = {Improving Primary-Vertex Reconstruction with a Minimum-Cost Lifted Multicut Graph Partitioning Algorithm},
  author = {Kostyukhin, V. and Keuper, M. and Ibragimov, I. and Owtscharenko, N. and Cristinziani, M.},
  date = {2023-07-01},
  journaltitle = {Journal of Instrumentation},
  shortjournal = {J. Inst.},
  volume = {18},
  number = {07},
  pages = {P07013},
  issn = {1748-0221},
  doi = {10.1088/1748-0221/18/07/P07013},
  url = {https://iopscience.iop.org/article/10.1088/1748-0221/18/07/P07013},
  urldate = {2024-02-26},
  abstract = {Particle physics experiments often require the simultaneous reconstruction of many interaction vertices. Usually, this problem is solved by ad hoc heuristic algorithms. We propose a universal approach to address the multiple vertex finding through a principled formulation as a minimum-cost lifted multicut problem. The suggested algorithm is tested in a typical LHC environment with multiple proton-proton interaction vertices. Reconstruction errors caused by the particle detectors complicate the solution and require the introduction of special metrics to assess the vertex-finding performance. We demonstrate that the minimum-cost lifted multicut approach outperforms heuristic algorithms and works well up to the highest vertex multiplicity expected at the LHC.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/89ZC3979/Kostyukhin et al. - 2023 - Improving primary-vertex reconstruction with a min.pdf}
}

@inproceedings{krawczukGGGANGeometricGraph2020,
  title = {{{GG-GAN}}: {{A}} Geometric Graph Generative Adversarial Network, 2021},
  shorttitle = {{{GG-GAN}}},
  booktitle = {{{URL}} {{https://openreview.}} Net/Forum},
  author = {Krawczuk, Igor and Abranches, Pedro and Loukas, Andreas and Cevher, Volkan},
  date = {2020},
  keywords = {⛔ No DOI found}
}

@inproceedings{krawczukGGGANGeometricGraph2020a,
  title = {{{GG-GAN}}: {{A}} Geometric Graph Generative Adversarial Network, 2021},
  shorttitle = {{{GG-GAN}}},
  booktitle = {{{URL}} {{https://openreview.}} Net/Forum},
  author = {Krawczuk, Igor and Abranches, Pedro and Loukas, Andreas and Cevher, Volkan},
  date = {2020},
  url = {https://scholar.google.com/scholar?cluster=3169180068992793390&hl=en&oi=scholarr},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found}
}

@inproceedings{krinkeConstraintsTapeOutContinuous2019,
  title = {From {{Constraints}} to {{Tape-Out}}: {{Towards}} a {{Continuous AMS Design Flow}}},
  shorttitle = {From {{Constraints}} to {{Tape-Out}}},
  booktitle = {2019 {{IEEE}} 22nd {{International Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} \& {{Systems}} ({{DDECS}})},
  author = {Krinke, Andreas and Horst, Tilman and Gläser, Georg and Grabmann, Martin and Markus, Tobias and Prautsch, Benjamin and Hatnik, Uwe and Lienig, Jens},
  date = {2019-04},
  pages = {1--10},
  issn = {2473-2117},
  doi = {10.1109/DDECS.2019.8724669},
  url = {https://ieeexplore.ieee.org/document/8724669},
  urldate = {2024-02-19},
  abstract = {The effort in designing analog/mixed-signal (AMS) integrated circuits is characterized by the largely manual work involved in the design of analog cells and their integration into the overall circuit. This inequality in effort between analog and digital cells increases with the use of modern, more complex technology nodes. To mitigate this problem, this paper presents four methods to improve existing mixed-signal design flows: (1) automatic schematic generation from a system-level model, (2) flexible automatic analog layout generation, (3) constraint propagation and budget calculation for dependency resolution, and (4) verification of nonfunctional effects. The implementation of these steps results in a novel AMS design flow with a significantly higher degree of automation.},
  eventtitle = {2019 {{IEEE}} 22nd {{International Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} \& {{Systems}} ({{DDECS}})},
  keywords = {Automation,Computer architecture,Hardware design languages,Layout,Libraries,Phase locked loops,Tools},
  file = {/home/krawczuk/Zotero/storage/GJJCN9GP/Krinke et al. - 2019 - From Constraints to Tape-Out Towards a Continuous.pdf;/home/krawczuk/Zotero/storage/2UTNK387/8724669.html}
}

@online{kuImagenHubStandardizingEvaluation2023,
  title = {{{ImagenHub}}: {{Standardizing}} the Evaluation of Conditional Image Generation Models},
  shorttitle = {{{ImagenHub}}},
  author = {Ku, Max and Li, Tianle and Zhang, Kai and Lu, Yujie and Fu, Xingyu and Zhuang, Wenwen and Chen, Wenhu},
  date = {2023-12-03},
  eprint = {2310.01596},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.01596},
  url = {http://arxiv.org/abs/2310.01596},
  urldate = {2024-02-25},
  abstract = {Recently, a myriad of conditional image generation and editing models have been developed to serve different downstream tasks, including text-to-image generation, text-guided image editing, subject-driven image generation, control-guided image generation, etc. However, we observe huge inconsistencies in experimental conditions: datasets, inference, and evaluation metrics - render fair comparisons difficult. This paper proposes ImagenHub, which is a one-stop library to standardize the inference and evaluation of all the conditional image generation models. Firstly, we define seven prominent tasks and curate high-quality evaluation datasets for them. Secondly, we built a unified inference pipeline to ensure fair comparison. Thirdly, we design two human evaluation scores, i.e. Semantic Consistency and Perceptual Quality, along with comprehensive guidelines to evaluate generated images. We train expert raters to evaluate the model outputs based on the proposed metrics. Our human evaluation achieves a high inter-worker agreement of Krippendorff's alpha on 76\% models with a value higher than 0.4. We comprehensively evaluated a total of around 30 models and observed three key takeaways: (1) the existing models' performance is generally unsatisfying except for Text-guided Image Generation and Subject-driven Image Generation, with 74\% models achieving an overall score lower than 0.5. (2) we examined the claims from published papers and found 83\% of them hold with a few exceptions. (3) None of the existing automatic metrics has a Spearman's correlation higher than 0.2 except subject-driven image generation. Moving forward, we will continue our efforts to evaluate newly published models and update our leaderboard to keep track of the progress in conditional image generation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Multimedia},
  file = {/home/krawczuk/Zotero/storage/FW48IV25/Ku et al. - 2023 - ImagenHub Standardizing the evaluation of conditi.pdf;/home/krawczuk/Zotero/storage/TAZX8NDB/2310.html}
}

@online{kuImagenHubStandardizingEvaluation2023a,
  title = {{{ImagenHub}}: {{Standardizing}} the Evaluation of Conditional Image Generation Models},
  shorttitle = {{{ImagenHub}}},
  author = {Ku, Max and Li, Tianle and Zhang, Kai and Lu, Yujie and Fu, Xingyu and Zhuang, Wenwen and Chen, Wenhu},
  date = {2023-12-03},
  eprint = {2310.01596},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.01596},
  url = {http://arxiv.org/abs/2310.01596},
  urldate = {2024-02-25},
  abstract = {Recently, a myriad of conditional image generation and editing models have been developed to serve different downstream tasks, including text-to-image generation, text-guided image editing, subject-driven image generation, control-guided image generation, etc. However, we observe huge inconsistencies in experimental conditions: datasets, inference, and evaluation metrics - render fair comparisons difficult. This paper proposes ImagenHub, which is a one-stop library to standardize the inference and evaluation of all the conditional image generation models. Firstly, we define seven prominent tasks and curate high-quality evaluation datasets for them. Secondly, we built a unified inference pipeline to ensure fair comparison. Thirdly, we design two human evaluation scores, i.e. Semantic Consistency and Perceptual Quality, along with comprehensive guidelines to evaluate generated images. We train expert raters to evaluate the model outputs based on the proposed metrics. Our human evaluation achieves a high inter-worker agreement of Krippendorff's alpha on 76\% models with a value higher than 0.4. We comprehensively evaluated a total of around 30 models and observed three key takeaways: (1) the existing models' performance is generally unsatisfying except for Text-guided Image Generation and Subject-driven Image Generation, with 74\% models achieving an overall score lower than 0.5. (2) we examined the claims from published papers and found 83\% of them hold with a few exceptions. (3) None of the existing automatic metrics has a Spearman's correlation higher than 0.2 except subject-driven image generation. Moving forward, we will continue our efforts to evaluate newly published models and update our leaderboard to keep track of the progress in conditional image generation.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics,Computer Science - Multimedia},
  file = {/home/krawczuk/Zotero/storage/XRJCYCMQ/Ku et al. - 2023 - ImagenHub Standardizing the evaluation of conditi.pdf;/home/krawczuk/Zotero/storage/82WELBKW/2310.html}
}

@inproceedings{kunalGANAGraphConvolutional2020e,
  title = {{{GANA}}: {{Graph Convolutional Network Based Automated Netlist Annotation}} for {{Analog Circuits}}},
  shorttitle = {{{GANA}}},
  booktitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Kunal, Kishor and Dhar, Tonmoy and Madhusudan, Meghna and Poojary, Jitesh and Sharma, Arvind and Xu, Wenbin and Burns, Steven M. and Hu, Jiang and Harjani, Ramesh and Sapatnekar, Sachin S.},
  date = {2020-03},
  pages = {55--60},
  issn = {1558-1101},
  doi = {10.23919/DATE48585.2020.9116329},
  url = {https://ieeexplore.ieee.org/abstract/document/9116329?casa_token=pmDFJicmP08AAAAA:1rCal6CTB7-Eamcy3AvSogNocH1GGEo4qlxNM7-egouIg952ujAHMwdEnWLA-IPDRraV4uaxiUeh},
  urldate = {2024-02-19},
  abstract = {Automated subcircuit identification and annotation enables the creation of hierarchical representations of analog netlists, and can facilitate a variety of design automation tasks such as circuit layout and optimization. Subcircuit identification must navigate the numerous alternative structures that can implement any analog function, but traditional graph-based methods cannot easily identify the large number of such structural variants. The novel approach in this paper is based on the use of a trained graph convolutional neural network (GCN) that identifies netlist elements for circuit blocks at upper levels of the design hierarchy. Structures at lower levels of hierarchy are identified using graph-based algorithms. The proposed recognition scheme organically detects layout constraints, such as symmetry and matching, whose identification is essential for high-quality hierarchical layout. The subcircuit identification method demonstrates a high degree of accuracy over a wide range of analog designs, successfully identifies larger circuits that contain subblocks such as OTAs, LNAs, mixers, oscillators, and band-pass filters, and provides hierarchical decompositions of such circuits.},
  eventtitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {Annotations,Bipartite graph,Convolution,Current mirrors,Layout,Topology,Transistors},
  file = {/home/krawczuk/Zotero/storage/QVGYP4WI/Kunal et al. - 2020 - GANA Graph Convolutional Network Based Automated .pdf;/home/krawczuk/Zotero/storage/5SV8PYA2/9116329.html}
}

@inproceedings{kupriyanovHighSpeedEventDrivenRTL2004,
  title = {High-{{Speed Event-Driven RTL Compiled Simulation}}},
  booktitle = {Computer {{Systems}}: {{Architectures}}, {{Modeling}}, and {{Simulation}}},
  author = {Kupriyanov, Alexey and Hannig, Frank and Teich, Jürgen},
  editor = {Pimentel, Andy D. and Vassiliadis, Stamatis},
  date = {2004},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {519--529},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-27776-7_53},
  abstract = {In this paper we present a new approach for generating high-speed optimized event-driven register transfer level (RTL) compiled simulators. The generation of the simulators is part of our BUILDABONG [7] framework, which aims at architecture and compiler co-generation for special purpose processors. The main focus of the paper is on the transformation of a given architecture’s circuit into a graph and applying on it an essential graph decomposition algorithm to transform the graph into subgraphs denoting the minimal subsets of sequential elements which have to be reevaluated during each simulation cycle. As a second optimization, we present a partitioning algorithm, which introduces intermediate registers to minimize the number of evaluations of combinational nodes during a simulation cycle. The simulator’s superior performance compared to an existing commercial simulator is shown. Finally, we demonstrate the pertinence of our approach by simulating a MIPS processor.},
  isbn = {978-3-540-27776-7},
  langid = {english},
  keywords = {Abstract State Machine,Graph Decomposition,Register Transfer,Simulation Cycle,Simulation Speed},
  file = {/home/krawczuk/Zotero/storage/JFY9NP3C/Kupriyanov et al. - 2004 - High-Speed Event-Driven RTL Compiled Simulation.pdf}
}

@article{kuznetsovMolGrowGraphNormalizing2021a,
  title = {{{MolGrow}}: {{A Graph Normalizing Flow}} for {{Hierarchical Molecular Generation}}},
  shorttitle = {{{MolGrow}}},
  author = {Kuznetsov, Maksim and Polykovskiy, Daniil},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {9},
  pages = {8226--8234},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i9.17001},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17001},
  urldate = {2024-02-22},
  abstract = {We propose a hierarchical normalizing flow model for generating molecular graphs. The model produces new molecular structures from a single-node graph by recursively splitting every node into two. All operations are invertible and can be used as plug-and-play modules. The hierarchical nature of the latent codes allows for precise changes in the resulting graph: perturbations in the first layer cause global structural changes, while perturbations in the consequent layers change the resulting molecule only marginally. Proposed model outperforms existing generative graph models on the distribution learning task. We also show successful experiments on global and constrained optimization of chemical properties using latent codes of the model.},
  issue = {9},
  langid = {english},
  keywords = {Graph-based Machine Learning},
  file = {/home/krawczuk/Zotero/storage/VPR36Y2S/Kuznetsov and Polykovskiy - 2021 - MolGrow A Graph Normalizing Flow for Hierarchical.pdf}
}

@article{lafonDiffusionMapsCoarsegraining2006,
  title = {Diffusion Maps and Coarse-Graining: A Unified Framework for Dimensionality Reduction, Graph Partitioning, and Data Set Parameterization},
  shorttitle = {Diffusion Maps and Coarse-Graining},
  author = {Lafon, S. and Lee, A.B.},
  date = {2006-09},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {28},
  number = {9},
  pages = {1393--1403},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2006.184},
  url = {https://ieeexplore.ieee.org/abstract/document/1661543?casa_token=r7SGqxty3uAAAAAA:v_U04YujQFSbPp3lYzprOgaQAX7V-oRuEKCD8jW3udjNolVinnLfat0k5oq8RL-JqSmnowKBpM18},
  urldate = {2024-02-22},
  abstract = {We provide evidence that nonlinear dimensionality reduction, clustering, and data set parameterization can be solved within one and the same framework. The main idea is to define a system of coordinates with an explicit metric that reflects the connectivity of a given data set and that is robust to noise. Our construction, which is based on a Markov random walk on the data, offers a general scheme of simultaneously reorganizing and subsampling graphs and arbitrarily shaped data sets in high dimensions using intrinsic geometry. We show that clustering in embedding spaces is equivalent to compressing operators. The objective of data partitioning and clustering is to coarse-grain the random walk on the data while at the same time preserving a diffusion operator for the intrinsic geometry or connectivity of the data set up to some accuracy. We show that the quantization distortion in diffusion space bounds the error of compression of the operator, thus giving a rigorous justification for k-means clustering in diffusion space and a precise measure of the performance of general clustering algorithms.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {clustering,Clustering algorithms,clustering similarity measures,compression (coding),Distortion measurement,Eigenvalues and eigenfunctions,Extraterrestrial measurements,Geometry,graph algorithms.,graph-theoretic methods,information visualization,knowledge retrieval,Machine learning,Markov processes,Noise robustness,Noise shaping,Nonlinear distortion,quantization,Quantization,text analysis,Text analysis},
  file = {/home/krawczuk/Zotero/storage/2SA869Z3/Lafon and Lee - 2006 - Diffusion maps and coarse-graining a unified fram.pdf;/home/krawczuk/Zotero/storage/NFX3P5LK/1661543.html}
}

@online{LateBreakingResults,
  title = {Late {{Breaking Results}}: {{Attention}} in {{Graph2Seq Neural Networks}} towards {{Push-Button Analog IC Placement}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9586177?casa_token=YkwAi19gql0AAAAA:jq63XhXbWzGuT8FYjFwMbchwC2Fh-5PE30Hte_oACun-3RT6F2J4wWreex2NvwvfFYzHpyazN82m},
  urldate = {2024-02-20}
}

@inproceedings{latorreFindingActualDescent2023,
  title = {Finding Actual Descent Directions for Adversarial Training},
  booktitle = {11th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Latorre, Fabian and Krawczuk, Igor and Dadi, Leello Tadesse and Pethick, Thomas Michaelsen and Cevher, Volkan},
  date = {2023},
  url = {https://infoscience.epfl.ch/record/300850},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/CL524EQA/Latorre et al. - 2023 - Finding actual descent directions for adversarial .pdf}
}

@online{LAYGO2CustomLayout,
  title = {{{LAYGO2}}: {{A Custom Layout Generation Engine Based}} on {{Dynamic Templates}} and {{Grids}} for {{Advanced CMOS Technologies}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10182288?casa_token=utSQvNQevNYAAAAA:sg0l8MYAatz66IZkNNcB2dfu8Th4eXuMD7makdOnI7dAOYt8G0vY_ODoJOyJ3wtTh2dV52u_z8XS},
  urldate = {2024-02-26},
  file = {/home/krawczuk/Zotero/storage/TZ226FVQ/10182288.html}
}

@online{LearningGraphTopological,
  title = {Learning {{Graph Topological Features}} via {{GAN}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/8638941},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/WKZKG7DS/8638941.html}
}

@incollection{leeSemiconductorManufacturingAutomation2023,
  title = {Semiconductor {{Manufacturing Automation}}},
  booktitle = {Springer {{Handbook}} of {{Automation}}},
  author = {Lee, Tae-Eog and Kim, Hyun-Jung and Yu, Tae-Sun},
  editor = {Nof, Shimon Y.},
  date = {2023},
  series = {Springer {{Handbooks}}},
  pages = {841--863},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-96729-1_38},
  url = {https://doi.org/10.1007/978-3-030-96729-1_38},
  urldate = {2024-02-19},
  abstract = {We review automation requirements and technologies for semiconductor manufacturing. We first discuss equipment integration architectures and control to meet automation requirements for modern fabs. We explain tool architectures and operational issues for modern integrated tools such as cluster tools, which combine several processing modules with wafer-handling robots. We then review recent progress in tool science for scheduling and control of integrated tools and discuss control software architecture, design, and development for integrated tools. Next, we discuss requirements and technologies in fab integration architectures and operation such as modern fab architectures and automated material-handling systems, communication architecture and networking, fab control application integration, and fab control and management. We finally discuss recent trends on smart fabs with machine learning.},
  isbn = {978-3-030-96729-1},
  langid = {english},
  keywords = {Assembly process,Cyclic schedule,Fab architecture,Semiconductor equipment,Semiconductor manufacturing system,Smart fab,Wafer fabrication process}
}

@article{leeSupervisedPretrainingCan2023a,
  title = {Supervised {{Pretraining Can Learn In-Context Reinforcement Learning}}},
  author = {Lee, Jonathan and Xie, Annie and Pacchiano, Aldo and Chandak, Yash and Finn, Chelsea and Nachum, Ofir and Brunskill, Emma},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8644b61a9bc87bf7844750a015feb600-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/R5LVPVWB/Lee et al. - 2023 - Supervised Pretraining Can Learn In-Context Reinfo.pdf}
}

@online{lemaireSANGEAScalableAttributed2023,
  title = {{{SANGEA}}: {{Scalable}} and {{Attributed Network Generation}}},
  shorttitle = {{{SANGEA}}},
  author = {Lemaire, Valentin and Achenchabe, Youssef and Ody, Lucas and Souid, Houssem Eddine and Aversano, Gianmarco and Posocco, Nicolas and Skhiri, Sabri},
  date = {2023-09-27},
  eprint = {2309.15648},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.15648},
  url = {http://arxiv.org/abs/2309.15648},
  urldate = {2024-02-25},
  abstract = {The topic of synthetic graph generators (SGGs) has recently received much attention due to the wave of the latest breakthroughs in generative modelling. However, many state-of-the-art SGGs do not scale well with the graph size. Indeed, in the generation process, all the possible edges for a fixed number of nodes must often be considered, which scales in \$\textbackslash mathcal\{O\}(N\^{}2)\$, with \$N\$ being the number of nodes in the graph. For this reason, many state-of-the-art SGGs are not applicable to large graphs. In this paper, we present SANGEA, a sizeable synthetic graph generation framework which extends the applicability of any SGG to large graphs. By first splitting the large graph into communities, SANGEA trains one SGG per community, then links the community graphs back together to create a synthetic large graph. Our experiments show that the graphs generated by SANGEA have high similarity to the original graph, in terms of both topology and node feature distribution. Additionally, these generated graphs achieve high utility on downstream tasks such as link prediction. Finally, we provide a privacy assessment of the generated graphs to show that, even though they have excellent utility, they also achieve reasonable privacy scores.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GKVUJ5CW/Lemaire et al. - 2023 - SANGEA Scalable and Attributed Network Generation.pdf;/home/krawczuk/Zotero/storage/UPWHJ23R/2309.html}
}

@online{lezcano-casadoAdaptiveMomentumMethods2020,
  title = {Adaptive and {{Momentum Methods}} on {{Manifolds Through Trivializations}}},
  author = {Lezcano-Casado, Mario},
  date = {2020-10-09},
  eprint = {2010.04617},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2010.04617},
  url = {http://arxiv.org/abs/2010.04617},
  urldate = {2024-02-23},
  abstract = {Adaptive methods do not have a direct generalization to manifolds as the adaptive term is not invariant. Momentum methods on manifolds suffer from efficiency problems stemming from the curvature of the manifold. We introduce a framework to generalize adaptive and momentum methods to arbitrary manifolds by noting that for every differentiable manifold, there exists a radially convex open set that covers almost all the manifold. Being radially convex, this set is diffeomorphic to \$\textbackslash mathbb\{R\}\^{}n\$. This gives a natural generalization of any adaptive and momentum-based algorithm to a set that covers almost all the manifold in an arbitrary manifolds. We also show how to extend these methods to the context of gradient descent methods with a retraction. For its implementation, we bring an approximation to the exponential of matrices that needs just of 5 matrix multiplications, making it particularly efficient on GPUs. In practice, we see that this family of algorithms closes the numerical gap created by an incorrect use of momentum and adaptive methods on manifolds. At the same time, we see that the most efficient algorithm of this family is given by simply pulling back the problem to the tangent space at the initial point via the exponential map.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/home/krawczuk/Zotero/storage/M3VBANV3/Lezcano-Casado - 2020 - Adaptive and Momentum Methods on Manifolds Through.pdf;/home/krawczuk/Zotero/storage/EM73HRA7/2010.html}
}

@online{lezcano-casadoGeometricOptimisationManifolds2022a,
  title = {Geometric {{Optimisation}} on {{Manifolds}} with {{Applications}} to {{Deep Learning}}},
  author = {Lezcano-Casado, Mario},
  date = {2022-03-09},
  eprint = {2203.04794},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2203.04794},
  url = {http://arxiv.org/abs/2203.04794},
  urldate = {2024-02-23},
  abstract = {We design and implement a Python library to help the non-expert using all these powerful tools in a way that is efficient, extensible, and simple to incorporate into the workflow of the data scientist, practitioner, and applied researcher. The algorithms implemented in this library have been designed with usability and GPU efficiency in mind, and they can be added to any PyTorch model with just one extra line of code. We showcase the effectiveness of these tools on an application of optimisation on manifolds in the setting of time series analysis. In this setting, orthogonal and unitary optimisation is used to constraint and regularise recurrent models and avoid vanishing and exploding gradient problems. The algorithms designed for GeoTorch allow us to achieve state of the art results in the standard tests for this family of models. We use tools from comparison geometry to give bounds on quantities that are of interest in optimisation problems. In particular, we build on the work of (Kaul 1976) to give explicit bounds on the norm of the second derivative of the Riemannian exponential.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/krawczuk/Zotero/storage/F9GNQP5J/Lezcano-Casado - 2022 - Geometric Optimisation on Manifolds with Applicati.pdf;/home/krawczuk/Zotero/storage/WUWWENHG/2203.html}
}

@online{lHowDesignProcess2019,
  title = {How Is the {{Design Process}} of {{Microchips}}: {{Analog IC Design Flow}} to {{Tapeout}}},
  shorttitle = {How Is the {{Design Process}} of {{Microchips}}},
  author = {L, Alberto},
  date = {2019-09-01T11:09:56+00:00},
  url = {https://miscircuitos.com/design-process-of-chips-asics-flow-from-design-to-tapeout/},
  urldate = {2024-02-19},
  abstract = {Flowchart with the Analog IC Design Flow from the architecture to tape out. An overview of how ASIC integrated circuits chips are designed and fabricated.},
  langid = {american},
  organization = {{MisCircuitos.com}},
  file = {/home/krawczuk/Zotero/storage/DUHCBXHV/design-process-of-chips-asics-flow-from-design-to-tapeout.html}
}

@article{liAdaptiveLayoutDecomposition2022a,
  title = {Adaptive {{Layout Decomposition With Graph Embedding Neural Networks}}},
  author = {Li, Wei and Ma, Yuzhe and Lin, Yibo and Yu, Bei},
  date = {2022-11},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {11},
  pages = {5030--5042},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2022.3140729},
  url = {https://ieeexplore.ieee.org/abstract/document/9672139?casa_token=H7aXYoXqoRsAAAAA:FV_lv-zfdfH0hasodGZR4fYBmowWv7vRcyeYF5q8oJCYfI_5_HAH-UaEFLsFAqxVzSHRw3xTiTuQ},
  urldate = {2024-02-22},
  abstract = {Multiple patterning layout decomposition (MPLD) has been widely investigated, but so far there is no decomposer that dominates others in terms of both result quality and efficiency. This observation motivates us to explore how to adaptively select the most suitable MPLD strategy for a given layout graph, which is nontrivial and still an open problem. In this article, we propose a layout decomposition framework based on graph convolutional networks to obtain the graph embeddings of the layout. The graph embeddings are used for graph library construction, decomposer selection, graph matching, stitch removal prediction, and graph coloring. In addition, we design a fast nonstitch layout decomposition algorithm that purely depends on the message passing graph neural network. The experimental results show that our graph embedding-based framework can achieve optimal decompositions in the widely used benchmark with a significant runtime drop even compared with fast but nonoptimal heuristics.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Adaptation models,Color,Costs,Design methodology,Layout,layout decomposition,Libraries,Lithography,Runtime,VLSI design},
  file = {/home/krawczuk/Zotero/storage/P9FNYHG8/Li et al. - 2022 - Adaptive Layout Decomposition With Graph Embedding.pdf;/home/krawczuk/Zotero/storage/EWLQL9E8/9672139.html}
}

@article{liangMILEMultiLevelFramework2021,
  title = {{{MILE}}: {{A Multi-Level Framework}} for {{Scalable Graph Embedding}}},
  shorttitle = {{{MILE}}},
  author = {Liang, Jiongqian and Gurukar, Saket and Parthasarathy, Srinivasan},
  date = {2021-05-22},
  journaltitle = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {15},
  pages = {361--372},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v15i1.18067},
  url = {https://ojs.aaai.org/index.php/ICWSM/article/view/18067},
  urldate = {2024-02-22},
  abstract = {Recently there has been a surge of interest in designing graph embedding methods. Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements. In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework – a generic methodology allowing contemporary graph embedding methods to scale to large graphs. MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph. It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a graph convolution neural network that it learns. The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them. We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs. Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while generating embeddings of better quality, for the task of node classification. MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation. Our code and data are publicly available with detailed instructions for adding new base embedding methods: https://github.com/jiongqian/MILE.},
  langid = {english},
  keywords = {communities identification,expertise and authority discovery,Social network analysis},
  file = {/home/krawczuk/Zotero/storage/3LTASK4A/Liang et al. - 2021 - MILE A Multi-Level Framework for Scalable Graph E.pdf}
}

@inproceedings{liaoEfficientGraphGeneration2019e,
  title = {Efficient {{Graph Generation}} with {{Graph Recurrent Attention Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Hamilton, Will and Duvenaud, David K and Urtasun, Raquel and Zemel, Richard},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d0921d442ee91b896ad95059d13df618-Abstract.html},
  urldate = {2024-02-25},
  abstract = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block.  Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. Our code is released at: \textbackslash url\{https://github.com/lrjconan/GRAN\}.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/6Q6VZIVU/Liao et al. - 2019 - Efficient Graph Generation with Graph Recurrent At.pdf}
}

@online{LibraryfreeStructureRecognition,
  title = {Library-Free {{Structure Recognition}} for {{Analog Circuits}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9474102?casa_token=g6W7e4QDJ-8AAAAA:AWFq8MmUWRffGreMfbsiF2pS-zHiO3Jgr9WB5sTsQmwyIntwp7CcyA62Tc7A4QvKAnDdUeQZFh8P},
  urldate = {2024-02-20}
}

@inproceedings{liBreakingSampleSize2020a,
  title = {Breaking the {{Sample Size Barrier}} in {{Model-Based Reinforcement Learning}} with a {{Generative Model}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Gen and Wei, Yuting and Chi, Yuejie and Gu, Yuantao and Chen, Yuxin},
  date = {2020},
  volume = {33},
  pages = {12861--12872},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/96ea64f3a1aa2fd00c72faacf0cb8ac9-Abstract.html},
  urldate = {2024-02-26},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/4CHUZ558/Li et al. - 2020 - Breaking the Sample Size Barrier in Model-Based Re.pdf}
}

@incollection{lienigAddressingReliabilityPhysical2020,
  title = {Addressing {{Reliability}} in {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {257--302},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_7},
  url = {https://doi.org/10.1007/978-3-030-39284-0_7},
  urldate = {2024-02-19},
  abstract = {Reliability of electronic circuits is becoming an increasing concern due to the ongoing downscaling of the structural dimensions and the continuous increase in performance requirements. This final chapter addresses the many options available to a layout designer, given the enormous influence of physical design on circuit reliability. Hence, the goal of this chapter is to summarize the state of the art in reliability-driven physical design and related mitigating measures. We start by presenting reliability issues that can lead to temporary circuit malfunctions. We discuss in this context parasitic effects in the bulk of silicon (Sect. 7.1), at its surface (Sect. 7.2), and in the interconnect layers (Sect. 7.3). Our main goal is to show how these effects can be suppressed through appropriate layout measures. After having presented temporarily-induced malfunctions and their mitigation options, we discuss the growing challenges of preventing ICs from irreversible damage. This requires the investigation of overvoltage events (Sect. 7.4) and migration processes, such as electromigration, thermal and stress migration (Sect. 7.5). Again, not only do we discuss the physical background to this damage, we also present appropriate mitigation measures.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigBridgesTechnologyInterfaces2020,
  title = {Bridges to {{Technology}}: {{Interfaces}}, {{Design Rules}}, and {{Libraries}}},
  shorttitle = {Bridges to {{Technology}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {83--126},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_3},
  url = {https://doi.org/10.1007/978-3-030-39284-0_3},
  urldate = {2024-02-19},
  abstract = {Having presented fabrication technology for IC chips in Chap. 2, we now investigate in detail an important aspect of the physical design process: data interfaces. We introduce circuit, layout and mask data structures, that is, the main input and output data in the design steps, in this chapter. First, we explain the input to physical design—circuit data—while focusing on schematics and netlists (Sect. 3.1); we then discuss the output of the physical design step: layout data such as layers and polygons (Sect. 3.2). Mask data, which are the data required by the foundry and generated at the end of the design process, are described in Sect. 3.3. Here, we introduce “layout post processing”, where amendments and additions to the chip layout data are performed in order to convert a physical layout into data for mask production. Technology data, provided by the chip manufacturing foundry, are crucial for producing the physical design. An important portion of these data are technological constraints which are modeled in the geometrical design rules used in physical design. Essentially, geometrical design rules are constraints for physical design, whose compliance ensures the manufacturability of the layout results. Geometrical design rules are presented in detail in Sect. 3.4. Technology data are organized in libraries. These libraries, which are extensively used in IC and PCB design, are covered in our final Sect. 3.5.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@book{lienigFundamentalsLayoutDesign2020,
  title = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0},
  url = {http://link.springer.com/10.1007/978-3-030-39284-0},
  urldate = {2024-02-19},
  isbn = {978-3-030-39283-3 978-3-030-39284-0},
  langid = {english},
  keywords = {Analog layout,Design rules,Failure mechanisms,Layout constraints,Layout design,Layout generators,Matching,Parasitic effects,Physical design automation,Semiconductor fabrication},
  file = {/home/krawczuk/Zotero/storage/86TKUXRJ/Lienig and Scheible - 2020 - Fundamentals of Layout Design for Electronic Circu.pdf}
}

@incollection{lienigIntroduction2020,
  title = {Introduction},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {1--29},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_1},
  url = {https://doi.org/10.1007/978-3-030-39284-0_1},
  urldate = {2024-02-19},
  abstract = {This chapter gives a sound introduction to the technologies, tasks and methodologies used to design the layout of an electronic circuit. With this basic design knowledge as a foundation, the subsequent chapters then delve deeper into specific constraints and aspects of physical design, such as semiconductor technologies (Chap. 2 ), interfaces, design rules and libraries (Chap. 3 ), design flows and models (Chap. 4 ), design steps (Chap. 5 ), analog design specifics (Chap. 6 ), and finally reliability measures (Chap. 7 ). In Sect. 1.1, we introduce several of the most common fabrication technologies for electronic systems. The central topic of this book is the physical design of integrated circuits (aka chips, ICs) but hybrid technologies and printed circuit boards (PCBs) are also considered. In Sect. 1.2 of our introduction, we examine in more detail the significance and peculiarities of this related branch of modern electronics—also known as microelectronics. In Sect. 1.3, we then consider the physical design of both integrated circuits and printed circuits boards with a specific emphasis on their primary design steps. After these opening sections, we close the introductory chapter in Sect. 1.4 by presenting our motivation for this book and describing the organization of the chapters that follow.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigMethodologiesPhysicalDesign2020,
  title = {Methodologies for {{Physical Design}}: {{Models}}, {{Styles}}, {{Tasks}}, and {{Flows}}},
  shorttitle = {Methodologies for {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {127--164},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_4},
  url = {https://doi.org/10.1007/978-3-030-39284-0_4},
  urldate = {2024-02-19},
  abstract = {In Chap. 2 we covered technologies and in Chap. 3 we saw how these technologies interface with physical design. Here in Chap. 4 we now provide an end-to-end overview of the physical design process, namely how to physically construct the layout of an electronic circuit. In this chapter we present the fundamental knowledge an engineer must possess to carry out this task. In Chap. 5 we then discuss each of the specific physical design steps in further detail. We begin the chapter by introducing the design flow (Sect. 4.1), design models (Sect. 4.2) and design styles (Sect. 4.3). Next, we investigate various design tasks and related tools (Sect. 4.4), before discussing optimization goals and design constraints (Sect. 4.5). Up to this point our treatise has focused mainly on the digital design flow. In Sect. 4.6 we introduce the characteristics of, and differences between, analog, digital, and mixed-signal design flows. Looking toward the future, we conclude the chapter by presenting two different yet complementary visions for analog-design automation to overcome the analog-digital design gap (Sect. 4.7).},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigSpecialLayoutTechniques2020,
  title = {Special {{Layout Techniques}} for {{Analog IC Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {213--255},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_6},
  url = {https://doi.org/10.1007/978-3-030-39284-0_6},
  urldate = {2024-02-19},
  abstract = {While the physical design steps introduced in Chaps. 4 and 5 are universal, analog integrated circuits present further challenges that require additional layout techniques. There are many differences between analog and digital, and as such, the design flows and tools vary in both cases. Analog circuits are generally less complex in terms of transistor count and are designed in a manual fashion. The distinct lack of design automation means that manual design is still in widespread use today, requiring specialist knowledge that is unique to analog design. This specialist knowledge is covered in this chapter. We discussed analog design flows in Chap. 4 (Sects. 4.6 and 4.7), previously. Now we present layout techniques that accompany these analog flows, which an analog layout designer must be fully aware of. We start with an introduction to sheet resistances and wells (Sects. 6.1 and 6.2) as this knowledge is needed for the sizing and understanding of analog devices, which we then cover in Sect. 6.3. The methodology for cell generators, which produce such analog devices, is presented in Sect. 6.4. An explanation of the fundamental importance of symmetry and a treatise of resulting matching concepts (Sects. 6.5 and 6.6) conclude this chapter.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigStepsPhysicalDesign2020,
  title = {Steps in {{Physical Design}}: {{From Netlist Generation}} to {{Layout Post Processing}}},
  shorttitle = {Steps in {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {165--211},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_5},
  url = {https://doi.org/10.1007/978-3-030-39284-0_5},
  urldate = {2024-02-19},
  abstract = {Due to its complexity, the physical design process is divided into several primary steps. Having introduced in Chap. 4 the flow, constraints and methodologies of today’s physical design process, we now investigate the various steps required to generate its output: a layout. These steps, which transform a netlist into optimized mask data, are dealt with one by one in this chapter. A layout is generated from a netlist. We first describe how a netlist is created, that is, either by using hardware description languages (HDLs) in digital design (Sect. 5.1), or by deriving it from a schematic, as is common in analog design (Sect. 5.2). Then the physical design steps, comprising partitioning, floorplanning, placement, and routing, are presented in detail (Sect. 5.3). All of these steps are supported by highly sophisticated EDA tools in the case of digital designs, which is our focus here. We also discuss in this section the key aspects of symbolic compaction, standard-cell design and PCB design. When the physical design phase is completed, the resulting layout must be verified. This verification step confirms both functional correctness and design manufacturability. Methodologies and tools for comprehensive design verification, with a focus on physical verification, are covered in Sect. 5.4. Finally, we briefly touch on layout post-processing methodologies, such as resolution enhancement techniques (RET), that might impact physical design (Sect. 5.5).},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigStepsPhysicalDesign2020a,
  title = {Steps in {{Physical Design}}: {{From Netlist Generation}} to {{Layout Post Processing}}},
  shorttitle = {Steps in {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {165--211},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_5},
  url = {https://doi.org/10.1007/978-3-030-39284-0_5},
  urldate = {2024-02-19},
  abstract = {Due to its complexity, the physical design process is divided into several primary steps. Having introduced in Chap. 4 the flow, constraints and methodologies of today’s physical design process, we now investigate the various steps required to generate its output: a layout. These steps, which transform a netlist into optimized mask data, are dealt with one by one in this chapter. A layout is generated from a netlist. We first describe how a netlist is created, that is, either by using hardware description languages (HDLs) in digital design (Sect. 5.1), or by deriving it from a schematic, as is common in analog design (Sect. 5.2). Then the physical design steps, comprising partitioning, floorplanning, placement, and routing, are presented in detail (Sect. 5.3). All of these steps are supported by highly sophisticated EDA tools in the case of digital designs, which is our focus here. We also discuss in this section the key aspects of symbolic compaction, standard-cell design and PCB design. When the physical design phase is completed, the resulting layout must be verified. This verification step confirms both functional correctness and design manufacturability. Methodologies and tools for comprehensive design verification, with a focus on physical verification, are covered in Sect. 5.4. Finally, we briefly touch on layout post-processing methodologies, such as resolution enhancement techniques (RET), that might impact physical design (Sect. 5.5).},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigTechnologyKnowHowSilicon2020,
  title = {Technology {{Know-How}}: {{From Silicon}} to {{Devices}}},
  shorttitle = {Technology {{Know-How}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {31--82},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_2},
  url = {https://doi.org/10.1007/978-3-030-39284-0_2},
  urldate = {2024-02-19},
  abstract = {We discuss the fabrication technologies for IC chips in this chapter. We will focus on the main process steps and especially on those aspects that are of particular importance for understanding how they affect, and in some cases drive, the layout of ICs. All our analyses in this chapter will be for silicon as the base material; the principles and understanding gained can be applied to other substrates as well. Following a brief introduction to the fundamentals of IC fabrication (Sect. 2.1) and the base material used in it, namely silicon (Sect. 2.2), we discuss the photolithography process deployed for all structuring work in Sect. 2.3. We will then present in Sect. 2.4 some theoretical opening remarks on typical phenomena encountered in IC fabrication. Knowledge of these phenomena is very useful for understanding the process steps we cover in Sects. 2.5–2.8. We examine a simple exemplar process in Sect. 2.9 and observe how a field-effect transistor (FET) – the most important device in modern integrated circuits—is created. To drive the key points home, we provide a review of each topic at the end of every section from the point of view of layout design by discussing relevant physical design aspects.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@article{limExpressiveSignEquivariant2024,
  title = {Expressive {{Sign Equivariant Networks}} for {{Spectral Geometric Learning}}},
  author = {Lim, Derek and Robinson, Joshua and Jegelka, Stefanie and Maron, Haggai},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/3516aa3393f0279e04c099f724664f99-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YTKNFA26/Lim et al. - 2024 - Expressive Sign Equivariant Networks for Spectral .pdf}
}

@incollection{liMultiLayeredNetworkEmbedding2018,
  title = {Multi-{{Layered Network Embedding}}},
  booktitle = {Proceedings of the 2018 {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author = {Li, Jundong and Chen, Chen and Tong, Hanghang and Liu, Huan},
  date = {2018-05-07},
  series = {Proceedings},
  pages = {684--692},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611975321.77},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611975321.77},
  urldate = {2024-02-22},
  abstract = {Network embedding has gained more attentions in recent years. It has been shown that the learned low-dimensional node vector representations could advance a myriad of graph mining tasks such as node classification, community detection, and link prediction. A vast majority of the existing efforts are overwhelmingly devoted to single-layered networks or homogeneous networks with a single type of nodes and node interactions. However, in many real-world applications, a variety of networks could be abstracted and presented in a multilayered fashion. Typical multi-layered networks include critical infrastructure systems, collaboration platforms, social recommender systems, to name a few. Despite the widespread use of multi-layered networks, it remains a daunting task to learn vector representations of different types of nodes due to the bewildering combination of both within-layer connections and cross-layer network dependencies. In this paper, we study a novel problem of multi-layered network embedding. In particular, we propose a principled framework – MANE to model both within-layer connections and cross-layer network dependencies simultaneously in a unified optimization framework for embedding representation learning. Experiments on real-world multi-layered networks corroborate the effectiveness of the proposed framework.},
  file = {/home/krawczuk/Zotero/storage/ZT4Z6MQ3/Li et al. - 2018 - Multi-Layered Network Embedding.pdf}
}

@online{lippeCategoricalNormalizingFlows2021a,
  title = {Categorical {{Normalizing Flows}} via {{Continuous Transformations}}},
  author = {Lippe, Phillip and Gavves, Efstratios},
  date = {2021-01-21},
  eprint = {2006.09790},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.09790},
  url = {http://arxiv.org/abs/2006.09790},
  urldate = {2024-02-24},
  abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate \textbackslash emph\{Categorical Normalizing Flows\}, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/Z9W6FBUY/Lippe and Gavves - 2021 - Categorical Normalizing Flows via Continuous Trans.pdf;/home/krawczuk/Zotero/storage/49CZS9LC/2006.html}
}

@inproceedings{liSampleEfficientReinforcementLearning2021,
  title = {Sample-{{Efficient Reinforcement Learning Is Feasible}} for {{Linearly Realizable MDPs}} with {{Limited Revisiting}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Li, Gen and Chen, Yuxin and Chi, Yuejie and Gu, Yuantao and Wei, Yuting},
  date = {2021},
  volume = {34},
  pages = {16671--16685},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/8b5700012be65c9da25f49408d959ca0-Abstract.html},
  urldate = {2024-02-26},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/KCE5QMGM/Li et al. - 2021 - Sample-Efficient Reinforcement Learning Is Feasibl.pdf}
}

@inproceedings{liuDETDetectingSystem2020a,
  title = {S {\textsuperscript{3}} {{DET}}: {{Detecting System Symmetry Constraints}} for {{Analog Circuits}} with {{Graph Similarity}}},
  shorttitle = {S {\textsuperscript{3}} {{DET}}},
  booktitle = {2020 25th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Liu, Mingjie and Li, Wuxi and Zhu, Keren and Xu, Biying and Lin, Yibo and Shen, Linxiao and Tang, Xiyuan and Sun, Nan and Pan, David Z.},
  date = {2020-01},
  pages = {193--198},
  publisher = {{IEEE}},
  location = {{Beijing, China}},
  doi = {10.1109/ASP-DAC47756.2020.9045109},
  url = {https://ieeexplore.ieee.org/document/9045109/},
  urldate = {2024-02-20},
  abstract = {Symmetry and matching between critical building blocks have a significant impact on analog system performance. However, there is limited research on generating system level symmetry constraints. In this paper, we propose a novel method of detecting system symmetry constraints for analog circuits with graph similarity. Leveraging spectral graph analysis and graph centrality, the proposed algorithm can be applied to circuits and systems of large scale and different architectures. To the best of our knowledge, this is the first work in detecting system level symmetry constraints for analog and mixed-signal (AMS) circuits. Experimental results show that the proposed method can achieve high accuracy of 88.3\% with low false alarm rate of less than 1.1\% in largescale AMS designs.},
  eventtitle = {2020 25th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  isbn = {978-1-72814-123-7},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/8YFFVMHY/Liu et al. - 2020 - S 3 DET Detecting System Symmetry Cons.pdf}
}

@article{liuGNNCapChipScaleInterconnect2024,
  title = {{{GNN-Cap}}: {{Chip-Scale Interconnect Capacitance Extraction Using Graph Neural Network}}},
  shorttitle = {{{GNN-Cap}}},
  author = {Liu, Lihao and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2024},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  pages = {1--1},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2023.3331942},
  url = {https://ieeexplore.ieee.org/document/10314730/},
  urldate = {2024-02-22},
  abstract = {Interconnect capacitive parasitics are becoming increasingly dominant at finer technology nodes. Chip-scale interconnect capacitance extraction is a critical but challenging task. The structure patterns of nanometer-scale on-chip interconnects are complex. The accuracy of widely used patternmatching-based capacitance extraction methods is limited by labor-intensive pattern library construction. This work presents GNN-Cap, a graph neural network-based method for chip-scale interconnect capacitance extraction. GNN-Cap uses graph presentation learning to model the complex interconnect structural patterns, which enables accurate and efficient prediction of wiring capacitances. Compared with StarRC, the de facto commercial capacitance extraction tool, GNN-Cap achieves a speed up of 11 X to 13 X, and reduces the average relative errors of total and coupling capacitances by 81\% and 59\%, respectively.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/7C3A2BXP/Liu et al. - 2024 - GNN-Cap Chip-Scale Interconnect Capacitance Extra.pdf}
}

@article{liuGONEndtoendOptimization2022,
  title = {{{GON}}: {{End-to-end}} Optimization Framework for Constraint Graph Optimization Problems},
  shorttitle = {{{GON}}},
  author = {Liu, Chuan and Wang, Jingwei and Cao, Yunkang and Liu, Min and Shen, Weiming},
  date = {2022-10-27},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {254},
  pages = {109697},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2022.109697},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705122008590},
  urldate = {2024-02-22},
  abstract = {Real-world computational applications often require solving combinatorial optimization problems on graphs, i.e., graph optimization problems (GOPs). An emerging trend is using graph neural networks (GNNs) to tackle GOPs. However, for GOPs with constraints, a great challenge faced by GNNs-based methods is to produce optimal solutions that satisfy the constraints. Existing methods relying on supervised learning require a large amount of labeled data, while unsupervised learning methods often require designing elaborate network architectures. To address these limitations, we propose an end-to-end optimization framework for constraint GOPs, coined Graph Optimization Network (GON). GON is a simple and effective method aiming to address general GOPs without designing specific network architectures for different problems. The proposed framework is evaluated on two typical constraint GOPs, one with only soft constraints (the balanced graph partitioning problem) and the other with hard constraints (the maximum independent set problem). We model the two problems as node assignment problems and design the corresponding constraint-aware loss function for each so that the model can be directly optimized to generate optimal solutions that satisfy the constraints. Experiments on various benchmarks show that the proposed GON equipped with a vanilla GNN can achieve excellent performance on both GOPs, superior to state-of-the-art methods. The results suggest that GON is a promising solver to address NP-hard GOPs.},
  keywords = {Balanced graph partitioning,Graph neural networks,Graph optimization,Maximum independent set},
  file = {/home/krawczuk/Zotero/storage/PQT9ETG3/S0950705122008590.html}
}

@inproceedings{liuGraphNormalizingFlows2019a,
  title = {Graph {{Normalizing Flows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/1e44fdf9c44d7328fecc02d677ed704d-Abstract.html},
  urldate = {2024-02-24},
  abstract = {We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/F9Y9EHVC/Liu et al. - 2019 - Graph Normalizing Flows.pdf}
}

@inproceedings{liuOpenSAROpenSource2021,
  title = {{{OpenSAR}}: {{An Open Source Automated End-to-end SAR ADC Compiler}}},
  shorttitle = {{{OpenSAR}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Chen, Hao and Sun, Nan and Pan, David Z.},
  date = {2021-11-01},
  pages = {1--9},
  publisher = {{IEEE}},
  location = {{Munich, Germany}},
  doi = {10.1109/ICCAD51958.2021.9643494},
  url = {https://ieeexplore.ieee.org/document/9643494/},
  urldate = {2024-02-20},
  abstract = {Despite recent developments in automated analog sizing and analog layout generation, there is doubt whether analog design automation techniques could scale to system-level designs. On the other hand, analog designs are considered major roadblocks for open source hardware with limited available design automation tools. In this work, we present OpenSAR, the first open source automated end-to-end successive approximation register (SAR) analog-to-digital converter (ADC) compiler. OpenSAR only requires system performance specifications as the minimal input and outputs DRC and LVS clean layouts. Compared with prior work, we leverage automated placement and routing to generate analog building blocks, removing the need to design layout templates or libraries. We optimize the redundant non-binary capacitor digital-toanalog converter (CDAC) array design for yield considerations with a template-based layout generator that interleaves capacitor rows and columns to reduce process gradient mismatch. Post layout simulations demonstrate that the generated prototype designs achieve state-of-theart resolution, speed, and energy efficiency.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  isbn = {978-1-66544-507-8},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/9XET6G6T/Liu et al. - 2021 - OpenSAR An Open Source Automated End-to-end SAR A.pdf}
}

@article{liVarianceReducedGradientEstimation2023,
  title = {Variance-{{Reduced Gradient Estimation}} via {{Noise-Reuse}} in {{Online Evolution Strategies}}},
  author = {Li, Oscar and Harrison, James and Sohl-Dickstein, Jascha and Smith, Virginia and Metz, Luke},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8e69a97cbdd91ac0808603fa589d6c17-Abstract-Conference.html},
  urldate = {2024-02-22},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/2GYBGSEP/Li et al. - 2023 - Variance-Reduced Gradient Estimation via Noise-Reu.pdf}
}

@article{liWireSizingNontree2007,
  title = {Wire Sizing for Non-Tree Topology},
  author = {Li, Zhuo and Zhou, Ying and Shi, Weiping},
  date = {2007-05},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {26},
  number = {5},
  pages = {872--880},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2007.8361581},
  url = {https://ieeexplore.ieee.org/abstract/document/8361581?casa_token=hD2tSo05Z9EAAAAA:1S9tJPpBOkHyMFEglZG4oO27Ghp51MRw8xphvZ7HITcMR1_Ybg7ooneh-TEV_Zv6CHBYmWGtbpiB},
  urldate = {2024-02-20},
  abstract = {Most existing methods for interconnect wire sizing are designed for RC trees. With the increasing popularity of the non-tree topology in clock networks and multiple link networks, wire sizing for non-tree networks becomes an important problem. In this paper, we propose the first systematic method to size the wires of general non-tree RC networks. Our method consists of three steps: 1) decompose a non-tree RC network into a tree RC network such that the Elmore delay at every sink remains unchanged; 2) size wires of the tree; and 3) merge the wires back to the original non-tree network. All three steps can be implemented in low-order polynomial time. Using this method, previous wiresizing techniques for tree topology for various objectives, such as minimizing the maximum delay, minimizing the total area or power, and reducing skew variability under process variations, can be applied to non-tree topologies. For certain types of networks, such as the tree+link network, our method gives the optimal solution, provided the tree wire sizing is optimal. Compared with the previous best wire-sizing method for non-tree circuits we can achieve 2\% to 17\% Elmore delay reduction with 14\% to 30\% total wire area reduction. Compared with unsized minimum width networks, our delay is 25\% less and the skew is 34\% less, under SPICE simulation. For the tree+link network, we can achieve significant delay reduction and zero skew in nominal case, while get up to 66\% skew variation reduction.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Capacitance,Clocks,Delays,Interconnect synthesis,Network topology,optimization,physicaldesign,postlayout resynthesis,Resistance,routing,timing optimization,Topology,Wires},
  file = {/home/krawczuk/Zotero/storage/6SGLMDGC/Li et al. - 2007 - Wire sizing for non-tree topology.pdf}
}

@inproceedings{loperaSurveyGraphNeural2021d,
  ids = {loperaSurveyGraphNeural2021e},
  title = {A {{Survey}} of {{Graph Neural Networks}} for {{Electronic Design Automation}}},
  booktitle = {2021 {{ACM}}/{{IEEE}} 3rd {{Workshop}} on {{Machine Learning}} for {{CAD}} ({{MLCAD}})},
  author = {Lopera, Daniela Sánchez and Servadei, Lorenzo and Kiprit, Gamze Naz and Hazra, Souvik and Wille, Robert and Ecker, Wolfgang},
  date = {2021-08},
  pages = {1--6},
  doi = {10.1109/MLCAD52597.2021.9531070},
  url = {https://ieeexplore.ieee.org/abstract/document/9531070?casa_token=vqNtApKK8gkAAAAA:bK9rjq4aTs56Oc_6fi7jRjHZgBJzSPLK1zxTl4Fgf2-dz6EdFnpmao8sw-tbVIVoPQcDzl4nMjE},
  urldate = {2024-02-24},
  abstract = {Driven by Moore’s law, the chip design complexity is steadily increasing. Electronic Design Automation (EDA) has been able to cope with the challenging very large-scale integration process, assuring scalability, reliability, and proper time-to-market. However, EDA approaches are time and resource-demanding, and they often do not guarantee optimal solutions. To alleviate these, Machine Learning (ML) has been incorporated into many stages of the design flow, such as in placement and routing. Many solutions employ Euclidean data and ML techniques without considering that many EDA objects are represented naturally as graphs. The trending Graph Neural Networks are an opportunity to solve EDA problems directly using graph structures for circuits, intermediate RTLs, and netlists. In this paper, we present a comprehensive review of the existing works linking the EDA flow for chip design and Graph Neural Networks.},
  eventtitle = {2021 {{ACM}}/{{IEEE}} 3rd {{Workshop}} on {{Machine Learning}} for {{CAD}} ({{MLCAD}})},
  keywords = {Design automation,Electronic Design Automation,Graph neural networks,Graph Neural Networks,Machine learning,Machine Learning,Measurement,Register-Transfer Level,Reliability engineering,Routing,Scalability,Very Large-scale Integration},
  file = {/home/krawczuk/Zotero/storage/CWUDS85W/Lopera et al. - 2021 - A Survey of Graph Neural Networks for Electronic D.pdf;/home/krawczuk/Zotero/storage/FZ3LVAXW/9531070.html}
}

@article{luAutomaticOpAmpGeneration2023,
  title = {Automatic {{Op-Amp Generation}} from {{Specification}} to {{Layout}}},
  author = {Lu, Jialin and Lei, Liangbo and Huang, Jiangli and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2023},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  publisher = {{IEEE}},
  doi = {10.1109/TCAD.2023.3296374},
  url = {https://ieeexplore.ieee.org/abstract/document/10185586/?casa_token=YacwtZ2cDAMAAAAA:2Q22uayJlS-1TX591B3jALrZ4pbZ-URyWS_Hf-jAb3NmJeDLMN_z_i1NtF_2vdXLZcoYEZRYsIcD},
  urldate = {2024-02-19}
}

@article{ludkeAddThinDiffusion2024,
  title = {Add and {{Thin}}: {{Diffusion}} for {{Temporal Point Processes}}},
  shorttitle = {Add and {{Thin}}},
  author = {Lüdke, David and Biloš, Marin and Shchur, Oleksandr and Lienen, Marten and Günnemann, Stephan},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/b1d9c7e7bd265d81aae8d74a7a6bd7f1-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/RA2M5LWM/Lüdke et al. - 2024 - Add and Thin Diffusion for Temporal Point Process.pdf}
}

@inproceedings{luoGraphDFDiscreteFlow2021d,
  title = {{{GraphDF}}: {{A Discrete Flow Model}} for {{Molecular Graph Generation}}},
  shorttitle = {{{GraphDF}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Luo, Youzhi and Yan, Keqiang and Ji, Shuiwang},
  date = {2021-07-01},
  pages = {7192--7203},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/luo21a.html},
  urldate = {2024-02-20},
  abstract = {We consider the problem of molecular graph generation using deep models. While graphs are discrete, most existing methods use continuous latent variables, resulting in inaccurate modeling of discrete graph structures. In this work, we propose GraphDF, a novel discrete latent variable model for molecular graph generation based on normalizing flow methods. GraphDF uses invertible modulo shift transforms to map discrete latent variables to graph nodes and edges. We show that the use of discrete latent variables reduces computational costs and eliminates the negative effect of dequantization. Comprehensive experimental results show that GraphDF outperforms prior methods on random generation, property optimization, and constrained optimization tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/AIBUSQZL/Luo et al. - 2021 - GraphDF A Discrete Flow Model for Molecular Graph.pdf;/home/krawczuk/Zotero/storage/GBUG92KT/Luo et al. - 2021 - GraphDF A Discrete Flow Model for Molecular Graph.pdf}
}

@inproceedings{luTopologyOptimizationOperational2022,
  title = {Topology {{Optimization}} of {{Operational Amplifier}} in {{Continuous Space}} via {{Graph Embedding}}},
  booktitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Lu, Jialin and Lei, Liangbo and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2022-03},
  pages = {142--147},
  issn = {1558-1101},
  doi = {10.23919/DATE54114.2022.9774676},
  url = {https://ieeexplore.ieee.org/abstract/document/9774676?casa_token=SM8jM4GK_qMAAAAA:gPM11JjhdtGnORbikJKlZiaUsja3S33V-N-070gPHDeJyJDRR-U4LbhlSOy-yqm5DCBnFU6H3FL2},
  urldate = {2024-02-20},
  abstract = {Operational amplifier is a key building block in analog circuits. However, the design process of the operational amplifier is complex and time-consuming, as there are no practical automation tools available in the industry. This paper presents a new topology optimization method for operational amplifiers. The behavioral description of the operational amplifier is described using a directed acyclic graph (DAG), which is then transformed into a low-dimensional embedding in continuous space using a variational graph autoencoder. Topology search is performed in the continuous embedding space using stochastic optimization methods, such as Bayesian Optimization. The yield search results are then transformed back to operational amplifier topologies using a graph decoder. The proposed method is also equipped with a surrogate model for performance prediction. Experimental results show that the proposed approach can achieve significant speedup over the genetic searching algorithms. The produced three-stage operational amplifiers offer competitive performance compared to manual designs.},
  eventtitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {Directed acyclic graph,Operational amplifiers,Optimization methods,Performance evaluation,Predictive models,Representation learning,Stochastic processes},
  file = {/home/krawczuk/Zotero/storage/C7WJRM8M/Lu et al. - 2022 - Topology Optimization of Operational Amplifier in .pdf;/home/krawczuk/Zotero/storage/PVVSSYM5/9774676.html}
}

@online{madhawaGraphNVPInvertibleFlow2019a,
  title = {{{GraphNVP}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{GraphNVP}}},
  author = {Madhawa, Kaushalya and Ishiguro, Katushiko and Nakago, Kosuke and Abe, Motoki},
  date = {2019-05-28},
  eprint = {1905.11600},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1905.11600},
  url = {http://arxiv.org/abs/1905.11600},
  urldate = {2024-02-24},
  abstract = {We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/WK79C9MS/Madhawa et al. - 2019 - GraphNVP An Invertible Flow Model for Generating .pdf;/home/krawczuk/Zotero/storage/NXTB9LPV/1905.html}
}

@inproceedings{madjiheuremTemporalCreditAssignment2023,
  title = {Beyond {{Temporal Credit Assignment}} in {{Reinforcement Learning}}},
  author = {Madjiheurem, Sephora and Stachenfeld, Kim and Battaglia, Peter and Hamrick, Jessica B.},
  date = {2023-03-03},
  url = {https://openreview.net/forum?id=wEb6VYsaYx},
  urldate = {2024-02-26},
  abstract = {In reinforcement learning, traditional value-based methods rely heavily on time as the main proxy for propagating information across the state space. This often results in slow learning and does not scale to large and complex environments. Here, we propose to leverage prior information about the structure of the the environment to assign credit non-temporally to improve learning efficiency. Specifically, we introduce the concept of structural neighbours, which are sets of states with similar semantic structures and which have equivalent values under the optimal policy. We augment traditional value-based RL methods (TD(0), Dyna and Dueling DQN) with a learning mechanism based on structural neighbours. Our empirical results show that by incorporating structural updates, learning efficiency can be greatly improved on a variety of environments ranging from simple tabular grid worlds to those which require function approximation, including the complex and high-dimensional game of Solitaire.},
  eventtitle = {Workshop on {{Reincarnating Reinforcement Learning}} at {{ICLR}} 2023},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/L9FGSCG9/Madjiheurem et al. - 2023 - Beyond Temporal Credit Assignment in Reinforcement.pdf}
}

@online{MagicVLSIa,
  title = {Magic {{VLSI}}},
  url = {http://opencircuitdesign.com/magic/},
  urldate = {2024-02-26},
  file = {/home/krawczuk/Zotero/storage/VUARFGUS/magic.html}
}

@online{MagicVLSIb,
  title = {Magic {{VLSI}}},
  url = {http://opencircuitdesign.com/magic/},
  urldate = {2024-02-26}
}

@online{MagicVLSILayout,
  title = {The {{Magic VLSI Layout System}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/4069504?casa_token=fI3poE5vN7oAAAAA:vNmXStmnwiqMtO_nfw0puR2r4cM4ef2OoAGBDvfEkJBrZDhaNF2tfe1yhjjPUJDA-nEYKLtrhXVk},
  urldate = {2024-02-26},
  file = {/home/krawczuk/Zotero/storage/QJRZVM3N/4069504.html}
}

@article{maLaplacianCanonizationMinimalist2024,
  title = {Laplacian {{Canonization}}: {{A Minimalist Approach}} to {{Sign}} and {{Basis Invariant Spectral Embedding}}},
  shorttitle = {Laplacian {{Canonization}}},
  author = {Ma, George and Wang, Yifei and Wang, Yisen},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/257b3a7438b1f3709e91a86adf2fdc0a-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/PL5JSHRM/Ma et al. - 2024 - Laplacian Canonization A Minimalist Approach to S.pdf}
}

@article{malliarosCoreDecompositionNetworks2020,
  title = {The Core Decomposition of Networks: Theory, Algorithms and Applications},
  shorttitle = {The Core Decomposition of Networks},
  author = {Malliaros, Fragkiskos D. and Giatsidis, Christos and Papadopoulos, Apostolos N. and Vazirgiannis, Michalis},
  date = {2020-01-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {29},
  number = {1},
  pages = {61--92},
  issn = {0949-877X},
  doi = {10.1007/s00778-019-00587-4},
  url = {https://doi.org/10.1007/s00778-019-00587-4},
  urldate = {2024-02-20},
  abstract = {The core decomposition of networks has attracted significant attention due to its numerous applications in real-life problems. Simply stated, the core decomposition of a network (graph) assigns to each graph node v, an integer number c(v) (the core number), capturing how well v is connected with respect to its neighbors. This concept is strongly related to the concept of graph degeneracy, which has a long history in graph theory. Although the core decomposition concept is extremely simple, there is an enormous interest in the topic from diverse application domains, mainly because it can be used to analyze a network in a simple and concise manner by quantifying the significance of graph nodes. Therefore, there exists a respectable number of research works that either propose efficient algorithmic techniques under different settings and graph types or apply the concept to another problem or scientific area. Based on this large interest in the topic, in this survey, we perform an in-depth discussion of core decomposition, focusing mainly on: (i) the basic theory and fundamental concepts, (ii) the algorithmic techniques proposed for computing it efficiently under different settings, and (iii) the applications that can benefit significantly from it.},
  langid = {english},
  keywords = {Algorithms,Core decomposition,Graph degeneracy,Graph mining,Graph theory},
  file = {/home/krawczuk/Zotero/storage/HL6XIPN8/Malliaros et al. - 2020 - The core decomposition of networks theory, algori.pdf}
}

@inproceedings{mangalagiriAnalogLayoutSynthesis2019,
  title = {Analog {{Layout Synthesis}}: {{Are We There Yet}}?},
  shorttitle = {Analog {{Layout Synthesis}}},
  booktitle = {Proceedings of the 2019 {{International Symposium}} on {{Physical Design}}},
  author = {Mangalagiri, Prasanth},
  date = {2019-04-04},
  series = {{{ISPD}} '19},
  pages = {127},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3299902.3311065},
  url = {https://doi.org/10.1145/3299902.3311065},
  urldate = {2024-02-19},
  abstract = {Over the past decade, spurred by advances in mobile computing, there has been a fundamental shift in computing needs of consumer applications. There has been an industry-wide transition from highly CPU-centric to a peripheral-centric, connectivity and data-driven computing. This has paved way to the resurgence of Analog Mixed Signal Designs in both system-on-chip, and core computing architectures. However, the design automation capabilities used in production analog design flows have remained primarily manual with assisted-automation. Analog layout design and layout parasitic dependent circuit convergence remain a key bottleneck in industrial analog IP design. In this talk, we analyze the current state of analog design automation. We present a continuum of design scenarios, ranging from leading-edge design, to design migration across incremental process derivatives, and define the context of analog layout synthesis in each of these scenarios. We present an overview of recent advances in EDA research specific to analog layout automation [1] [2] and discuss their strengths and weaknesses when adapted to industrial analog IP design flows. Motivated by the confluence of emerging trends in EDA [3], and machine learning research [4], we discuss opportunities to bridge the "last-mile" gaps in automation, by combining constraint-driven, generator-based automation approaches, with statistical data-driven predictive methods.},
  isbn = {978-1-4503-6253-5},
  keywords = {analog layout synthesis,design convergence,eda.,layout migration,machine learning,physical design automation}
}

@inproceedings{maronProvablyPowerfulGraph2019b,
  title = {Provably {{Powerful Graph Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maron, Haggai and Ben-Hamu, Heli and Serviansky, Hadar and Lipman, Yaron},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bb04af0f7ecaee4aae62035497da1387-Abstract.html},
  urldate = {2024-02-24},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/N6B52E85/Maron et al. - 2019 - Provably Powerful Graph Networks.pdf}
}

@article{martinkusAbDiffuserFullatomGeneration2024,
  title = {{{AbDiffuser}}: Full-Atom Generation of in-Vitro Functioning Antibodies},
  shorttitle = {{{AbDiffuser}}},
  author = {Martinkus, Karolis and Ludwiczak, Jan and Liang, Wei-Ching and Lafrance-Vanasse, Julien and Hotzel, Isidro and Rajpal, Arvind and Wu, Yan and Cho, Kyunghyun and Bonneau, Richard and Gligorijevic, Vladimir and Loukas, Andreas},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/801ec05b0aae9fcd2ef35c168bd538e0-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/CVCVTSC8/Martinkus et al. - 2024 - AbDiffuser full-atom generation of in-vitro funct.pdf}
}

@inproceedings{martinkusSPECTRESpectralConditioning2022b,
  title = {{{SPECTRE}}: {{Spectral Conditioning Helps}} to {{Overcome}} the {{Expressivity Limits}} of {{One-shot Graph Generators}}},
  shorttitle = {{{SPECTRE}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Martinkus, Karolis and Loukas, Andreas and Perraudin, Nathanaël and Wattenhofer, Roger},
  date = {2022-06-28},
  pages = {15159--15179},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/martinkus22a.html},
  urldate = {2024-02-20},
  abstract = {We approach the graph generation problem from a spectral perspective by first generating the dominant parts of the graph Laplacian spectrum and then building a graph matching these eigenvalues and eigenvectors. Spectral conditioning allows for direct modeling of the global and local graph structure and helps to overcome the expressivity and mode collapse issues of one-shot graph generators. Our novel GAN, called SPECTRE, enables the one-shot generation of much larger graphs than previously possible with one-shot models. SPECTRE outperforms state-of-the-art deep autoregressive generators in terms of modeling fidelity, while also avoiding expensive sequential generation and dependence on node ordering. A case in point, in sizable synthetic and real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best competitor that does not overfit and is 23-to-30 times faster than autoregressive generators.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/E3J8ZP25/Martinkus et al. - 2022 - SPECTRE Spectral Conditioning Helps to Overcome t.pdf}
}

@article{masteliniSWINNEfficientNearest2024,
  title = {{{SWINN}}: {{Efficient}} Nearest Neighbor Search in Sliding Windows Using Graphs},
  shorttitle = {{{SWINN}}},
  author = {Mastelini, Saulo Martiello and Veloso, Bruno and Halford, Max and family=Carvalho, given=André Carlos Ponce de Leon Ferreira, prefix=de, useprefix=true and Gama, João},
  date = {2024-01-01},
  journaltitle = {Information Fusion},
  shortjournal = {Information Fusion},
  volume = {101},
  pages = {101979},
  issn = {1566-2535},
  doi = {10.1016/j.inffus.2023.101979},
  url = {https://www.sciencedirect.com/science/article/pii/S1566253523002956},
  urldate = {2024-02-26},
  abstract = {Nearest neighbor search (NNS) is one of the main concerns in data stream applications since similarity queries can be used in multiple scenarios. Online NNS is usually performed on a sliding window by lazily scanning every element currently stored in the window. This paper proposes Sliding Window-based Incremental Nearest Neighbors (SWINN), a graph-based online search index algorithm for speeding up NNS in potentially never-ending and dynamic data stream tasks. Our proposal broadens the application of online NNS-based solutions, as even moderately large data buffers become impractical to handle when a naive NNS strategy is selected. SWINN enables efficient handling of large data buffers by using an incremental strategy to build and update a search graph supporting any distance metric. Vertices can be added and removed from the search graph. To keep the graph reliable for search queries, lightweight graph maintenance routines are run. According to experimental results, SWINN is significantly faster than performing a naive complete scan of the data buffer while keeping competitive search recall values. We also apply SWINN to online classification and regression tasks and show that our proposal is effective against popular online machine learning algorithms.},
  keywords = {FIFO,Graph,Nearest neighbor search,Proximity query,Sliding window},
  file = {/home/krawczuk/Zotero/storage/VALQWN6I/S1566253523002956.html}
}

@online{mathieuAlphaStarUnpluggedLargeScale2023,
  title = {{{AlphaStar Unplugged}}: {{Large-Scale Offline Reinforcement Learning}}},
  shorttitle = {{{AlphaStar Unplugged}}},
  author = {Mathieu, Michaël and Ozair, Sherjil and Srinivasan, Srivatsan and Gulcehre, Caglar and Zhang, Shangtong and Jiang, Ray and Paine, Tom Le and Powell, Richard and Żołna, Konrad and Schrittwieser, Julian and Choi, David and Georgiev, Petko and Toyama, Daniel and Huang, Aja and Ring, Roman and Babuschkin, Igor and Ewalds, Timo and Bordbar, Mahyar and Henderson, Sarah and Colmenarejo, Sergio Gómez and family=Oord, given=Aäron, prefix=van den, useprefix=false and Czarnecki, Wojciech Marian and family=Freitas, given=Nando, prefix=de, useprefix=true and Vinyals, Oriol},
  date = {2023-08-07},
  eprint = {2308.03526},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.03526},
  url = {http://arxiv.org/abs/2308.03526},
  urldate = {2024-02-26},
  abstract = {StarCraft II is one of the most challenging simulated reinforcement learning environments; it is partially observable, stochastic, multi-agent, and mastering StarCraft II requires strategic planning over long time horizons with real-time low-level execution. It also has an active professional competitive scene. StarCraft II is uniquely suited for advancing offline RL algorithms, both because of its challenging nature and because Blizzard has released a massive dataset of millions of StarCraft II games played by human players. This paper leverages that and establishes a benchmark, called AlphaStar Unplugged, introducing unprecedented challenges for offline reinforcement learning. We define a dataset (a subset of Blizzard's release), tools standardizing an API for machine learning methods, and an evaluation protocol. We also present baseline agents, including behavior cloning, offline variants of actor-critic and MuZero. We improve the state of the art of agents using only offline data, and we achieve 90\% win rate against previously published AlphaStar behavior cloning agent.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/7LBH7WYB/Mathieu et al. - 2023 - AlphaStar Unplugged Large-Scale Offline Reinforce.pdf;/home/krawczuk/Zotero/storage/7GJJFMPE/2308.html}
}

@inproceedings{maziarzLearningExtendMolecular2021,
  title = {Learning to {{Extend Molecular Scaffolds}} with {{Structural Motifs}}},
  author = {Maziarz, Krzysztof and Jackson-Flux, Henry Richard and Cameron, Pashmina and Sirockin, Finton and Schneider, Nadine and Stiefl, Nikolaus and Segler, Marwin and Brockschmidt, Marc},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=ZTsoE8G3GG},
  urldate = {2024-02-20},
  abstract = {Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/IYPVPWMU/Maziarz et al. - 2021 - Learning to Extend Molecular Scaffolds with Struct.pdf}
}

@inproceedings{meadeGateLevelNetlistReverse2016,
  title = {Gate-{{Level Netlist Reverse Engineering Tool Set}} for {{Functionality Recovery}} and {{Malicious Logic Detection}}},
  author = {Meade, Travis and Zhang, Shaojie and Jin, Yier and Zhao, Zheng and Pan, David},
  date = {2016-11-01},
  pages = {342--346},
  publisher = {{ASM International}},
  doi = {10.31399/asm.cp.istfa2016p0342},
  url = {https://dx.doi.org/10.31399/asm.cp.istfa2016p0342},
  urldate = {2024-02-19},
  abstract = {Abstract. Reliance on third-party resources, including thirdparty IP cores and fabrication foundries, as well as wide usage of commercial-off-the-shelf (COTS) components has raised concerns that backdoors and/or hardware Trojans may be inserted into fabricated chips. Defending against hardware backdoors and/or Trojans has primarily focused on detection at various stages in the supply chain. Netlist reverse engineering tools have been investigated as an alternative to existing chip-level reverse engineering methods which can help recover functional netlists from fabricated chips, but fall short of detecting malicious logic or recovering high-level functionality. In this work, we develop a netlist reverse engineering tool-set which recovers high-level functionality from the netlist, thereby aiding malicious logic detection. The tool-set performs state register identification, control logic recovery and datapath tracking, which facilitates validation of encrypted/obfuscated hardware IP cores. Relying on 3-SAT algorithms and topology-based computational methods, we demonstrate that the developed tool-set can handle netlists of various complexities.},
  eventtitle = {{{ISTFA}} 2016},
  langid = {english}
}

@inproceedings{meadeGateLevelNetlistReverse2016a,
  title = {Gate-{{Level Netlist Reverse Engineering Tool Set}} for {{Functionality Recovery}} and {{Malicious Logic Detection}}},
  author = {Meade, Travis and Zhang, Shaojie and Jin, Yier and Zhao, Zheng and Pan, David},
  date = {2016-11-01},
  pages = {342--346},
  publisher = {{ASM International}},
  doi = {10.31399/asm.cp.istfa2016p0342},
  url = {https://dx.doi.org/10.31399/asm.cp.istfa2016p0342},
  urldate = {2024-02-19},
  abstract = {Abstract. Reliance on third-party resources, including thirdparty IP cores and fabrication foundries, as well as wide usage of commercial-off-the-shelf (COTS) components has raised concerns that backdoors and/or hardware Trojans may be inserted into fabricated chips. Defending against hardware backdoors and/or Trojans has primarily focused on detection at various stages in the supply chain. Netlist reverse engineering tools have been investigated as an alternative to existing chip-level reverse engineering methods which can help recover functional netlists from fabricated chips, but fall short of detecting malicious logic or recovering high-level functionality. In this work, we develop a netlist reverse engineering tool-set which recovers high-level functionality from the netlist, thereby aiding malicious logic detection. The tool-set performs state register identification, control logic recovery and datapath tracking, which facilitates validation of encrypted/obfuscated hardware IP cores. Relying on 3-SAT algorithms and topology-based computational methods, we demonstrate that the developed tool-set can handle netlists of various complexities.},
  eventtitle = {{{ISTFA}} 2016},
  langid = {english}
}

@inproceedings{meadeNetlistReverseEngineering2016,
  title = {Netlist Reverse Engineering for High-Level Functionality Reconstruction},
  booktitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Meade, Travis and Zhang, Shaojie and Jin, Yier},
  date = {2016-01},
  pages = {655--660},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2016.7428086},
  url = {https://ieeexplore.ieee.org/abstract/document/7428086?casa_token=QaB4X0R0lY0AAAAA:LY9xjkYObFJFeANY4kbmFr3X122ry0Q1xoQX7zWMryI99VeJS2zqbxlHsldNfiDpPcv5JWb2I7Dl},
  urldate = {2024-02-19},
  abstract = {In a modern IC design flow, from specification development to chip fabrication, various security threats are emergent. Of particular concern are modifications made to third-party IP cores and commercial off-the-shelf (COTS) chips where no golden models are available for comparisons. Toward this direction, we develop a tool, named Reverse Engineering Finite State Machine (REFSM), that helps end-users reconstruct a high-level description of the control logic from a flattened netlist. We demonstrate that REFSM effectively recovers circuit control logic from netlists with varying degrees of complexity. Experimental results also showed that the developed tool can easily identify malicious logic from a flattened (or even obfuscated) netlist. If combined with chip level reverse engineering techniques, the developed REFSM tool can help detect the insertion of hardware Trojans in fabricated circuits.},
  eventtitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  keywords = {Hardware,IP networks,Logic gates,Registers,Reverse engineering,Trojan horses},
  file = {/home/krawczuk/Zotero/storage/VI6VGNCB/Meade et al. - 2016 - Netlist reverse engineering for high-level functio.pdf;/home/krawczuk/Zotero/storage/GVGJUHTK/7428086.html}
}

@article{meissnerFEATSFrameworkExplorative2015d,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}}},
  shorttitle = {{{FEATS}}},
  author = {Meissner, Markus and Hedrich, Lars},
  date = {2015-02},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {34},
  number = {2},
  pages = {213--226},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2014.2376987},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=7W00nHRy4joAAAAA:-NpwTCtaXxFtwCYJSnhNC1ZDlMRv_6NxpQFZLRE9OlqvgScAq5z60SejgPlV3fs_nRW4pm7RSSuT},
  urldate = {2024-02-20},
  abstract = {This paper proposes a new methodology for automated analog circuit synthesis, aiming to address the challenges known from other analog synthesis approaches: unsatisfactory time predictability due to stochastic-driven circuit generation methods, the dereliction of the creative part during the design process, and the inflexibility leading to synthesis tools, which mostly only handle just one circuit class. This contribution presents the underlying concepts and ideas to provide the predictability, flexibility, and creative freedom in order to elevate analog circuit design to the next step. A circuit generation algorithm is presented, which allows a full design-space exploration. Furthermore, an isomorphism algorithm is developed, which reduces a given set of circuits to its unique being one of the first methodologies addressing this issue. Thus, the algorithm handles vast amounts of circuits in a very efficient manner. The results demonstrate the claimed feasibility and applicability of the synthesis framework in general and in the context of system design.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Abstracts,Algorithm design and analysis,Analog circuits,design for space exploration,Design for space exploration,Engines,Libraries,performance optimization,Ports (Computers),symbolic techniques,symmetry detection,Symmetry detection,synthesis,Synthesis,Topology},
  file = {/home/krawczuk/Zotero/storage/8NNKF6ZF/Meissner and Hedrich - 2015 - FEATS Framework for Explorative Analog Topology S.pdf}
}

@article{meissnerFEATSFrameworkExplorative2015e,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}}},
  shorttitle = {{{FEATS}}},
  author = {Meissner, Markus and Hedrich, Lars},
  date = {2015-02},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {34},
  number = {2},
  pages = {213--226},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2014.2376987},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=7W00nHRy4joAAAAA:-NpwTCtaXxFtwCYJSnhNC1ZDlMRv_6NxpQFZLRE9OlqvgScAq5z60SejgPlV3fs_nRW4pm7RSSuT},
  urldate = {2024-02-20},
  abstract = {This paper proposes a new methodology for automated analog circuit synthesis, aiming to address the challenges known from other analog synthesis approaches: unsatisfactory time predictability due to stochastic-driven circuit generation methods, the dereliction of the creative part during the design process, and the inflexibility leading to synthesis tools, which mostly only handle just one circuit class. This contribution presents the underlying concepts and ideas to provide the predictability, flexibility, and creative freedom in order to elevate analog circuit design to the next step. A circuit generation algorithm is presented, which allows a full design-space exploration. Furthermore, an isomorphism algorithm is developed, which reduces a given set of circuits to its unique being one of the first methodologies addressing this issue. Thus, the algorithm handles vast amounts of circuits in a very efficient manner. The results demonstrate the claimed feasibility and applicability of the synthesis framework in general and in the context of system design.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Abstracts,Algorithm design and analysis,Analog circuits,design for space exploration,Design for space exploration,Engines,Libraries,performance optimization,Ports (Computers),symbolic techniques,symmetry detection,Symmetry detection,synthesis,Synthesis,Topology},
  file = {/home/krawczuk/Zotero/storage/VT3SSR4J/Meissner and Hedrich - 2015 - FEATS Framework for Explorative Analog Topology S.pdf}
}

@article{metropolisEquationStateCalculations1953a,
  title = {Equation of {{State Calculations}} by {{Fast Computing Machines}}},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  date = {1953-06-01},
  journaltitle = {The Journal of Chemical Physics},
  shortjournal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  issn = {0021-9606},
  doi = {10.1063/1.1699114},
  url = {https://doi.org/10.1063/1.1699114},
  urldate = {2024-02-25},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two‐dimensional rigid‐sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four‐term virial coefficient expansion.},
  file = {/home/krawczuk/Zotero/storage/YDKUJRY9/Equation-of-State-Calculations-by-Fast-Computing.html}
}

@online{metzUnrolledGenerativeAdversarial2017a,
  title = {Unrolled {{Generative Adversarial Networks}}},
  author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  date = {2017-05-12},
  eprint = {1611.02163},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1611.02163},
  url = {http://arxiv.org/abs/1611.02163},
  urldate = {2024-02-22},
  abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/2BTV6DU9/Metz et al. - 2017 - Unrolled Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/D3YJYM8J/1611.html}
}

@article{meurerPythonArrayAPI2023,
  title = {Python {{Array API Standard}}: {{Toward Array Interoperability}} in the {{Scientific Python Ecosystem}}},
  author = {Meurer, Aaron and Reines, Athan and Gommers, Ralf and Fang, Yao-Lung L and Kirkham, John and Barber, Matthew and Müller, Andreas and Zha, Sheng and Shanabrook, Saul and Gacha, Stephannie Jiménez and Lezcano-Casado, Mario and Fan, Thomas J and Reddy, Tyler and Passos, Alexandre and Kwon, Hyukjin and Oliphant, Travis},
  date = {2023},
  abstract = {The Python array API standard specifies standardized application programming interfaces (APIs) and behaviors for array and tensor objects and operations as commonly found in libraries such as NumPy [1], CuPy [2], PyTorch [3], JAX [4], TensorFlow [5], Dask [6], and MXNet [7]. The establishment and subsequent adoption of the standard aims to reduce ecosystem fragmentation and facilitate array library interoperability in user code and among array-consuming libraries, such as scikit-learn [8] and SciPy [9]. A key benefit of array interoperability for downstream consumers of the standard is device agnosticism, whereby previously CPU-bound implementations can more readily leverage hardware acceleration via graphics processing units (GPUs), tensor processing units (TPUs), and other accelerator devices.},
  langid = {english},
  keywords = {❓ Multiple DOI},
  file = {/home/krawczuk/Zotero/storage/4WF7ZAY7/Meurer et al. - 2023 - Python Array API Standard Toward Array Interopera.pdf}
}

@online{Mg2vecLearningRelationshipPreserving,
  title = {Mg2vec: {{Learning Relationship-Preserving Heterogeneous Graph Representations}} via {{Metagraph Embedding}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9089251?casa_token=4RGA3jCG5zsAAAAA:oEZwjXhBIJreFavjipIsBtilcbT4v_yGF7oi9rVPWRP40c4s3R9KfQs_2jyNrP4Nln_k7ztEjnkg},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/ULI74JT3/9089251.html}
}

@inproceedings{michelPathNeuralNetworks2023,
  title = {Path {{Neural Networks}}: {{Expressive}} and {{Accurate Graph Neural Networks}}},
  shorttitle = {Path {{Neural Networks}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Michel, Gaspard and Nikolentzos, Giannis and Lutzeyer, Johannes F. and Vazirgiannis, Michalis},
  date = {2023-07-03},
  pages = {24737--24755},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/michel23a.html},
  urldate = {2024-02-24},
  abstract = {Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/ZR53J8K5/Michel et al. - 2023 - Path Neural Networks Expressive and Accurate Grap.pdf}
}

@article{minaReviewMachineLearning2022e,
  title = {A {{Review}} of {{Machine Learning Techniques}} in {{Analog Integrated Circuit Design Automation}}},
  author = {Mina, Rayan and Jabbour, Chadi and Sakr, George E.},
  date = {2022-01},
  journaltitle = {Electronics},
  volume = {11},
  number = {3},
  pages = {435},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics11030435},
  url = {https://www.mdpi.com/2079-9292/11/3/435},
  urldate = {2024-02-20},
  abstract = {Analog integrated circuit design is widely considered a time-consuming task due to the acute dependence of analog performance on the transistors’ and passives’ dimensions. An important research effort has been conducted in the past decade to reduce the front-end design cycles of analog circuits by means of various automation approaches. On the other hand, the significant progress in high-performance computing hardware has made machine learning an attractive and accessible solution for everyone. The objectives of this paper were: (1) to provide a comprehensive overview of the existing state-of-the-art machine learning techniques used in analog circuit sizing and analyze their effectiveness in achieving the desired goals; (2) to point out the remaining open challenges, as well as the most relevant research directions to be explored. Finally, the different analog circuits on which machine learning techniques were applied are also presented and their results discussed from a circuit designer perspective.},
  issue = {3},
  langid = {english},
  keywords = {analog IC design,automated circuit sizing,deep neural networks,machine learning},
  file = {/home/krawczuk/Zotero/storage/SEF5SAU4/Mina et al. - 2022 - A Review of Machine Learning Techniques in Analog .pdf}
}

@online{minderSALSACLRSSparseScalable2023,
  title = {{{SALSA-CLRS}}: {{A Sparse}} and {{Scalable Benchmark}} for {{Algorithmic Reasoning}}},
  shorttitle = {{{SALSA-CLRS}}},
  author = {Minder, Julian and Grötschla, Florian and Mathys, Joël and Wattenhofer, Roger},
  date = {2023-11-20},
  eprint = {2309.12253},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2309.12253},
  url = {http://arxiv.org/abs/2309.12253},
  urldate = {2024-02-26},
  abstract = {We introduce an extension to the CLRS algorithmic learning benchmark, prioritizing scalability and the utilization of sparse representations. Many algorithms in CLRS require global memory or information exchange, mirrored in its execution model, which constructs fully connected (not sparse) graphs based on the underlying problem. Despite CLRS's aim of assessing how effectively learned algorithms can generalize to larger instances, the existing execution model becomes a significant constraint due to its demanding memory requirements and runtime (hard to scale). However, many important algorithms do not demand a fully connected graph; these algorithms, primarily distributed in nature, align closely with the message-passing paradigm employed by Graph Neural Networks. Hence, we propose SALSA-CLRS, an extension of the current CLRS benchmark specifically with scalability and sparseness in mind. Our approach includes adapted algorithms from the original CLRS benchmark and introduces new problems from distributed and randomized algorithms. Moreover, we perform a thorough empirical evaluation of our benchmark. Code is publicly available at https://github.com/jkminder/SALSA-CLRS.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/CFVKSGA5/Minder et al. - 2023 - SALSA-CLRS A Sparse and Scalable Benchmark for Al.pdf;/home/krawczuk/Zotero/storage/8L4ZK95Y/2309.html}
}

@online{mirjanicLatentSpaceRepresentations2023a,
  title = {Latent {{Space Representations}} of {{Neural Algorithmic Reasoners}}},
  author = {Mirjanić, Vladimir V. and Pascanu, Razvan and Veličković, Petar},
  date = {2023-07-17},
  eprint = {2307.08874},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2307.08874},
  url = {http://arxiv.org/abs/2307.08874},
  urldate = {2024-02-26},
  abstract = {Neural Algorithmic Reasoning (NAR) is a research area focused on designing neural architectures that can reliably capture classical computation, usually by learning to execute algorithms. A typical approach is to rely on Graph Neural Network (GNN) architectures, which encode inputs in high-dimensional latent spaces that are repeatedly transformed during the execution of the algorithm. In this work we perform a detailed analysis of the structure of the latent space induced by the GNN when executing algorithms. We identify two possible failure modes: (i) loss of resolution, making it hard to distinguish similar values; (ii) inability to deal with values outside the range observed during training. We propose to solve the first issue by relying on a softmax aggregator, and propose to decay the latent space in order to deal with out-of-range values. We show that these changes lead to improvements on the majority of algorithms in the standard CLRS-30 benchmark when using the state-of-the-art Triplet-GMPNN processor. Our code is available at \textbackslash href\{https://github.com/mirjanic/nar-latent-spaces\}\{https://github.com/mirjanic/nar-latent-spaces\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/5J8UL7LG/Mirjanić et al. - 2023 - Latent Space Representations of Neural Algorithmic.pdf;/home/krawczuk/Zotero/storage/KPYRJM3D/2307.html}
}

@inproceedings{miteaAutomatedConstraintdrivenTopology2011a,
  title = {Automated Constraint-Driven Topology Synthesis for Analog Circuits},
  booktitle = {2011 {{Design}}, {{Automation}} \& {{Test}} in {{Europe}}},
  author = {Mitea, Oliver and Meissner, Markus and Hedrich, Lars and Jores, Peter},
  date = {2011-03},
  pages = {1--4},
  issn = {1558-1101},
  doi = {10.1109/DATE.2011.5763264},
  url = {https://ieeexplore.ieee.org/abstract/document/5763264?casa_token=FCUFCCq0IToAAAAA:jXu9dEY_XxgfJymLAYjN0_-vgyvirF4Yu0xNS5d9uNVxqQf9PV_ccPxoLD5-q0WuM-6vt39MvWTp},
  urldate = {2024-02-19},
  abstract = {This contribution will present a fully automated approach for explorative topology synthesis of small analog circuit blocks. Circuits are composed from a library of basic building blocks. Therefore, various algorithms are used to explore the entire design space, even allowing to generate unusual circuits. Correct combination of the basic blocks is accomplished through generic electrical rules, which ensure the fundamental electrical functionality of the generated circuit. Additionally, symmetry constraints are introduced to narrow the design space, which leads to more reasonable circuits. Further a replaceable bias-voltage generator is included into the circuit to replicate real world circumstances. For the first evaluation and selection of best candidate circuits, fast symbolic analysis techniques are used. The final sizing is done through a parallelized industrial based sizing method. Experimental results show the feasibility of this synthesis approach.},
  eventtitle = {2011 {{Design}}, {{Automation}} \& {{Test}} in {{Europe}}},
  keywords = {Algorithm design and analysis,Analog circuits,Circuit topology,Design automation,Generators,Libraries,Topology},
  file = {/home/krawczuk/Zotero/storage/R2JA3ILU/Mitea et al. - 2011 - Automated constraint-driven topology synthesis for.pdf;/home/krawczuk/Zotero/storage/WRFNQTBG/5763264.html}
}

@incollection{moDevelopmentWorldIC2024,
  title = {Development of {{World IC Industry}}},
  booktitle = {Handbook of {{Integrated Circuit Industry}}},
  author = {Mo, Da-Kang and Li, Ke},
  editor = {Wang, Yangyuan and Chi, Min-Hwa and Lou, Jesse Jen-Chung and Chen, Chun-Zhang},
  date = {2024},
  pages = {45--79},
  publisher = {{Springer Nature}},
  location = {{Singapore}},
  doi = {10.1007/978-981-99-2836-1_4},
  url = {https://doi.org/10.1007/978-981-99-2836-1_4},
  urldate = {2024-02-23},
  abstract = {The major IC enterprises facing to the market directly are fabless design enterprises (no production line), integrated device manufacturers (IDM), and intellectual property (IP) providers. EDA enterprises primarily provide design methodologies and tool suites but not for providing chip fabrication services, while wafer foundry (or Fab) can provide IC fabrication services. In the IC industry chain, IPs are commonly provided as process-proven and can be embedded in chips or known good dies (KGD); packaging and testing companies mainly provide services for Fabs, and IDM companies, materials companies, and special equipment companies mainly provide the required materials and equipment, respectively, for chip manufacturers. The industry is subject to the influences by several semiconductor organizations, such as World Semiconductor Council (WSC), Semiconductor Equipment and Materials International (SEMI), and Global Semiconductor Association (GSA). Additionally, there are also several IC market research and consulting companies, World Semiconductor Trade Statistics (WSTS), IC Insights, Gartner, Trend Force, and Yole, whose roles as if are IC liaisons in between IC designs and research, chip manufacturing, and product marketing.},
  isbn = {978-981-9928-36-1},
  langid = {english},
  keywords = {Fabless,Foundry,GSA,Integrated device manufacturers (IDM),Intellectual property (IP),SEMI}
}

@article{mondalEquivariantAdaptationLarge2024,
  title = {Equivariant {{Adaptation}} of {{Large Pretrained Models}}},
  author = {Mondal, Arnab Kumar and Panigrahi, Siba Smarak and Kaba, Oumar and Mudumba, Sai Rajeswar and Ravanbakhsh, Siamak},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d5856318032ef3630cb580f4e24f823-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/X7CNH72X/Mondal et al. - 2024 - Equivariant Adaptation of Large Pretrained Models.pdf}
}

@software{montorsiF18mNetlistviewer2024,
  title = {F18m/Netlist-Viewer},
  author = {Montorsi, Francesco},
  date = {2024-02-22T13:58:51Z},
  origdate = {2023-08-22T22:05:14Z},
  url = {https://github.com/f18m/netlist-viewer},
  urldate = {2024-02-26},
  abstract = {SPICE netlist visualizer},
  keywords = {circuits,circuits-and-electronics,electrical-engineering,electronics,netlists,spice}
}

@thesis{murphyNovelFrameworkInvariant2021,
  type = {thesis},
  title = {A {{Novel Framework}} for {{Invariant Neural Networks Applied}} to {{Graph}} and {{Set Data}}},
  author = {Murphy, Ryan L.},
  date = {2021-04-29},
  institution = {{Purdue University Graduate School}},
  doi = {10.25394/PGS.14502192.v1},
  url = {https://hammer.purdue.edu/articles/thesis/A_Novel_Framework_for_Invariant_Neural_Networks_Applied_to_Graph_and_Set_Data/14502192/1},
  urldate = {2024-02-26},
  abstract = {The generalization performance of neural networks can be improved by respecting invariances inherent in a problem. For instance, it is often the case that rotating an imagedoes not change its meaning; the target variable is likely invariant to rotations of the input. Models for graph and set data are typically designed to be permutation-invariant, since the order of set elements and isomorphisms of graphs usually do not change their meaning, but a unified paradigm for both types of data is lacking. Moreover, existing models are either insufficiently flexible or difficult to reliably train in practice. This dissertation presents Janossy pooling (JP), a novel framework for training neural networks that are (approximately) invariant to a prespecified finite group of transformations (e.g., permutations), with a focus on graphs and sets. Motivated by partial exchangeability and Janossy densities, we view transformation-invariant models as averages over all transformations of the input, each passed to a transformation-sensitive function. As the set of transformations can be very large, we advance three approximation strategies: π-Stochastic Gradient Descent (π-SGD), k-ary approximations, and poly-canonical orderings. Compared to state-of-the-art approaches, JP is capable of expressing a richer class of models than Message Passing Graph Neural Networks and does not necessitate the use of highly complex and discontinuous functions for modeling sets effectively. Empirical evidence, including experiments on molecular and protein-protein-interaction datasets, supports the theory we develop and the practical benefits of JP. However, on another note, it may be unclear whether enforcing invariances is appropriate in any given task, such as in the case of time-evolving graphs.Accordingly, we propose a data-driven scheme where the extent of transformation sensitivity is determined by a regularization hyperparameter. This approach invokes the Birkhoff-von Neumann theorem, making it directly applicable to both graphs and sets, and establishes a link between existing methods and infinitely strong regularization. Finally, we verify this approach experimentally.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/586QSDPF/Murphy - 2021 - A Novel Framework for Invariant Neural Networks Ap.pdf}
}

@incollection{mutzel19GraphDrawing2004,
  title = {19. {{Graph Drawing}}: {{Exact Optimization Helps}}!},
  shorttitle = {19. {{Graph Drawing}}},
  booktitle = {The {{Sharpest Cut}}},
  author = {Mutzel, Petra and Jünger, Michael},
  date = {2004-01},
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  pages = {327--352},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898718805.ch19},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718805.ch19},
  urldate = {2024-02-22},
  abstract = {19.1 Introduction Graph drawing deals with the design and implementation of algorithms for generating automatic layouts of graphs that can be read and understood easily. A good drawing should reveal the structure of the given graph. With applications in business process modeling, software (re-)engineering, and database design, the field of graph drawing is becoming increasingly important. Figure 19.1(a) shows a diagram of the dependencies of the electric power industry as it appeared in a German newspaper [51], while Figure 19.1(b) shows an automatically generated layout of the same diagram. Figure 19.2(a) shows the original drawing of a unified modeling language (UML) diagram taken from [24], while Figure 19.2(b) shows an automatically generated layout of the same diagram. It is difficult to model the niceness of a layout, since this often depends on the particular application. However, there exist some criteria that are commonly accepted as important. The vertices should be evenly distributed over the space, overlaps between nodes and other objects should be avoided, and the lengths of the edges and the drawing area should be small. Among the most important criteria is a small number of edge crossings.},
  isbn = {978-0-89871-552-1},
  file = {/home/krawczuk/Zotero/storage/M3DG2ZNA/Mutzel and Jünger - 2004 - 19. Graph Drawing Exact Optimization Helps!.pdf}
}

@article{nakamotoCalQLCalibratedOffline2023,
  title = {Cal-{{QL}}: {{Calibrated Offline RL Pre-Training}} for {{Efficient Online Fine-Tuning}}},
  shorttitle = {Cal-{{QL}}},
  author = {Nakamoto, Mitsuhiko and Zhai, Simon and Singh, Anikait and Sobol Mark, Max and Ma, Yi and Finn, Chelsea and Kumar, Aviral and Levine, Sergey},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/c44a04289beaf0a7d968a94066a1d696-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/ZXTK8B9J/Nakamoto et al. - 2023 - Cal-QL Calibrated Offline RL Pre-Training for Eff.pdf}
}

@online{NetworkRoutingOptimization,
  title = {Network {{Routing Optimization Based}} on {{Machine Learning Using Graph Networks Robust}} against {{Topology Change}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/9016573},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/JA8XF8X3/9016573.html}
}

@online{NeulftNovelApproach,
  title = {Neulft: {{A Novel Approach}} to {{Nonlinear Canonical Polyadic Decomposition}} on {{High-Dimensional Incomplete Tensors}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9779437?casa_token=gNy0YjrACWcAAAAA:ojNOGgD1Fsk-4OH8BuhC0xpmsoSQx3spKqNhhVa15_tdxL9BFgH-iEmFwHWSFQade9IaMg8sI6gg},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/TSELBYDV/9779437.html}
}

@inproceedings{neunerLibraryfreeStructureRecognition2021,
  title = {Library-Free {{Structure Recognition}} for {{Analog Circuits}}},
  booktitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Neuner, Maximilian and Abel, Inga and Graeb, Helmut},
  date = {2021-02-01},
  pages = {1366--1371},
  publisher = {{IEEE}},
  location = {{Grenoble, France}},
  doi = {10.23919/DATE51398.2021.9474102},
  url = {https://ieeexplore.ieee.org/document/9474102/},
  urldate = {2024-02-20},
  abstract = {Extracting structural information of a design is one crucial aspect of many circuit verification and synthesis methods. State-of-the-art structure recognition methods use a predefined building block library to identify the basic building blocks in a circuit. However, the capability of these algorithms is limited by the scope, correctness and completeness of the provided library. This paper presents a new method to automatically generate the recognition rules required to identify a given circuit topology in a large design. Device pairs are grouped into building blocks by analyzing their characteristics, e.g., their connectivity, to enable a structure recognition as unambiguous as possible. The resulting blocks are consecutively assembled to larger blocks until the full building block description of the given topology has been established. Building block libraries dedicated to one specific topology type, e.g., operational amplifiers, can be obtained by applying the method to its basic version, subsequently extending the generated library by the additional elements required to identify its topology variants using the presented method. Experimental results for six folded cascode amplifier and five level shifter topologies are given.},
  eventtitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  isbn = {978-3-9819263-5-4},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/H5ELFFFM/Neuner et al. - 2021 - Library-free Structure Recognition for Analog Circ.pdf}
}

@article{newmanFindingEvaluatingCommunity2004,
  title = {Finding and Evaluating Community Structure in Networks},
  author = {Newman, M. E. J. and Girvan, M.},
  date = {2004-02-26},
  journaltitle = {Physical Review E},
  shortjournal = {Phys. Rev. E},
  volume = {69},
  number = {2},
  pages = {026113},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevE.69.026113},
  url = {https://link.aps.org/doi/10.1103/PhysRevE.69.026113},
  urldate = {2024-02-25},
  abstract = {We propose and study a set of algorithms for discovering community structure in networks—natural divisions of network nodes into densely connected subgroups. Our algorithms all share two definitive features: first, they involve iterative removal of edges from the network to split it into communities, the edges removed being identified using any one of a number of possible “betweenness” measures, and second, these measures are, crucially, recalculated after each removal. We also propose a measure for the strength of the community structure found by our algorithms, which gives us an objective metric for choosing the number of communities into which a network should be divided. We demonstrate that our algorithms are highly effective at discovering community structure in both computer-generated and real-world network data, and show how they can be used to shed light on the sometimes dauntingly complex structure of networked systems.},
  file = {/home/krawczuk/Zotero/storage/WX6H5QJ4/Newman and Girvan - 2004 - Finding and evaluating community structure in netw.pdf;/home/krawczuk/Zotero/storage/D4LH64XI/PhysRevE.69.html}
}

@article{nikishinDeepReinforcementLearning2023a,
  title = {Deep {{Reinforcement Learning}} with {{Plasticity Injection}}},
  author = {Nikishin, Evgenii and Oh, Junhyuk and Ostrovski, Georg and Lyle, Clare and Pascanu, Razvan and Dabney, Will and Barreto, Andre},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/75101364dc3aa7772d27528ea504472b-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YJBZZ8KI/Nikishin et al. - 2023 - Deep Reinforcement Learning with Plasticity Inject.pdf}
}

@inproceedings{niuPermutationInvariantGraph2020b,
  title = {Permutation {{Invariant Graph Generation}} via {{Score-Based Generative Modeling}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  date = {2020-06-03},
  pages = {4474--4484},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v108/niu20a.html},
  urldate = {2024-02-24},
  abstract = {Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/BU6Q5TN3/Niu et al. - 2020 - Permutation Invariant Graph Generation via Score-B.pdf;/home/krawczuk/Zotero/storage/QRJAS4HU/Niu et al. - 2020 - Permutation Invariant Graph Generation via Score-B.pdf}
}

@online{NovelCircuitTopology,
  title = {Novel Circuit Topology Synthesis Method Using Circuit Feature Mining and Symbolic Comparison | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6800231?casa_token=akoAiJ_81HQAAAAA:waBB34WJe20EgDuESVSR8xnrvHaP718KEb7m87Vv3S9Zbpq9dn18PVRP_yE_ckfngQA3lLhyGf_L},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/GF8XXTFD/6800231.html}
}

@book{ochottaPracticalSynthesisHighPerformance2012,
  title = {Practical {{Synthesis}} of {{High-Performance Analog Circuits}}},
  author = {Ochotta, Emil S. and Mukherjee, Tamal and Rutenbar, Rob A. and Carley, L. Richard},
  date = {2012-12-06},
  eprint = {WAfTBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Practical Synthesis of High-Performance Analog Circuits presents a technique for automating the design of analog circuits.  Market competition and the astounding pace of technological innovation exert tremendous pressure on circuit design engineers to turn ideas into products quickly and get them to market. In digital Application Specific Integrated Circuit (ASIC) design, computer aided design (CAD) tools have substantially eased this pressure by automating many of the laborious steps in the design process, thereby allowing the designer to maximise his design expertise.  But the world is not solely digital.  Cellular telephones, magnetic disk drives, neural networks and speech recognition systems are a few of the recent technological innovations that rely on a core of analog circuitry and exploit the density and performance of mixed analog/digital ASICs. To maximize profit, these mixed-signal ASICs must also make it to market as quickly as possible. However, although the engineer working on the digital portion of the ASIC can rely on sophisticated CAD tools to automate much of the design process, there is little help for the engineer working on the analog portion of the chip. With the exception of simulators to verify the circuit design when it is complete, there are almost no general purpose CAD tools that an analog design engineer can take advantage of to automate the analog design flow and reduce his time to market.  Practical Synthesis of High-Performance Analog Circuits presents a new variation-tolerant analog synthesis strategy that is a significant step towards ending the wait for a practical analog synthesis tool. A new synthesis strategy is presented that can fully automate the path from a circuit topology and performance specifications to a sized variation-tolerant circuit schematic. This strategy relies on asymptotic waveform evaluation to predict circuit performance and simulated annealing to solve a novel non-linear infinite programming optimization formulation of the circuit synthesis problem via a sequence of smaller optimization problems.  Practical Synthesis of High-Performance Analog Circuits will be of interest to analog circuit designers, CAD/EDA industry professionals, academics and students.},
  isbn = {978-1-4615-5565-0},
  langid = {english},
  pagetotal = {308},
  keywords = {{Computers / Design, Graphics \& Media / CAD-CAM},Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Circuits / General}
}

@online{OpenSAROpenSource,
  title = {{{OpenSAR}}: {{An Open Source Automated End-to-end SAR ADC Compiler}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643494?casa_token=vdB_Xn_qm0UAAAAA:BSytjW9dx6dpMX8eJP5851lyqjRebOMuql_c0nx_j8rM-oCnbAtSSpK110B5yHJevetfYWeMSYQZ},
  urldate = {2024-02-20}
}

@online{oseAnalogICDesigna,
  title = {Analog {{IC Design}}},
  author = {OSE, IEEE SSCS},
  url = {https://sscs-ose.github.io/analog/},
  urldate = {2024-02-26},
  abstract = {IEEE SSCS Open-Source Ecosystem},
  langid = {english},
  organization = {{SSCS Open-Source Ecosystem}},
  file = {/home/krawczuk/Zotero/storage/NGR8EYLR/analog.html}
}

@article{ousterhoutMagicVLSILayout1985,
  title = {The {{Magic VLSI Layout System}}},
  author = {Ousterhout, John K. and Hamachi, Gordon T. and Mayo, Robert N. and Scott, Walter S. and Taylor, George S.},
  date = {1985-02},
  journaltitle = {IEEE Design \& Test of Computers},
  volume = {2},
  number = {1},
  pages = {19--30},
  issn = {1558-1918},
  doi = {10.1109/MDT.1985.294681},
  url = {https://ieeexplore.ieee.org/abstract/document/4069504?casa_token=fI3poE5vN7oAAAAA:vNmXStmnwiqMtO_nfw0puR2r4cM4ef2OoAGBDvfEkJBrZDhaNF2tfe1yhjjPUJDA-nEYKLtrhXVk},
  urldate = {2024-02-26},
  abstract = {Magic is a new IC layout system that includes several facilities traditionally contained in separate batch-processing programs. Magic incorporates expertise about design rules, connectivity, and routing directly into the layout editor and uses this information to provide several unusual features. They include a continuous design-rule checker that operates in background and maintains an up-to-date picture of violations; a hierarchical circuit extractor that only re-extracts portions of the circuit that have changed; an operation called plowing that permits interactive stretching and compaction; and a suite of routing tools that can work under and around existing connections in the channels. A design style called logs and a data structure called corner stitching are used to achieve an efficient implementation of the system.},
  eventtitle = {{{IEEE Design}} \& {{Test}} of {{Computers}}},
  keywords = {Application specific integrated circuits,Circuit simulation,Data mining,Data structures,Design automation,Large scale integration,Routing,Spatial databases,Very large scale integration,Wires},
  file = {/home/krawczuk/Zotero/storage/9KJTUVRY/Ousterhout et al. - 1985 - The Magic VLSI Layout System.pdf;/home/krawczuk/Zotero/storage/HUF7GSA6/4069504.html}
}

@online{paineMakingEfficientUse2019a,
  title = {Making {{Efficient Use}} of {{Demonstrations}} to {{Solve Hard Exploration Problems}}},
  author = {Paine, Tom Le and Gulcehre, Caglar and Shahriari, Bobak and Denil, Misha and Hoffman, Matt and Soyer, Hubert and Tanburn, Richard and Kapturowski, Steven and Rabinowitz, Neil and Williams, Duncan and Barth-Maron, Gabriel and Wang, Ziyu and family=Freitas, given=Nando, prefix=de, useprefix=true and Team, Worlds},
  date = {2019-09-03},
  eprint = {1909.01387},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1909.01387},
  url = {http://arxiv.org/abs/1909.01387},
  urldate = {2024-02-26},
  abstract = {This paper introduces R2D3, an agent that makes efficient use of demonstrations to solve hard exploration problems in partially observable environments with highly variable initial conditions. We also introduce a suite of eight tasks that combine these three properties, and show that R2D3 can solve several of the tasks where other state of the art methods (both with and without demonstrations) fail to see even a single successful trajectory after tens of billions of steps of exploration.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/U8KHUDRQ/Paine et al. - 2019 - Making Efficient Use of Demonstrations to Solve Ha.pdf;/home/krawczuk/Zotero/storage/FZBWY6NK/1909.html}
}

@online{panEDALearnComprehensiveRTLtoSignoff2023,
  title = {{{EDALearn}}: {{A Comprehensive RTL-to-Signoff EDA Benchmark}} for {{Democratized}} and {{Reproducible ML}} for {{EDA Research}}},
  shorttitle = {{{EDALearn}}},
  author = {Pan, Jingyu and Chang, Chen-Chia and Xie, Zhiyao and Chen, Yiran},
  date = {2023-12-04},
  eprint = {2312.01674},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2312.01674},
  url = {http://arxiv.org/abs/2312.01674},
  urldate = {2024-02-25},
  abstract = {The application of Machine Learning (ML) in Electronic Design Automation (EDA) for Very Large-Scale Integration (VLSI) design has garnered significant research attention. Despite the requirement for extensive datasets to build effective ML models, most studies are limited to smaller, internally generated datasets due to the lack of comprehensive public resources. In response, we introduce EDALearn, the first holistic, open-source benchmark suite specifically for ML tasks in EDA. This benchmark suite presents an end-to-end flow from synthesis to physical implementation, enriching data collection across various stages. It fosters reproducibility and promotes research into ML transferability across different technology nodes. Accommodating a wide range of VLSI design instances and sizes, our benchmark aptly represents the complexity of contemporary VLSI designs. Additionally, we provide an in-depth data analysis, enabling users to fully comprehend the attributes and distribution of our data, which is essential for creating efficient ML models. Our contributions aim to encourage further advances in the ML-EDA domain.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/NUZ3PA33/Pan et al. - 2023 - EDALearn A Comprehensive RTL-to-Signoff EDA Bench.pdf;/home/krawczuk/Zotero/storage/4DBHCPK7/2312.html}
}

@inproceedings{papezSumProductSetNetworks2023,
  title = {Sum-{{Product-Set Networks}}},
  author = {Papez, Milan and Rektoris, Martin and Pevný, Tomáš and Smidl, Vaclav},
  date = {2023-07-13},
  url = {https://openreview.net/forum?id=8hqxY5fUwg},
  urldate = {2024-02-20},
  abstract = {Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.},
  eventtitle = {The 6th {{Workshop}} on {{Tractable Probabilistic Modeling}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/32AFN8B4/Papez et al. - 2023 - Sum-Product-Set Networks.pdf}
}

@article{parkSemiconductorsIntersectionGeoeconomics2023,
  title = {Semiconductors at the {{Intersection}} of {{Geoeconomics}}, {{Technonationalism}}, and {{Global Value Chains}}},
  author = {Park, Seohee},
  date = {2023-08},
  journaltitle = {Social Sciences},
  volume = {12},
  number = {8},
  pages = {466},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-0760},
  doi = {10.3390/socsci12080466},
  url = {https://www.mdpi.com/2076-0760/12/8/466},
  urldate = {2024-02-23},
  abstract = {This study provides a historical and contemporary analysis of the United States’ strategies in the global semiconductor industry, framed within Joseph Nye’s three-dimensional chessboard analysis. This study examines the strategic responses of the United States from the 1980s to the present, connecting these shifts to changes in international politics and geoeconomic alliances. It scrutinizes how the U.S. utilized its unipolar power to respond to Japan’s growing semiconductor industry influence in the 1980s and its adoption of free-market principles during the globalization era of the 1990s and 2000s. It further discusses how these multilateral shifts have led to a resurgence of technonationalism in the late 2010s, responding to asymmetric interdependence in the global value chain of the semiconductor industry. This research contributes to the comprehension of the dynamics of the industry within international politics and suggests insights into the ongoing Sino–American competition and strategic realignment in the sector.},
  issue = {8},
  langid = {english},
  keywords = {geoeconomics,global value chain (GVC),globalization,semiconductor industry,technonationalism},
  file = {/home/krawczuk/Zotero/storage/X4HWCMHE/Park - 2023 - Semiconductors at the Intersection of Geoeconomics.pdf}
}

@article{perezFiLMVisualReasoning2018b,
  title = {{{FiLM}}: {{Visual Reasoning}} with a {{General Conditioning Layer}}},
  shorttitle = {{{FiLM}}},
  author = {Perez, Ethan and Strub, Florian and family=Vries, given=Harm, prefix=de, useprefix=false and Dumoulin, Vincent and Courville, Aaron},
  date = {2018-04-29},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468},
  doi = {10.1609/aaai.v32i1.11671},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11671},
  urldate = {2024-02-25},
  abstract = {We introduce a general-purpose conditioning method for neural networks called FiLM: Feature-wise Linear Modulation. FiLM layers influence neural network computation via a simple, feature-wise affine transformation based on conditioning information. We show that FiLM layers are highly effective for visual reasoning - answering image-related questions which require a multi-step, high-level process - a task which has proven difficult for standard deep learning methods that do not explicitly model reasoning. Specifically, we show on visual reasoning tasks that FiLM layers 1) halve state-of-the-art error for the CLEVR benchmark, 2) modulate features in a coherent manner, 3) are robust to ablations and architectural modifications, and 4) generalize well to challenging, new data from few examples or even zero-shot.},
  issue = {1},
  langid = {english},
  keywords = {Language and Vision},
  file = {/home/krawczuk/Zotero/storage/FP4YYKYA/Perez et al. - 2018 - FiLM Visual Reasoning with a General Conditioning.pdf}
}

@article{petracheApproximationGeneralizationTradeoffsApproximate2023,
  title = {Approximation-{{Generalization Trade-offs}} under ({{Approximate}}) {{Group Equivariance}}},
  author = {Petrache, Mircea and Trivedi, Shubhendu},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/c35f8e2fc6d81f195009a1d2ae5f6ae9-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/C7BI33EG/Petrache and Trivedi - 2023 - Approximation-Generalization Trade-offs under (App.pdf}
}

@inproceedings{piechDeepKnowledgeTracing2015,
  title = {Deep {{Knowledge Tracing}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas J and Sohl-Dickstein, Jascha},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html},
  urldate = {2024-02-22},
  abstract = {Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.In this paper we explore the benefit of using recurrent neural networks to model student learning.This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.We show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.These results suggest a promising new line of research for knowledge tracing.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/TYMUETP9/Piech et al. - 2015 - Deep Knowledge Tracing.pdf}
}

@incollection{pilipczukComputingTreeDecompositions2020,
  title = {Computing {{Tree Decompositions}}},
  booktitle = {Treewidth, {{Kernels}}, and {{Algorithms}}: {{Essays Dedicated}} to {{Hans L}}. {{Bodlaender}} on the {{Occasion}} of {{His}} 60th {{Birthday}}},
  author = {Pilipczuk, Michał},
  editor = {Fomin, Fedor V. and Kratsch, Stefan and family=Leeuwen, given=Erik Jan, prefix=van, useprefix=true},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {189--213},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-42071-0_14},
  url = {https://doi.org/10.1007/978-3-030-42071-0_14},
  urldate = {2024-02-20},
  abstract = {In this chapter we review the most important algorithmic approaches to the following problem: given a graph G, compute a tree decomposition of G of (nearly) optimum width. We present the 4-approximation algorithm running in time \$\$\textbackslash mathcal \{O\}(27\^{}k\textbackslash cdot k\^{}2\textbackslash cdot n\^{}2)\$\$O(27k·k2·n2), which was first proposed by Robertson and Seymour in the Graph Minors series, and we discuss the main ideas behind the exact algorithm of Bodlaender that runs in linear fixed-parameter time~[2].},
  isbn = {978-3-030-42071-0},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/4I9E6FXP/Pilipczuk - 2020 - Computing Tree Decompositions.pdf}
}

@misc{piveteauMethodInterfacingHardware2022,
  title = {Method for Interfacing with Hardware Accelerators},
  author = {Piveteau, Christophe and Ioannou, Nikolas and Krawczuk, Igor and Le Gallo-Bourdeau, Manuel and Sebastian, Abu and Eleftheriou, Evangelos Stavros},
  date = {2022-02-15},
  url = {https://patents.google.com/patent/US11250107/en},
  urldate = {2024-02-27},
  organization = {{Google Patents}},
  file = {/home/krawczuk/Zotero/storage/XSQS6NLJ/Piveteau et al. - 2022 - Method for interfacing with hardware accelerators.pdf}
}

@online{podellSDXLImprovingLatent2023a,
  title = {{{SDXL}}: {{Improving Latent Diffusion Models}} for {{High-Resolution Image Synthesis}}},
  shorttitle = {{{SDXL}}},
  author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Penna, Joe and Rombach, Robin},
  date = {2023-07-04},
  eprint = {2307.01952},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2307.01952},
  urldate = {2024-02-25},
  abstract = {We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/krawczuk/Zotero/storage/TXA5ARXF/Podell et al. - 2023 - SDXL Improving Latent Diffusion Models for High-R.pdf;/home/krawczuk/Zotero/storage/CQIZPC26/2307.html}
}

@article{pratherTreeCircuits1965,
  title = {On {{Tree Circuits}}},
  author = {Prather, Ronald E.},
  date = {1965-12},
  journaltitle = {IEEE Transactions on Electronic Computers},
  volume = {EC-14},
  number = {6},
  pages = {841--851},
  issn = {0367-7508},
  doi = {10.1109/PGEC.1965.264078},
  url = {https://ieeexplore.ieee.org/abstract/document/4038603?casa_token=T7OMi2npWwcAAAAA:815vqzdNgMR9_rp6Zx4rJ0KfWSqjC7CEzTlv9po0jZLCPu8gVCmdvjmdf5foIFWNizQBdTuOMq5l},
  urldate = {2024-02-20},
  abstract = {This article is primarily concerned with means for finding economical tree circuit realizations-iterative applications of decompositions f(xn, xn-1,..., x1) = Fi(Gi(xn, xn-1,..., xi,..., x1), Hi,(xn, xn-1,..., xi,..., x1), xi) and their associated circuitry-for Boolean functions f(xn, xn-1,..., x1). A uniform estimate or inequality shows that when synthesis is effected with two-input-per-gate circuitry, tree circuits are preferable to those which result from conventional sum of products techniques. The practical significance of the estimate is illustrated by its application to the tree circuit synthesis problem. The relationship of tree circuit theory to decomposition theory is established and the extension of the present theory to the corresponding multiple-output and incompletely-specified problems is indicated.},
  eventtitle = {{{IEEE Transactions}} on {{Electronic Computers}}},
  keywords = {Application software,Boolean functions,Circuit synthesis,Circuit theory,Cost function,Diodes}
}

@article{prautschGeneratingGeneratorUserDriven2023,
  title = {Generating the {{Generator}}: {{A User-Driven}} and {{Template-Based Approach}} towards {{Analog Layout Automation}}},
  shorttitle = {Generating the {{Generator}}},
  author = {Prautsch, Benjamin and Eichler, Uwe and Hatnik, Uwe},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {4},
  pages = {1047},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics12041047},
  url = {https://www.mdpi.com/2079-9292/12/4/1047},
  urldate = {2024-02-19},
  abstract = {Various analog design automation attempts have addressed the shortcomings of the still largely manual and, thus, inefficient and risky analog design approach. These methods can roughly be divided into synthesis and procedural generation. An important key aspect has, however, rarely been considered: usability. While synthesis requires sophisticated constraints, procedural generators require expert programmers. Both prevent users from adopting the respective method. Thus, we propose a new approach to automatically create procedural generators in a user-driven way. First, analog generators, which also create symbols and layouts, are utilized during schematic entry to encapsulate common analog building blocks. Second, automatic code creation builds a hierarchical generator for all views with the schematic as input. Third, the approach links the building block generators with the layout through an object-oriented template library that is accessible through generator parameters, allowing the user to control the arrangement. No programming is required to reach this state. We believe that our approach will significantly ease the transition of analog designers to procedural generation. At the same time, the templates allow for a “bridge” to open frameworks and synthesis approaches so that the methodologies can be both better spread and combined. This way, comprehensive frameworks of both synthesis-based and procedural-based analog automation methods can be built in a user-driven way, and designers are enabled to gain early layout insight and ease IP reusability.},
  issue = {4},
  langid = {english},
  keywords = {analog layout,code generation,design automation,EDA,generators,IC design,reuse,templates,usability},
  file = {/home/krawczuk/Zotero/storage/FDLRB8W5/Prautsch et al. - 2023 - Generating the Generator A User-Driven and Templa.pdf}
}

@inreference{ProgramSynthesis2023,
  title = {Program Synthesis},
  booktitle = {Wikipedia},
  date = {2023-11-14T22:02:39Z},
  url = {https://en.wikipedia.org/w/index.php?title=Program_synthesis&oldid=1185150452},
  urldate = {2024-02-20},
  abstract = {In computer science, program synthesis is the task to construct a program that provably satisfies a given high-level formal specification. In contrast to program verification, the program is to be constructed rather than given; however, both fields make use of formal proof techniques, and both comprise approaches of different degrees of automation. In contrast to automatic programming techniques, specifications in program synthesis are usually non-algorithmic statements in an appropriate logical calculus.The primary application of program synthesis is to relieve the programmer of the burden of writing correct, efficient code that satisfies a specification. However, program synthesis also has applications to superoptimization and inference of loop invariants.},
  langid = {english},
  annotation = {Page Version ID: 1185150452},
  file = {/home/krawczuk/Zotero/storage/NU5V4JLC/Program_synthesis.html}
}

@thesis{puertasDirectTreeDecomposition2014,
  type = {Doctoral thesis},
  title = {Direct Tree Decomposition of Geometric Constraint Graphs},
  author = {Puertas, Tarres and Isabel, Marta},
  date = {2014-12-18},
  journaltitle = {TDX (Tesis Doctorals en Xarxa)},
  institution = {{Universitat Politècnica de Catalunya}},
  doi = {10.5821/dissertation-2117-95581},
  url = {https://upcommons.upc.edu/handle/2117/95581},
  urldate = {2024-02-20},
  abstract = {DOI: 10.5821/dissertation-2117-95581},
  langid = {english},
  keywords = {Algorismes computacionals,Àrees temàtiques de la UPC::Informàtica,Disseny assistit per ordinador,Geometria computacional,Grafs,Sistemes CAD-CAM,Teoria de},
  annotation = {Accepted: 2015-01-14T12:39:14Z},
  file = {/home/krawczuk/Zotero/storage/LGC7URIF/Puertas and Isabel - 2014 - Direct tree decomposition of geometric constraint .pdf}
}

@inproceedings{puLearningLearnGraph2021,
  title = {Learning to {{Learn Graph Topologies}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pu, Xingyue and Cao, Tianyue and Zhang, Xiaoyun and Dong, Xiaowen and Chen, Siheng},
  date = {2021},
  volume = {34},
  pages = {4249--4262},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/21e4ef94f2a6b23597efabaec584b504-Abstract.html},
  urldate = {2024-02-20},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/HR7KWEZT/Pu et al. - 2021 - Learning to Learn Graph Topologies.pdf}
}

@article{qinBenefitsPermutationEquivarianceAuction2022,
  title = {Benefits of {{Permutation-Equivariance}} in {{Auction Mechanisms}}},
  author = {Qin, Tian and He, Fengxiang and Shi, Dingfeng and Huang, Wenbing and Tao, Dacheng},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {18131--18142},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/730d61b4d9ff794a028fa3a25b9b891d-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/5NNA9ILM/Qin et al. - 2022 - Benefits of Permutation-Equivariance in Auction Me.pdf}
}

@online{qinSparseTrainingDiscrete2023e,
  title = {Sparse {{Training}} of {{Discrete Diffusion Models}} for {{Graph Generation}}},
  author = {Qin, Yiming and Vignac, Clement and Frossard, Pascal},
  date = {2023-11-03},
  eprint = {2311.02142},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.02142},
  url = {http://arxiv.org/abs/2311.02142},
  urldate = {2024-02-20},
  abstract = {Generative models for graphs often encounter scalability challenges due to the inherent need to predict interactions for every node pair. Despite the sparsity often exhibited by real-world graphs, the unpredictable sparsity patterns of their adjacency matrices, stemming from their unordered nature, leads to quadratic computational complexity. In this work, we introduce SparseDiff, a denoising diffusion model for graph generation that is able to exploit sparsity during its training phase. At the core of SparseDiff is a message-passing neural network tailored to predict only a subset of edges during each forward pass. When combined with a sparsity-preserving noise model, this model can efficiently work with edge lists representations of graphs, paving the way for scalability to much larger structures. During the sampling phase, SparseDiff iteratively populates the adjacency matrix from its prior state, ensuring prediction of the full graph while controlling memory utilization. Experimental results show that SparseDiff simultaneously matches state-of-the-art in generation performance on both small and large graphs, highlighting the versatility of our method.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/DZTTPU7D/Qin et al. - 2023 - Sparse Training of Discrete Diffusion Models for G.pdf;/home/krawczuk/Zotero/storage/CF5ZF7XN/2311.html}
}

@article{qiuProgressPlacementOptimization2023,
  title = {Progress of {{Placement Optimization}} for {{Accelerating VLSI Physical Design}}},
  author = {Qiu, Yihang and Xing, Yan and Zheng, Xin and Gao, Peng and Cai, Shuting and Xiong, Xiaoming},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {2},
  pages = {337},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics12020337},
  url = {https://www.mdpi.com/2079-9292/12/2/337},
  urldate = {2024-02-25},
  abstract = {Placement is essential in very large-scale integration (VLSI) physical design, as it directly affects the design cycle. Despite extensive prior research on placement, achieving fast and efficient placement remains challenging because of the increasing design complexity. In this paper, we comprehensively review the progress of placement optimization from the perspective of accelerating VLSI physical design. It can help researchers systematically understand the VLSI placement problem and the corresponding optimization means, thereby advancing modern placement optimization research. We highlight emerging trends in modern placement-centric VLSI physical design flows, including placement optimizers and learning-based predictors. We first define the placement problem and review the classical placement algorithms, then discuss the application bottleneck of the classical placement algorithms in advanced technology nodes and give corresponding solutions. After that, we introduce the development of placement optimizers, including algorithm improvements and computational acceleration, pointing out that these two aspects will jointly promote accelerating VLSI physical design. We also present research working on learning-based predictors from various angles. Finally, we discuss the common and individual challenges encountered by placement optimizers and learning-based predictors.},
  issue = {2},
  langid = {english},
  keywords = {large-scale optimization,machine learning,physical design,placement challenges,prediction,VLSI placement},
  file = {/home/krawczuk/Zotero/storage/GCSA7U4W/Qiu et al. - 2023 - Progress of Placement Optimization for Acceleratin.pdf}
}

@article{rabinovichChurchSynthesisProblem2007,
  title = {The {{Church Synthesis Problem}} with {{Parameters}}},
  author = {Rabinovich, Alexander},
  date = {2007-11-14},
  journaltitle = {Logical Methods in Computer Science},
  volume = {Volume 3, Issue 4},
  eprint = {0708.3477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1233},
  issn = {1860-5974},
  doi = {10.2168/LMCS-3(4:9)2007},
  url = {http://arxiv.org/abs/0708.3477},
  urldate = {2024-02-20},
  abstract = {For a two-variable formula \&psi;(X,Y) of Monadic Logic of Order (MLO) the Church Synthesis Problem concerns the existence and construction of an operator Y=F(X) such that \&psi;(X,F(X)) is universally valid over Nat. B\textbackslash "\{u\}chi and Landweber proved that the Church synthesis problem is decidable; moreover, they showed that if there is an operator F that solves the Church Synthesis Problem, then it can also be solved by an operator defined by a finite state automaton or equivalently by an MLO formula. We investigate a parameterized version of the Church synthesis problem. In this version \&psi; might contain as a parameter a unary predicate P. We show that the Church synthesis problem for P is computable if and only if the monadic theory of},
  keywords = {Computer Science - Logic in Computer Science,F.4.1,F.4.3},
  file = {/home/krawczuk/Zotero/storage/IECGWSHX/Rabinovich - 2007 - The Church Synthesis Problem with Parameters.pdf;/home/krawczuk/Zotero/storage/4XQW8MDA/0708.html}
}

@inproceedings{ramezani-kebryaDistributedExtragradientOptimal2023,
  title = {Distributed Extra-Gradient with Optimal Complexity and Communication Guarantees},
  booktitle = {11th {{International Conference}} on {{Learning Representations}} ({{ICLR}})},
  author = {Ramezani-Kebrya, Ali and Antonakopoulos, Kimon and Krawczuk, Igor and Deschenaux, Justin and Cevher, Volkan},
  date = {2023},
  url = {https://infoscience.epfl.ch/record/300852},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/8WP3MLL2/Ramezani-Kebrya et al. - 2023 - Distributed extra-gradient with optimal complexity.pdf}
}

@article{rampasekRecipeGeneralPowerful2022b,
  title = {Recipe for a {{General}}, {{Powerful}}, {{Scalable Graph Transformer}}},
  author = {Rampášek, Ladislav and Galkin, Michael and Dwivedi, Vijay Prakash and Luu, Anh Tuan and Wolf, Guy and Beaini, Dominique},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {14501--14515},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/5d4834a159f1547b267a05a4e2b7cf5e-Abstract-Conference.html},
  urldate = {2024-02-25},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/JDNSTECB/Rampášek et al. - 2022 - Recipe for a General, Powerful, Scalable Graph Tra.pdf}
}

@inproceedings{rathDeepNeuralNetworks2023,
  title = {Deep {{Neural Networks}} with {{Efficient Guaranteed Invariances}}},
  booktitle = {Proceedings of {{The}} 26th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Rath, Matthias and Condurache, Alexandru Paul},
  date = {2023-04-11},
  pages = {2460--2480},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v206/rath23a.html},
  urldate = {2024-02-26},
  abstract = {We address the problem of improving the performance and in particular the sample complexity of deep neural networks by enforcing and guaranteeing invariances to symmetry transformations rather than learning them from data. Group-equivariant convolutions are a popular approach to obtain equivariant representations. The desired corresponding invariance is then imposed using pooling operations. For rotations, it has been shown that using invariant integration instead of pooling further improves the sample complexity. In this contribution, we first expand invariant integration beyond rotations to flips and scale transformations. We then address the problem of incorporating multiple desired invariances into a single network. For this purpose, we propose a multi-stream architecture, where each stream is invariant to a different transformation such that the network can simultaneously benefit from multiple invariances. We demonstrate our approach with successful experiments on Scaled-MNIST, SVHN, CIFAR-10 and STL-10.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/ZS5H4NCE/Rath and Condurache - 2023 - Deep Neural Networks with Efficient Guaranteed Inv.pdf}
}

@inproceedings{renWhyAreGraph2022b,
  title = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}? ({{Invited Paper}})},
  shorttitle = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}?},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Ren, Haoxing and Nath, Siddhartha and Zhang, Yanqing and Chen, Hao and Liu, Mingjie},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3561093},
  url = {https://dl.acm.org/doi/10.1145/3508352.3561093},
  urldate = {2024-02-20},
  abstract = {In this paper, we discuss the source of effectiveness of Graph Neural Networks (GNNs) in EDA, particularly in the VLSI design automation domain. We argue that the effectiveness comes from the fact that GNNs implicitly embed the prior knowledge and inductive biases associated with given VLSI tasks, which is one of the three approaches to make a learning algorithm physics-informed. These inductive biases are different to those common used in GNNs designed for other structured data, such as social networks and citation networks. We will illustrate this principle with several recent GNN examples in the VLSI domain, including predictive tasks such as switching activity prediction, timing prediction, parasitics prediction, layout symmetry prediction, as well as optimization tasks such as gate sizing and macro and cell transistor placement. We will also discuss the challenges of applications of GNN and the opportunity of applying self-supervised learning techniques with GNN for VLSI optimization.},
  isbn = {978-1-4503-9217-4},
  file = {/home/krawczuk/Zotero/storage/QW9J4549/Ren et al. - 2022 - Why are Graph Neural Networks Effective for EDA Pr.pdf}
}

@inproceedings{renWhyAreGraph2022c,
  title = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}? ({{Invited Paper}})},
  shorttitle = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}?},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Ren, Haoxing and Nath, Siddhartha and Zhang, Yanqing and Chen, Hao and Liu, Mingjie},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3561093},
  url = {https://dl.acm.org/doi/10.1145/3508352.3561093},
  urldate = {2024-02-24},
  abstract = {In this paper, we discuss the source of effectiveness of Graph Neural Networks (GNNs) in EDA, particularly in the VLSI design automation domain. We argue that the effectiveness comes from the fact that GNNs implicitly embed the prior knowledge and inductive biases associated with given VLSI tasks, which is one of the three approaches to make a learning algorithm physics-informed. These inductive biases are different to those common used in GNNs designed for other structured data, such as social networks and citation networks. We will illustrate this principle with several recent GNN examples in the VLSI domain, including predictive tasks such as switching activity prediction, timing prediction, parasitics prediction, layout symmetry prediction, as well as optimization tasks such as gate sizing and macro and cell transistor placement. We will also discuss the challenges of applications of GNN and the opportunity of applying self-supervised learning techniques with GNN for VLSI optimization.},
  isbn = {978-1-4503-9217-4}
}

@inproceedings{renWhyAreGraph2022d,
  title = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}? ({{Invited Paper}})},
  shorttitle = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}?},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Ren, Haoxing and Nath, Siddhartha and Zhang, Yanqing and Chen, Hao and Liu, Mingjie},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3561093},
  url = {https://dl.acm.org/doi/10.1145/3508352.3561093},
  urldate = {2024-02-24},
  abstract = {In this paper, we discuss the source of effectiveness of Graph Neural Networks (GNNs) in EDA, particularly in the VLSI design automation domain. We argue that the effectiveness comes from the fact that GNNs implicitly embed the prior knowledge and inductive biases associated with given VLSI tasks, which is one of the three approaches to make a learning algorithm physics-informed. These inductive biases are different to those common used in GNNs designed for other structured data, such as social networks and citation networks. We will illustrate this principle with several recent GNN examples in the VLSI domain, including predictive tasks such as switching activity prediction, timing prediction, parasitics prediction, layout symmetry prediction, as well as optimization tasks such as gate sizing and macro and cell transistor placement. We will also discuss the challenges of applications of GNN and the opportunity of applying self-supervised learning techniques with GNN for VLSI optimization.},
  isbn = {978-1-4503-9217-4}
}

@article{robertsonGraphMinorsObstructions1991,
  title = {Graph Minors. {{X}}. {{Obstructions}} to Tree-Decomposition},
  author = {Robertson, Neil and Seymour, P. D},
  date = {1991-07-01},
  journaltitle = {Journal of Combinatorial Theory, Series B},
  shortjournal = {Journal of Combinatorial Theory, Series B},
  volume = {52},
  number = {2},
  pages = {153--190},
  issn = {0095-8956},
  doi = {10.1016/0095-8956(91)90061-N},
  url = {https://www.sciencedirect.com/science/article/pii/009589569190061N},
  urldate = {2024-02-20},
  abstract = {Roughly, a graph has small “tree-width” if it can be constructed by piecing small graphs together in a tree structure. Here we study the obstructions to the existence of such a tree structure. We find, for instance: 1.(i) a minimax formula relating tree-width with the largest such obstructions2.(ii) an association between such obstructions and large grid minors of the graph3.(iii) a “tree-decomposition” of the graph into pieces corresponding with the obstructions. These results will be of use in later papers.}
}

@online{rodionovDiscreteNeuralAlgorithmic2024,
  title = {Discrete {{Neural Algorithmic Reasoning}}},
  author = {Rodionov, Gleb and Prokhorenkova, Liudmila},
  date = {2024-02-18},
  eprint = {2402.11628},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.11628},
  url = {http://arxiv.org/abs/2402.11628},
  urldate = {2024-02-26},
  abstract = {Neural algorithmic reasoning aims to capture computations with neural networks via learning the models to imitate the execution of classical algorithms. While common architectures are expressive enough to contain the correct model in the weights space, current neural reasoners are struggling to generalize well on out-of-distribution data. On the other hand, classical computations are not affected by distribution shifts as they can be described as transitions between discrete computational states. In this work, we propose to force neural reasoners to maintain the execution trajectory as a combination of finite predefined states. Trained with supervision on the algorithm's state transitions, such models are able to perfectly align with the original algorithm. To show this, we evaluate our approach on the SALSA-CLRS benchmark, where we get perfect test scores for all tasks. Moreover, the proposed architectural choice allows us to prove the correctness of the learned algorithms for any test data.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GK4I2P3Z/Rodionov and Prokhorenkova - 2024 - Discrete Neural Algorithmic Reasoning.pdf;/home/krawczuk/Zotero/storage/VGJ59I4N/2402.html}
}

@article{rodionovNeuralAlgorithmicReasoning2023,
  title = {Neural {{Algorithmic Reasoning Without Intermediate Supervision}}},
  author = {Rodionov, Gleb and Prokhorenkova, Liudmila},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/a2370db7c99791ad5d9f3ef48ad6d464-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/3JXL249K/Rodionov and Prokhorenkova - 2023 - Neural Algorithmic Reasoning Without Intermediate .pdf}
}

@article{rojecAnalogCircuitTopology2018,
  title = {Analog {{Circuit Topology Representation}} for {{Automated Synthesis}} and {{Optimization}}},
  author = {Rojec, Žiga and Olenšek, Jernej and Fajfar, Iztok},
  date = {2018-06-28},
  journaltitle = {Informacije MIDEM},
  volume = {48},
  number = {1},
  pages = {29--40},
  issn = {2232-6979},
  url = {https://ojs.midem-drustvo.si/index.php/InfMIDEM/article/view/465},
  urldate = {2024-02-19},
  abstract = {For several decades, computers have helped analog designers with circuit simulation and evaluation. To further simplify and speed-up designer’s work, novel methods are being introduced that help to fine-tune numerical parameters to meet the performance criteria. With a lack of capable engineers, a shortage of specific knowledge or time to design an analog building block, software for fully automated synthesis of both topology and parameters is becoming crucial. Most research in this field is based on circuit modifications according to evolutionary principles of surviving of the fittest. One of the challenges of the design of appropriate software is a representation of a circuit topology that will allow topology modifications with the smallest possible computational effort. Many existing solutions suffer either from the uncontrolled growth of the size of the circuit (so-called bloat) or from the limitation of the topology structure to a set of predefined blocks. In this paper, we discuss an analog circuit topology representation in a form of a binary upper-triangular matrix that is both bloat safe and offers a large solution space. We describe the basic structure of the matrix, the redundancy phenomena of logical elements, and the translation of the matrix representation to a regular SPICE netlist. We use an evolutionary algorithm to evolve the topology matrix and a classical parameter optimization algorithm to tune the circuit parameters. Based on a high-level circuit definition and a fixed building-block bank, our topology representation technique showed success in a fully automatic synthesis of passive circuits. We demonstrate the ability to automatically discover a passive high-pass filter topology.},
  issue = {1},
  langid = {english},
  keywords = {⛔ No DOI found,analog circuits,Automated synthesis,computer-aided design,evolutionary algorithms},
  file = {/home/krawczuk/Zotero/storage/UIG4WFV4/Rojec et al. - 2018 - Analog Circuit Topology Representation for Automat.pdf}
}

@inproceedings{rueggGeneralizationDirectedGraph2016,
  title = {A {{Generalization}} of the {{Directed Graph Layering Problem}}},
  booktitle = {Graph {{Drawing}} and {{Network Visualization}}},
  author = {Rüegg, Ulf and Ehlers, Thorsten and Spönemann, Miro and family=Hanxleden, given=Reinhard, prefix=von, useprefix=true},
  editor = {Hu, Yifan and Nöllenburg, Martin},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {196--208},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-50106-2_16},
  abstract = {The Directed Layering Problem (DLP) solves a step of the widely used layer-based approach to automatically draw directed acyclic graphs. To cater for cyclic graphs, usually a preprocessing step is used that solves the Feedback Arc Set Problem (FASP) to make the graph acyclic before a layering is determined.},
  isbn = {978-3-319-50106-2},
  langid = {english},
  keywords = {Feedback arc set,Integer programming,Layer assignment,Layer-based layout,Linear arrangement},
  file = {/home/krawczuk/Zotero/storage/CYM75N2Q/Rüegg et al. - 2016 - A Generalization of the Directed Graph Layering Pr.pdf}
}

@article{ruheCliffordGroupEquivariant2023,
  title = {Clifford {{Group Equivariant Neural Networks}}},
  author = {Ruhe, David and Brandstetter, Johannes and Forré, Patrick},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/c6e0125e14ea3d1a3de3c33fd2d49fc4-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/Y7XMRUZK/Ruhe et al. - 2023 - Clifford Group Equivariant Neural Networks.pdf}
}

@inproceedings{safariDWidthMoreNatural2005,
  title = {D-{{Width}}: {{A More Natural Measure}} for {{Directed Tree Width}}},
  shorttitle = {D-{{Width}}},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2005},
  author = {Safari, Mohammad Ali},
  editor = {Jȩdrzejowicz, Joanna and Szepietowski, Andrzej},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {745--756},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11549345_64},
  abstract = {Due to extensive research on tree-width for undirected graphs and due to its many applications in various fields it has been a natural desire for many years to generalize the idea of tree decomposition to directed graphs, but since many parameters in tree-width behave very differently in the world of digraphs, the generalization is still in its preliminary steps.},
  isbn = {978-3-540-31867-5},
  langid = {english}
}

@inproceedings{salvatoriAssociativeMemoriesPredictive2021a,
  title = {Associative {{Memories}} via {{Predictive Coding}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Salvatori, Tommaso and Song, Yuhang and Hong, Yujian and Sha, Lei and Frieder, Simon and Xu, Zhenghua and Bogacz, Rafal and Lukasiewicz, Thomas},
  date = {2021},
  volume = {34},
  pages = {3874--3886},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/1fb36c4ccf88f7e67ead155496f02338-Abstract.html},
  urldate = {2024-02-26},
  abstract = {Associative memories in the brain receive and store patterns of activity registered by the sensory neurons, and are able to retrieve them when necessary. Due to their importance in human intelligence, computational models of associative memories have been developed for several decades now. In this paper, we present a novel neural model for realizing associative memories, which is based on a hierarchical generative network that receives external stimuli via sensory neurons. It is trained using predictive coding, an error-based learning algorithm inspired by information processing in the cortex. To test the model's capabilities, we perform multiple retrieval experiments from both corrupted and incomplete data points. In an extensive comparison, we show that this new model outperforms in retrieval accuracy and robustness popular associative memory models, such as  autoencoders trained via backpropagation, and modern Hopfield networks. In particular, in completing partial data points, our model achieves remarkable results on natural image datasets, such as ImageNet, with a surprisingly high accuracy, even when only a tiny fraction of pixels of the original images is presented. Our model provides a plausible framework to study learning and retrieval of memories in the brain, as it closely mimics the behavior of the hippocampus as a memory index and generative model.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/CFHB2H73/Salvatori et al. - 2021 - Associative Memories via Predictive Coding.pdf}
}

@article{sanchezBenefitsDeepRL2021a,
  title = {On the Benefits of Deep {{RL}} in Accelerated {{MRI}} Sampling},
  author = {Sanchez, Thomas and Krawczuk, Igor and Cevher, Volkan},
  date = {2021},
  url = {https://openreview.net/forum?id=fRb9LBWUo56},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/3S4FD9H6/Sanchez et al. - 2021 - On the benefits of deep RL in accelerated MRI samp.pdf}
}

@article{sanchezClosedLoopDeep2019a,
  title = {Closed Loop Deep {{Bayesian}} Inversion: {{Uncertainty}} Driven Acquisition for Fast {{MRI}}},
  shorttitle = {Closed Loop Deep {{Bayesian}} Inversion},
  author = {Sanchez, Thomas and Krawczuk, Igor and Sun, Zhaodong and Cevher, Volkan},
  date = {2019},
  url = {https://openreview.net/forum?id=BJlPOlBKDB},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/XAVMHTGA/Sanchez et al. - 2019 - Closed loop deep Bayesian inversion Uncertainty d.pdf}
}

@article{sanchezComprehensiveSurveyElectronic2023,
  title = {A {{Comprehensive Survey}} on {{Electronic Design Automation}} and {{Graph Neural Networks}}: {{Theory}} and {{Applications}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Electronic Design Automation}} and {{Graph Neural Networks}}},
  author = {Sánchez, Daniela and Servadei, Lorenzo and Kiprit, Gamze Naz and Wille, Robert and Ecker, Wolfgang},
  date = {2023-03-31},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {28},
  number = {2},
  pages = {1--27},
  issn = {1084-4309, 1557-7309},
  doi = {10.1145/3543853},
  url = {https://dl.acm.org/doi/10.1145/3543853},
  urldate = {2024-02-20},
  abstract = {Driven by Moore’s law, the chip design complexity is steadily increasing. Electronic Design Automation (EDA) has been able to cope with the challenging very large-scale integration process, assuring scalability, reliability, and proper time-to-market. However, EDA approaches are time and resource demanding, and they often do not guarantee optimal solutions. To alleviate these, Machine Learning (ML) has been incorporated into many stages of the design flow, such as in placement and routing. Many solutions employ Euclidean data and ML techniques without considering that many EDA objects are represented naturally as graphs. The trending Graph Neural Networks (GNNs) are an opportunity to solve EDA problems directly using graph structures for circuits, intermediate Register Transfer Levels, and netlists. In this article, we present a comprehensive review of the existing works linking the EDA flow for chip design and GNNs. We map those works to a design pipeline by defining graphs, tasks, and model types. Furthermore, we analyze their practical implications and outcomes. We conclude by summarizing challenges faced when applying GNNs within the EDA design flow.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/QF9XR76W/Sánchez et al. - 2023 - A Comprehensive Survey on Electronic Design Automa.pdf}
}

@inproceedings{sanchezUncertaintyDrivenAdaptiveSampling2020,
  title = {Uncertainty-{{Driven Adaptive Sampling}} via {{GANs}}},
  booktitle = {{{NeurIPS}} 2020 {{Workshop}} on {{Deep Learning}} and {{Inverse Problems}}},
  author = {Sanchez, Thomas and Krawczuk, Igor and Sun, Zhaodong and Cevher, Volkan},
  date = {2020},
  url = {https://openreview.net/forum?id=lWLYCQmtvW},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found}
}

@inproceedings{sandriniEffectMetalBuffer2016c,
  title = {Effect of Metal Buffer Layer and Thermal Annealing on {{HfO}} X-Based {{ReRAMs}}},
  booktitle = {2016 {{IEEE International Conference}} on the {{Science}} of {{Electrical Engineering}} ({{ICSEE}})},
  author = {Sandrini, Jury and Attarimashalkoubeh, Behnoush and Shahrabi, Elmira and Krawczuk, Igor and Leblebici, Yusuf},
  date = {2016},
  pages = {1--5},
  publisher = {{Ieee}},
  doi = {10.1109/ICSEE.2016.7806101},
  url = {https://ieeexplore.ieee.org/abstract/document/7806101/},
  urldate = {2024-02-27}
}

@article{sarkarSpectralCharacterizationHierarchical2013a,
  title = {Spectral {{Characterization}} of {{Hierarchical Network Modularity}} and {{Limits}} of {{Modularity Detection}}},
  author = {Sarkar, Somwrita and Henderson, James A. and Robinson, Peter A.},
  date = {2013-01-28},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {8},
  number = {1},
  eprint = {23382895},
  eprinttype = {pmid},
  pages = {e54383},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0054383},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3557301/},
  urldate = {2024-02-22},
  abstract = {Many real world networks are reported to have hierarchically modular organization. However, there exists no algorithm-independent metric to characterize hierarchical modularity in a complex system. The main results of the paper are a set of methods to address this problem. First, classical results from random matrix theory are used to derive the spectrum of a typical stochastic block model hierarchical modular network form. Second, it is shown that hierarchical modularity can be fingerprinted using the spectrum of its largest eigenvalues and gaps between clusters of closely spaced eigenvalues that are well separated from the bulk distribution of eigenvalues around the origin. Third, some well-known results on fingerprinting non-hierarchical modularity in networks automatically follow as special cases, threreby unifying these previously fragmented results. Finally, using these spectral results, it is found that the limits of detection of modularity can be empirically established by studying the mean values of the largest eigenvalues and the limits of the bulk distribution of eigenvalues for an ensemble of networks. It is shown that even when modularity and hierarchical modularity are present in a weak form in the network, they are impossible to detect, because some of the leading eigenvalues fall within the bulk distribution. This provides a threshold for the detection of modularity. Eigenvalue distributions of some technological, social, and biological networks are studied, and the implications of detecting hierarchical modularity in real world networks are discussed.},
  pmcid = {PMC3557301},
  file = {/home/krawczuk/Zotero/storage/TL5RJIRA/Sarkar et al. - 2013 - Spectral Characterization of Hierarchical Network .pdf}
}

@inproceedings{sarmadRLGANNetReinforcementLearning2019b,
  title = {{{RL-GAN-Net}}: {{A Reinforcement Learning Agent Controlled GAN Network}} for {{Real-Time Point Cloud Shape Completion}}},
  shorttitle = {{{RL-GAN-Net}}},
  author = {Sarmad, Muhammad and Lee, Hyunjoo Jenny and Kim, Young Min},
  date = {2019},
  pages = {5898--5907},
  url = {https://openaccess.thecvf.com/content_CVPR_2019/html/Sarmad_RL-GAN-Net_A_Reinforcement_Learning_Agent_Controlled_GAN_Network_for_Real-Time_CVPR_2019_paper.html},
  urldate = {2024-02-26},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  file = {/home/krawczuk/Zotero/storage/7BGJ749G/Sarmad et al. - 2019 - RL-GAN-Net A Reinforcement Learning Agent Control.pdf}
}

@inproceedings{sawadaNetworkRoutingOptimization2020,
  title = {Network {{Routing Optimization Based}} on {{Machine Learning Using Graph Networks Robust}} against {{Topology Change}}},
  booktitle = {2020 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  author = {Sawada, Kaku and Kotani, Daisuke and Okabe, Yasuo},
  date = {2020-01},
  pages = {608--615},
  issn = {1976-7684},
  doi = {10.1109/ICOIN48656.2020.9016573},
  url = {https://ieeexplore.ieee.org/document/9016573},
  urldate = {2024-02-19},
  abstract = {There is an increasing demand of real-time routing optimazation using Sotware Defined Networking (SDN) for better Quality of Service. Since the problem of finding the optimum routing for any QoS metric is hard to solve for a medium or larger size network, quasi-optimization using metaheuristics, such as Genetic Algorithm (GA) and Simulated Annealing (SA), have been investigated, but it is still impossible to satisfy the requirement of real-time optimization. There have been some attempts to solve this with machine learning. By learning a model beforehand, it is possible to output a near-optimal solution in a short time during network operation. The open problem with this approach is that machine learning models cannot deal with topology change of the network. In this paper, we create a model which is robust for topology change by using Graph Networks. Applying the proposed model to maximum bandwidth utilization, we have gotten the accuracy of about 61.0\% for solution of GA, and the prediction time is 150 times faster than GA.},
  eventtitle = {2020 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  keywords = {Computational modeling,Graph Networks,Machine learning,Machine Learning,Network topology,Neural networks,Quality of service,Routing,Routing Opti-mization,Software Defined Networking,Topology},
  file = {/home/krawczuk/Zotero/storage/UQWYC8WG/Sawada et al. - 2020 - Network Routing Optimization Based on Machine Lear.pdf;/home/krawczuk/Zotero/storage/PCEJTVFD/9016573.html}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  date = {2009-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  url = {https://ieeexplore.ieee.org/document/4700287},
  urldate = {2024-02-24},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning},
  file = {/home/krawczuk/Zotero/storage/CPF3TLQH/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf;/home/krawczuk/Zotero/storage/2Z4EWFRJ/4700287.html}
}

@inproceedings{scheibleAutomationAnalogIC2015,
  title = {Automation of {{Analog IC Layout}}: {{Challenges}} and {{Solutions}}},
  shorttitle = {Automation of {{Analog IC Layout}}},
  booktitle = {Proceedings of the 2015 {{Symposium}} on {{International Symposium}} on {{Physical Design}}},
  author = {Scheible, Juergen and Lienig, Jens},
  date = {2015-03-29},
  series = {{{ISPD}} '15},
  pages = {33--40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2717764.2717781},
  url = {https://dl.acm.org/doi/10.1145/2717764.2717781},
  urldate = {2024-02-19},
  abstract = {Physical analog IC design has not been automated to the same degree as digital IC design. This shortfall is primarily rooted in the analog IC design problem itself, which is considerably more complex even for small problem sizes. Significant progress has been made in analog automation in several R\&D target areas in recent years. Constraint engineering and generator-based module approaches are among the innovations that have emerged. Our paper will first present a brief review of the state of the art of analog layout automation. We will then introduce active and open research areas and present two visions -- a "continuous layout design flow" and a "bottom-up meets top-down design flow" -- which could significantly push analog design automation towards its goal of analog synthesis.},
  isbn = {978-1-4503-3399-3},
  keywords = {analog design,analog layout automation,constraint engineering,design methodology,layout,physical design},
  file = {/home/krawczuk/Zotero/storage/YMDJ3H6B/Scheible and Lienig - 2015 - Automation of Analog IC Layout Challenges and Sol.pdf}
}

@inproceedings{scheibleAutomationAnalogIC2015a,
  title = {Automation of {{Analog IC Layout}}: {{Challenges}} and {{Solutions}}},
  shorttitle = {Automation of {{Analog IC Layout}}},
  booktitle = {Proceedings of the 2015 {{Symposium}} on {{International Symposium}} on {{Physical Design}}},
  author = {Scheible, Juergen and Lienig, Jens},
  date = {2015-03-29},
  series = {{{ISPD}} '15},
  pages = {33--40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2717764.2717781},
  url = {https://dl.acm.org/doi/10.1145/2717764.2717781},
  urldate = {2024-02-19},
  abstract = {Physical analog IC design has not been automated to the same degree as digital IC design. This shortfall is primarily rooted in the analog IC design problem itself, which is considerably more complex even for small problem sizes. Significant progress has been made in analog automation in several R\&D target areas in recent years. Constraint engineering and generator-based module approaches are among the innovations that have emerged. Our paper will first present a brief review of the state of the art of analog layout automation. We will then introduce active and open research areas and present two visions -- a "continuous layout design flow" and a "bottom-up meets top-down design flow" -- which could significantly push analog design automation towards its goal of analog synthesis.},
  isbn = {978-1-4503-3399-3},
  keywords = {analog design,analog layout automation,constraint engineering,design methodology,layout,physical design},
  file = {/home/krawczuk/Zotero/storage/P45CZEHG/Scheible and Lienig - 2015 - Automation of Analog IC Layout Challenges and Sol.pdf}
}

@inproceedings{schmittNeuralCircuitSynthesis2021a,
  title = {Neural {{Circuit Synthesis}} from {{Specification Patterns}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schmitt, Frederik and Hahn, Christopher and Rabe, Markus N and Finkbeiner, Bernd},
  date = {2021},
  volume = {34},
  pages = {15408--15420},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/8230bea7d54bcdf99cdfe85cb07313d5-Abstract.html},
  urldate = {2024-02-24},
  abstract = {We train hierarchical Transformers on the task of synthesizing hardware circuits directly out of high-level logical specifications in linear-time temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic challenge with a long history and an annual competition is organized to track the improvement of algorithms and tooling over time. New approaches using machine learning might open a lot of possibilities in this area, but suffer from the lack of sufficient amounts of training data. In this paper, we consider a method to generate large amounts of additional training data, i.e., pairs of specifications and circuits implementing them. We ensure that this synthetic data is sufficiently close to human-written specifications by mining common patterns from the specifications used in the synthesis competitions. We show that hierarchical Transformers trained on this synthetic data solve a significant portion of problems from the synthesis competitions, and even out-of-distribution examples from a recent case study.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/MNXLFDLE/Schmitt et al. - 2021 - Neural Circuit Synthesis from Specification Patter.pdf}
}

@online{SchnyderGridEmbeddingAlgorithm,
  title = {Schnyder's {{Grid-Embedding Algorithm}}},
  url = {https://ics.uci.edu/~eppstein/gina/schnyder/},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/QKMBLTSL/schnyder.html}
}

@inproceedings{schwarzerPretrainingRepresentationsDataEfficient2021,
  title = {Pretraining {{Representations}} for {{Data-Efficient Reinforcement Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schwarzer, Max and Rajkumar, Nitarshan and Noukhovitch, Michael and Anand, Ankesh and Charlin, Laurent and Hjelm, R Devon and Bachman, Philip and Courville, Aaron C},
  date = {2021},
  volume = {34},
  pages = {12686--12699},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/69eba34671b3ef1ef38ee85caae6b2a1-Abstract.html},
  urldate = {2024-02-26},
  abstract = {Data efficiency is a key challenge for deep reinforcement learning. We address this problem by using unlabeled data to pretrain an encoder which is then finetuned on a small amount of task-specific data. To encourage learning representations which capture diverse aspects of the underlying MDP, we employ a combination of latent dynamics modelling and unsupervised goal-conditioned RL. When limited to 100k steps of interaction on Atari games (equivalent to two hours of human experience), our approach significantly surpasses prior work combining offline representation pretraining with task-specific finetuning, and compares favourably with other pretraining methods that require orders of magnitude more data. Our approach shows particular promise when combined with larger models as well as more diverse, task-aligned observational data -- approaching human-level performance and data-efficiency on Atari in our best setting.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/NMKSAF26/Schwarzer et al. - 2021 - Pretraining Representations for Data-Efficient Rei.pdf}
}

@article{seljakSlicedNormalizingFlow,
  title = {Sliced {{Normalizing Flow}} Optimization and Sampling},
  author = {Seljak, Uroš},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/DZUVSSBG/Seljak - Sliced Normalizing Flow optimization and sampling.pdf}
}

@online{SemiconductorIndustryAssociation2024,
  title = {Semiconductor {{Industry Association}}},
  date = {2024-02-05},
  url = {https://www.semiconductors.org/policies/tax/market-data/?type=post},
  urldate = {2024-02-19},
  langid = {american},
  organization = {{Semiconductor Industry Association}},
  file = {/home/krawczuk/Zotero/storage/X545CNMK/market-data.html}
}

@inproceedings{settaluriAutoCktDeepReinforcement2020f,
  title = {{{AutoCkt}}: {{Deep Reinforcement Learning}} of {{Analog Circuit Designs}}},
  shorttitle = {{{AutoCkt}}},
  booktitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Settaluri, Keertana and Haj-Ali, Ameer and Huang, Qijing and Hakhamaneshi, Kourosh and Nikolic, Borivoje},
  date = {2020-03},
  pages = {490--495},
  publisher = {{IEEE}},
  location = {{Grenoble, France}},
  doi = {10.23919/DATE48585.2020.9116200},
  url = {https://ieeexplore.ieee.org/document/9116200/},
  urldate = {2024-02-20},
  abstract = {Domain specialization under energy constraints in deeply-scaled CMOS has been driving the need for agile development of Systems on a Chip (SoCs). While digital subsystems have design flows that are conducive to rapid iterations from specification to layout, analog and mixed-signal modules face the challenge of a long human-in-the-middle iteration loop that requires expert intuition to verify that post-layout circuit parameters meet the original design specification. Existing automated solutions that optimize circuit parameters for a given target design specification have limitations of being schematic-only, inaccurate, sample-inefficient or not generalizable. This work presents AutoCkt, a machine learning optimization framework trained using deep reinforcement learning that not only finds post-layout circuit parameters for a given target specification, but also gains knowledge about the entire design space through a sparse subsampling technique. Our results show that for multiple circuit topologies, AutoCkt is able to converge and meet all target specifications on at least 96.3\% of tested design goals in schematic simulation, on average 40× faster than a traditional genetic algorithm. Using the Berkeley Analog Generator, AutoCkt is able to design 40 LVS passed operational amplifiers in 68 hours, 9.6× faster than the state-of-the-art when considering layout parasitics.},
  eventtitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  isbn = {978-3-9819263-4-7},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/L2XLXA5J/Settaluri et al. - 2020 - AutoCkt Deep Reinforcement Learning of Analog Cir.pdf}
}

@article{settaluriAutomatedDesignAnalog2022a,
  title = {Automated {{Design}} of {{Analog Circuits Using Reinforcement Learning}}},
  author = {Settaluri, Keertana and Liu, Zhaokai and Khurana, Rishubh and Mirhaj, Arash and Jain, Rajeev and Nikolic, Borivoje},
  date = {2022-09},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  volume = {41},
  number = {9},
  pages = {2794--2807},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2021.3120547},
  url = {https://ieeexplore.ieee.org/document/9576505/},
  urldate = {2024-02-20},
  abstract = {Analog and mixed-signal (AMS) blocks are often a crucial and time-consuming part of System-on-Chip (SoC) design, primarily due to a manual circuit and layout iterations. Existing automated solutions for selecting circuit parameters for a given target specification are often not efficient, accurate, or reliable. In order for an automated sizing tool to be practical, we posit that it must: 1) return valid results for a large range of target specifications; 2) understand where and why it is unable to meet certain specifications; 3) consider true layout parasitic simulations for complete end-to-end design; and 4) be automated, allowing most of the design effort to fall on the tool. In this article, we address these critical points by establishing an automated reinforcement learning framework, AutoCkt, by 1) successfully deploying it on a complex two-stage transimpedance amplifier and two-stage folded cascode with biasing in the 16-nm FinFet technology; 2) implementing a new combined distribution deployment algorithm to improve efficiency; 3) analyzing in-depth the efficacy of the trained agent; and 4) demonstrating the functionality of this tool when considering a topology that is highly sensitive to layout parasitics. Our algorithm not only successfully reaches unique, valid, and practical performances, but also does so in state-of-the-art run time, up to 38X more efficient than prior work. In addition, our tool averages just four parasitic simulations obtained by using the Berkeley Analog Generator, to achieve a target specification post-layout for the folded cascode. AutoCkt successfully generates LVS-passed designs with validation in process corner variation results.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/6Z5FZJF9/Settaluri et al. - 2022 - Automated Design of Analog Circuits Using Reinforc.pdf}
}

@book{shaulyDesignRulesSemiconductor2022,
  title = {Design {{Rules}} in a {{Semiconductor Foundry}}},
  author = {Shauly, Eitan N.},
  date = {2022-11-30},
  eprint = {RdaTEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{CRC Press}},
  abstract = {Nowadays over 50\% of integrated circuits are fabricated at wafer foundries. This book presents a foundry-integrated perspective of the field and is a comprehensive and up-to-date manual designed to serve process, device, layout, and design engineers. It comprises chapters carefully selected to cover topics relevant for them to deal with their work. The book provides an insight into the different types of design rules (DRs) and considerations for setting new DRs. It discusses isolation, gate patterning, S/D contacts, metal lines, MOL, air gaps, and so on. It explains in detail the layout rules needed to support advanced planarization processes, different types of dummies, and related utilities as well as presents a large set of guidelines and layout-aware modeling for RF CMOS and analog modules. It also discusses the layout DRs for different mobility enhancement techniques and their related modeling, listing many of the dedicated rules for static random-access memory (SRAM), embedded polyfuse (ePF), and LogicNVM. The book also provides the setting and calibration of the process parameters set and describes the 28\textasciitilde 20 nm planar MOSFET process flow for low-power and high-performance mobile applications in a step-by-step manner. It includes FEOL and BEOL physical and environmental tests for qualifications together with automotive qualification and design for automotive (DfA). Written for the professionals, the book belongs to the bookshelf of microelectronic discipline experts.},
  isbn = {978-1-00-063135-7},
  langid = {english},
  pagetotal = {831},
  keywords = {Science / Life Sciences / General,Science / Physics / General,Science / Physics / Optics \& Light,Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Microelectronics,Technology \& Engineering / Electronics / Optoelectronics,Technology \& Engineering / Environmental / General,Technology \& Engineering / Materials Science / General,Technology \& Engineering / Microwaves}
}

@article{shiAutomaticGenerationMacromodels2023,
  title = {Automatic Generation of Macromodels and Design Equations for Application to {{Op Amp}} Design},
  author = {Shi, Guoyong},
  date = {2023-10},
  journaltitle = {International Journal of Circuit Theory and Applications},
  shortjournal = {Circuit Theory \& Apps},
  volume = {51},
  number = {10},
  pages = {4521--4549},
  issn = {0098-9886, 1097-007X},
  doi = {10.1002/cta.3673},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cta.3673},
  urldate = {2024-02-19},
  abstract = {Summary             Operational amplifier (Op Amp) is a special class of analog integrated circuits. Multiple‐stage Op Amp design is of particular interest recently. However, analysis of this class of circuits has to be done manually and requires advanced skills. Automatic analysis of this class of circuits is studied in this paper, which takes a novel approach by circuit recognition and symbolic generation. Circuit recognition is deterministic, implemented by deterministic circuit partitioning and functional block extraction methods. Symbolic analysis of extracted circuit blocks can subsequently be performed with greatly reduced complexity while generated models and equations bear more useful information for circuit design. Symbolic results so generated can be applied in design space exploration, including tasks like finding the limitation in circuit topology, conducting initial sizing, and determining performance tradeoffs. This paper proposes the key algorithms for realizing the recognition of a few well‐known functional blocks frequently used in CMOS Op Amp circuits and further explores the possibility of using the recognized circuit cells in combination with the gm/ID method for circuit sizing. Preliminary tests show that this approach is a potential candidate for locating the performance boundaries of multiple‐stage Op Amps.},
  langid = {english}
}

@article{shiGraphPairDecisionDiagram2013,
  title = {Graph-{{Pair Decision Diagram Construction}} for {{Topological Symbolic Circuit Analysis}}},
  author = {Shi, Guoyong},
  date = {2013-02},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {32},
  number = {2},
  pages = {275--288},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2012.2217963},
  url = {https://ieeexplore.ieee.org/abstract/document/6416104},
  urldate = {2024-02-20},
  abstract = {Symbolic circuit analysis is concerned with analytical construction of circuit response in the frequency (or time) domain, for which an efficient data structure is required. Recent research has justified that the binary decision diagram (BDD) is a superior data structure with the following distinguishing feature: a large number of product terms can be compactly represented by a BDD, on which numerical computations and analytical deductions can be performed directly. Using BDD for symbolic circuit analysis requires an efficient method for construction. In this paper, a graph-based construction method, called graph-pair decision diagram (GPDD), is developed. Given a small-signal circuit, a pair of graphs representing the circuit is created, from which a GPDD is constructed by successively reducing the graph pair. The GPDD algorithm, which generates cancellation-free symbolic terms, differs from the existing determinant decision diagram (DDD) algorithm. Detailed theory and implementable algorithms for the GPDD construction are developed, and a runtime performance comparison to DDD is made. It is demonstrated that the runtime performance using GPDD is comparable to that of DDD in terms of time and memory complexity for exact symbolic analysis, although the GPDD algorithm has to generate a much larger number of symbolic product terms.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Algorithm design and analysis,Analog design automation,binary decision diagram (BDD),Boolean functions,Circuit analysis,Data structures,determinant decision diagram (DDD),graph-pair decision diagram (GPDD),implicit enumeration,Integrated circuit modeling,SPICE,symbolic analysis,Voltage control},
  file = {/home/krawczuk/Zotero/storage/UNI5IUY9/6416104.html}
}

@article{shinLAYGO2CustomLayout2023,
  title = {{{LAYGO2}}: {{A Custom Layout Generation Engine Based}} on {{Dynamic Templates}} and {{Grids}} for {{Advanced CMOS Technologies}}},
  shorttitle = {{{LAYGO2}}},
  author = {Shin, Taeho and Lee, Dongjun and Kim, Dongwhee and Sung, Gaeryun and Shin, Wookjin and Jo, Yunseong and Park, Hyungjoo and Han, Jaeduk},
  date = {2023-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {12},
  pages = {4402--4412},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2023.3294462},
  url = {https://ieeexplore.ieee.org/document/10182288},
  urldate = {2024-02-19},
  abstract = {This article presents an automatic layout generation framework in advanced CMOS technologies. The framework extends the template-and-grid-based layout generation methodology to produce optimal layouts more efficiently. Layout templates and grids are dynamically created and adjusted during the generation phase to provide more reusability and flexibility. Virtual instances are used to encapsulate the dynamically generated layout structures. Internal node probes embedded in the dynamic templates capture parasitic effects precisely. The framework also implements various post-processing functions to handle process-specific tasks while maintaining the overall process portability of procedural layout generators. The post-processing functions include cut/dummy pattern generation and multiple-patterning adjustment. The generator description coverage is enhanced with circular grid indexing/slicing and conditional conversion operators. The layout generation framework is applied to generate various DRC/LVS clean layouts automatically in advanced CMOS technologies, achieving 0.66–249.35 transistors-per-line (the ratio of the generated transistor count to the source lines of code) generation efficiencies.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,CMOS technology,Codes,design rules,full-custom circuits,Generators,Layout,layout generation,Organizations,Routing,Task analysis},
  file = {/home/krawczuk/Zotero/storage/8DCBUVXC/10182288.html}
}

@inproceedings{shirzadTDGENGraphGeneration2022a,
  title = {{{TD-GEN}}: {{Graph Generation Using Tree Decomposition}}},
  shorttitle = {{{TD-GEN}}},
  booktitle = {Proceedings of {{The}} 25th {{International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Shirzad, Hamed and Hajimirsadeghi, Hossein and Abdi, Amir H. and Mori, Greg},
  date = {2022-05-03},
  pages = {5518--5537},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v151/shirzad22a.html},
  urldate = {2024-02-25},
  abstract = {We propose TD-GEN, a graph generation framework based on tree decomposition, and introduce a reduced upper bound on the maximum number of decisions needed for graph generation. The framework includes a permutation invariant tree generation model which forms the backbone of graph generation. Tree nodes are supernodes, each representing a cluster of nodes in the graph. Graph nodes and edges are incrementally generated inside the clusters by traversing the tree supernodes, respecting the structure of the tree decomposition, and following node sharing decisions between the clusters. Further, we discuss the shortcomings of the standard evaluation criteria based on statistical properties of the generated graphs. We propose to compare the generalizability of models based on expected likelihood. Empirical results on a variety of standard graph generation datasets demonstrate the superior performance of our method.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/EKR62RSX/Shirzad et al. - 2022 - TD-GEN Graph Generation Using Tree Decomposition.pdf}
}

@online{signal-aardvark-4179MetaGPTGrosslyMisreported2024,
  type = {Reddit Post},
  title = {[{{D}}] {{MetaGPT}} Grossly Misreported Baseline Numbers and Got an {{ICLR Oral}}!},
  author = {Signal-Aardvark-4179},
  date = {2024-02-22T17:06:50},
  url = {www.reddit.com/r/MachineLearning/comments/1axbm0f/d_metagpt_grossly_misreported_baseline_numbers/},
  urldate = {2024-02-22},
  organization = {{r/MachineLearning}},
  file = {/home/krawczuk/Zotero/storage/9JCG4UYV/d_metagpt_grossly_misreported_baseline_numbers.html}
}

@online{SignalDivisionAwareAnalogCircuit,
  title = {Signal-{{Division-Aware Analog Circuit Topology Synthesis Aided}} by {{Transfer Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10045726?casa_token=gzH1HcQ4RzQAAAAA:aj34Uh6FCyG-oxCWsuMwBYYzkcx1LbJuA1UVG42YWfTN0Mw-7ck2ZV_whOfZBV4y2RtXUz-h40nC},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/TQ5ITJF5/10045726.html}
}

@inproceedings{singhalLearningTransformGeneralizable2023,
  title = {Learning to {{Transform}} for {{Generalizable Instance-wise Invariance}}},
  author = {Singhal, Utkarsh and Esteves, Carlos and Makadia, Ameesh and Yu, Stella X.},
  date = {2023},
  pages = {6211--6221},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.html},
  urldate = {2024-02-20},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/A5A6R52I/Singhal et al. - 2023 - Learning to Transform for Generalizable Instance-w.pdf}
}

@online{singhLEGOBenchScientificLeaderboard2024,
  title = {{{LEGOBench}}: {{Scientific Leaderboard Generation Benchmark}}},
  shorttitle = {{{LEGOBench}}},
  author = {Singh, Shruti and Alam, Shoaib and Malwat, Husain and Singh, Mayank},
  date = {2024-02-21},
  eprint = {2401.06233},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2401.06233},
  url = {http://arxiv.org/abs/2401.06233},
  urldate = {2024-02-25},
  abstract = {The ever-increasing volume of paper submissions makes it difficult to stay informed about the latest state-of-the-art research. To address this challenge, we introduce LEGOBench, a benchmark for evaluating systems that generate scientific leaderboards. LEGOBench is curated from 22 years of preprint submission data on arXiv and more than 11k machine learning leaderboards on the PapersWithCode portal. We present four graph-based and two language model-based leaderboard generation task configurations. We evaluate popular encoder-only scientific language models as well as decoder-only large language models across these task configurations. State-of-the-art models showcase significant performance gaps in automatic leaderboard generation on LEGOBench. The code is available on GitHub ( https://github.com/lingo-iitgn/LEGOBench ) and the dataset is hosted on OSF ( https://osf.io/9v2py/?view\_only=6f91b0b510df498ba01595f8f278f94c ).},
  pubstate = {preprint},
  version = {2},
  keywords = {Computer Science - Computation and Language},
  file = {/home/krawczuk/Zotero/storage/XINCPUV7/Singh et al. - 2024 - LEGOBench Scientific Leaderboard Generation Bench.pdf;/home/krawczuk/Zotero/storage/CU8XATWQ/2401.html}
}

@inproceedings{sohl-dicksteinDeepUnsupervisedLearning2015b,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-06-01},
  pages = {2256--2265},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  urldate = {2024-02-22},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CPI9QRW6/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@online{songDenoisingDiffusionImplicit2022d,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  date = {2022-10-05},
  eprint = {2010.02502},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.02502},
  url = {http://arxiv.org/abs/2010.02502},
  urldate = {2024-02-25},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/X69BMS36/Song et al. - 2022 - Denoising Diffusion Implicit Models.pdf;/home/krawczuk/Zotero/storage/LNZJ9BMA/2010.html}
}

@inproceedings{songGenerativeModelingEstimating2019b,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
  urldate = {2024-02-25},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples  comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/C4HJKZEV/Song and Ermon - 2019 - Generative Modeling by Estimating Gradients of the.pdf}
}

@online{songMaximumLikelihoodTraining2021a,
  title = {Maximum {{Likelihood Training}} of {{Score-Based Diffusion Models}}},
  author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  date = {2021-10-20},
  eprint = {2101.09258},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2101.09258},
  url = {http://arxiv.org/abs/2101.09258},
  urldate = {2024-02-22},
  abstract = {Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32x32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/234MB2S3/Song et al. - 2021 - Maximum Likelihood Training of Score-Based Diffusi.pdf;/home/krawczuk/Zotero/storage/LJNBHJXL/2101.html}
}

@article{sorkhabiAutomatedTopologySynthesis2017a,
  title = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits: {{A}} Survey},
  shorttitle = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits},
  author = {Sorkhabi, Samin Ebrahim and Zhang, Lihong},
  date = {2017-01-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {56},
  pages = {128--138},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2016.10.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926016300931},
  urldate = {2024-02-19},
  abstract = {The puzzle of automatically synthesizing analog and radio frequency (RF) circuit topology has not yet been offered with an industrially-acceptable solution although endeavors still continue to seek a conquest in this area. This survey provides a comprehensive study of the techniques utilized for this purpose. The existing methods are analyzed from four different viewpoints, namely, structural view, conceptual view, implementation view, and application view. Different schemes are perused with their advantages and drawbacks discussed in the context of balanced performance between configuration-space coverage and search efficiency. Some prospective trends are pointed out to shed light on the upcoming research activities.},
  keywords = {Analog circuit topology synthesis,Analog electronic design automation,Classification},
  file = {/home/krawczuk/Zotero/storage/GWPEG9NF/S0167926016300931.html}
}

@article{sorkhabiAutomatedTopologySynthesis2017b,
  title = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits: {{A}} Survey},
  shorttitle = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits},
  author = {Sorkhabi, Samin Ebrahim and Zhang, Lihong},
  date = {2017-01-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {56},
  pages = {128--138},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2016.10.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926016300931},
  urldate = {2024-02-20},
  abstract = {The puzzle of automatically synthesizing analog and radio frequency (RF) circuit topology has not yet been offered with an industrially-acceptable solution although endeavors still continue to seek a conquest in this area. This survey provides a comprehensive study of the techniques utilized for this purpose. The existing methods are analyzed from four different viewpoints, namely, structural view, conceptual view, implementation view, and application view. Different schemes are perused with their advantages and drawbacks discussed in the context of balanced performance between configuration-space coverage and search efficiency. Some prospective trends are pointed out to shed light on the upcoming research activities.},
  keywords = {Analog circuit topology synthesis,Analog electronic design automation,Classification}
}

@inreference{SpatialNetwork2023,
  title = {Spatial Network},
  booktitle = {Wikipedia},
  date = {2023-10-12T03:52:20Z},
  url = {https://en.wikipedia.org/w/index.php?title=Spatial_network&oldid=1179742180},
  urldate = {2024-02-22},
  abstract = {A spatial network (sometimes also geometric graph) is a graph in which the vertices or edges are spatial elements associated with geometric objects, i.e., the nodes are located in a space equipped with a certain metric. The simplest mathematical realization of spatial network is a lattice or a random geometric graph (see figure in the right), where nodes are distributed uniformly at random over a two-dimensional plane; a pair of nodes are connected if the Euclidean distance is smaller than a given neighborhood radius. Transportation and mobility networks, Internet, mobile phone networks, power grids, social and contact networks and biological neural networks are all examples where the underlying space is relevant and where the graph's topology alone does not contain all the information. Characterizing and understanding the structure, resilience and the evolution of spatial networks is crucial for many different fields ranging from urbanism to epidemiology.},
  langid = {english},
  annotation = {Page Version ID: 1179742180},
  file = {/home/krawczuk/Zotero/storage/5XRMLJ55/Spatial_network.html}
}

@online{SpecificationTopologyAutomatica,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=ZzmTrAML-mAAAAAA:gl7tX0UqdKrlNwLRzn1Uj9717pljBhgCrxEE9JcvSntZSTQRWnsLqWbx1UQC6PP_V4OfCQoJvMTq},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/CLZT4656/9643552.html}
}

@online{SpecificationTopologyAutomaticb,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=OfMFhpoMuAQAAAAA:AfACosBN6oY2Vpsbk8_cjU86UjZRF_Ea-T_xy-llZuNF7C5ZRvRRToOmxHA4g0vG5tvB5wt09S5x},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/9NG9IXC3/9643552.html}
}

@online{SpecificationTopologyAutomaticc,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=OfMFhpoMuAQAAAAA:AfACosBN6oY2Vpsbk8_cjU86UjZRF_Ea-T_xy-llZuNF7C5ZRvRRToOmxHA4g0vG5tvB5wt09S5x},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/89F7WMIR/9643552.html}
}

@online{SpectralCharacterizationHierarchical,
  title = {Spectral {{Characterization}} of {{Hierarchical Network Modularity}} and {{Limits}} of {{Modularity Detection}} - {{PMC}}},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3557301/},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/BGNZ54PD/PMC3557301.html}
}

@article{staufferComputationalPolicyProcess2021,
  title = {Computational {{Policy Process Studies}}},
  author = {Stauffer, Maxime and Seifert, Konrad and Mengesha, Isaak and Krawczuk, Igor and Fischer, Jens and Serugendo, Giovanna Di Marzo},
  date = {2021},
  url = {https://www.researchgate.net/profile/Maxime-Stauffer-2/publication/351114474_Computational_Policy_Process_Studies/links/60883ae2881fa114b4319140/Computational-Policy-Process-Studies.pdf},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/EDV7VY7K/Stauffer et al. - 2021 - Computational Policy Process Studies.pdf}
}

@article{staufferComputationalTurnPolicy2022,
  title = {A {{Computational Turn}} in {{Policy Process Studies}}: {{Coevolving Network Dynamics}} of {{Policy Change}}},
  shorttitle = {A {{Computational Turn}} in {{Policy Process Studies}}},
  author = {Stauffer, Maxime and Mengesha, Isaak and Seifert, Konrad and Krawczuk, Igor and Fischer, Jens and Di Marzo Serugendo, Giovanna},
  date = {2022},
  journaltitle = {Complexity},
  volume = {2022},
  pages = {1--17},
  publisher = {{Hindawi Limited}},
  doi = {10.1155/2022/8210732},
  url = {https://www.hindawi.com/journals/complexity/2022/8210732/},
  urldate = {2024-02-27},
  file = {/home/krawczuk/Zotero/storage/V9E2GXCH/8210732.html}
}

@article{staufferSupplementaryMaterialComputational2022,
  title = {Supplementary Material {{A Computational Turn}} in {{Policy Process Studies}}: {{Co-evolving Network Dynamics}} of {{Policy Change}}},
  shorttitle = {Supplementary Material {{A Computational Turn}} in {{Policy Process Studies}}},
  author = {Stauffer, Maxime and Mengesha, Isaak and Seifert, Konrad and Krawczuk, Igor and Fischer, Jens and Serugendo, Giovanna Di Marzo},
  date = {2022},
  url = {https://downloads.hindawi.com/journals/complexity/2022/8210732.f1.pdf},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/DPAXT6CH/Stauffer et al. - 2022 - Supplementary material A Computational Turn in Pol.pdf}
}

@software{stefanschippersStefanSchippersXschem2024,
  title = {{{StefanSchippers}}/Xschem},
  author = {StefanSchippers},
  date = {2024-02-23T16:30:17Z},
  origdate = {2020-08-08T13:30:48Z},
  url = {https://github.com/StefanSchippers/xschem},
  urldate = {2024-02-26},
  abstract = {A schematic editor for VLSI/Asic/Analog custom designs, netlist backends for VHDL, Spice and Verilog. The tool is focused on hierarchy and parametric designs, to maximize circuit reuse.}
}

@article{sudermann-merxCrossingMinimalEdgeConstrained2021,
  title = {Crossing {{Minimal Edge-Constrained Layout Planning}} Using {{Benders Decomposition}}},
  author = {Sudermann-Merx, Nathan and Rebennack, Steffen and Timpe, Christian},
  date = {2021},
  journaltitle = {Production and Operations Management},
  volume = {30},
  number = {10},
  pages = {3429--3447},
  issn = {1937-5956},
  doi = {10.1111/poms.13441},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/poms.13441},
  urldate = {2024-02-22},
  abstract = {We present a new crossing number problem, which we refer to as the edge-constrained weighted two-layer crossing number problem (ECW2CN). The ECW2CN arises in layout planning of hose coupling stations at BASF, where the challenge is to find a crossing minimal assignment of tube-connected units to given positions on two opposing layers. This allows the use of robots in an effort to reduce the probability of operational disruptions and to increase human safety. Physical limitations imply maximal length and maximal curvature conditions on the tubes as well as spatial constraints imposed by the surrounding walls. This is the major difference of ECW2CN to all known variants of the crossing number problem. Such as many variants of the crossing number problem, ECW2CN is -hard. Because the optimization model grows fast with respect to the input data, we face out-of-memory errors for the monolithic model. Therefore, we develop two solution methods. In the first method, we tailor Benders decomposition toward the problem. The Benders subproblems are solved analytically and the Benders master problem is strengthened by additional cuts. Furthermore, we combine this Benders decomposition with ideas borrowed from fix-and-relax heuristics to design the Dynamic Fix-and-Relax Pump (DFRP). Based on an initial solution, DFRP improves successively feasible points by solving dynamically sampled smaller problems with Benders decomposition. Because the optimization model is a surrogate model for its time-dependent formulation, we evaluate the obtained solutions for different choices of the objective function via a simulation model. All algorithms are implemented efficiently using advanced features of the GuRoBi-Python API, such as callback functions and lazy constraints. We present a case study for BASF using real data and make the real-world data openly available.},
  langid = {english},
  keywords = {Benders decomposition with direct cut calculation,Benders-based branch-and-cut,crossing number,dynamic fix-and-relax pump,mixed integer linear programming,optimization,simulation},
  file = {/home/krawczuk/Zotero/storage/WXI4ZC8A/Sudermann-Merx et al. - 2021 - Crossing Minimal Edge-Constrained Layout Planning .pdf;/home/krawczuk/Zotero/storage/WDKX5D7Y/poms.html}
}

@inproceedings{suhFACTGenFrameworkAutomated2022a,
  title = {{{FACTGen}}: {{Framework}} for {{Automated Circuit Topology Generator}}},
  shorttitle = {{{FACTGen}}},
  booktitle = {2022 19th {{International SoC Design Conference}} ({{ISOCC}})},
  author = {Suh, Jangwon and Jung, Wanyeong},
  date = {2022-10},
  pages = {27--28},
  issn = {2163-9612},
  doi = {10.1109/ISOCC56007.2022.10031578},
  url = {https://ieeexplore.ieee.org/abstract/document/10031578},
  urldate = {2024-02-24},
  abstract = {Besides the popularity of computer-aided design methods, the circuit topology design itself still relies on human insights. We propose a framework for automated circuit topology generator (FACTGen). Our framework transforms the circuit topology generation problem into a continuous black-box optimization problem. FACTGen has succeeded to generate several 2T and 4T CMOS digital gate topologies.},
  eventtitle = {2022 19th {{International SoC Design Conference}} ({{ISOCC}})},
  keywords = {black-box optimization,circuit topology,Circuit topology,Closed box,Design automation,Generators,Logic gates,Search problems,topology generation,Transforms},
  file = {/home/krawczuk/Zotero/storage/YZL5WG3J/Suh and Jung - 2022 - FACTGen Framework for Automated Circuit Topology .pdf;/home/krawczuk/Zotero/storage/2ATXPR3G/10031578.html}
}

@inproceedings{sullivanParallelAlgorithmsGraph2013,
  title = {Parallel {{Algorithms}} for {{Graph Optimization Using Tree Decompositions}}},
  booktitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  author = {Sullivan, Blair D. and Weerapurage, Dinesh and Groër, Chris},
  date = {2013-05},
  pages = {1838--1847},
  doi = {10.1109/IPDPSW.2013.242},
  url = {https://ieeexplore.ieee.org/document/6651084},
  urldate = {2024-02-20},
  abstract = {Although many NP-hard graph optimization problems can be solved in polynomial time on graphs of bounded tree-width, the adoption of these techniques into mainstream scientific computation has been limited due to the high memory requirements of the dynamic programming tables and excessive runtimes of sequential implementations. This work addresses both challenges by proposing a set of new parallel algorithms for all steps of a tree decomposition-based approach to solve the maximum weighted independent set problem. A hybrid OpenMP/MPI implementation includes a highly scalable parallel dynamic programming algorithm leveraging the MADNESS task based runtime, and computational results demonstrate scaling. This work enables a significant expansion of the scale of graphs on which exact solutions to maximum weighted independent set can be obtained, and forms a framework for solving additional graph optimization problems with similar techniques.},
  eventtitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  keywords = {dynamic programming,Dynamic programming,graph algorithms,Heuristic algorithms,independent set,Memory management,Optimization,Parallel algorithms,parallel programming,Runtime,tree decomposition,Vegetation},
  file = {/home/krawczuk/Zotero/storage/2C5599ET/Sullivan et al. - 2013 - Parallel Algorithms for Graph Optimization Using T.pdf}
}

@inproceedings{sullivanParallelAlgorithmsGraph2013a,
  title = {Parallel {{Algorithms}} for {{Graph Optimization Using Tree Decompositions}}},
  booktitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  author = {Sullivan, Blair D. and Weerapurage, Dinesh and Groër, Chris},
  date = {2013-05},
  pages = {1838--1847},
  doi = {10.1109/IPDPSW.2013.242},
  url = {https://ieeexplore.ieee.org/document/6651084},
  urldate = {2024-02-20},
  abstract = {Although many NP-hard graph optimization problems can be solved in polynomial time on graphs of bounded tree-width, the adoption of these techniques into mainstream scientific computation has been limited due to the high memory requirements of the dynamic programming tables and excessive runtimes of sequential implementations. This work addresses both challenges by proposing a set of new parallel algorithms for all steps of a tree decomposition-based approach to solve the maximum weighted independent set problem. A hybrid OpenMP/MPI implementation includes a highly scalable parallel dynamic programming algorithm leveraging the MADNESS task based runtime, and computational results demonstrate scaling. This work enables a significant expansion of the scale of graphs on which exact solutions to maximum weighted independent set can be obtained, and forms a framework for solving additional graph optimization problems with similar techniques.},
  eventtitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  keywords = {dynamic programming,Dynamic programming,graph algorithms,Heuristic algorithms,independent set,Memory management,Optimization,Parallel algorithms,parallel programming,Runtime,tree decomposition,Vegetation},
  file = {/home/krawczuk/Zotero/storage/24CZIRZW/Sullivan et al. - 2013 - Parallel Algorithms for Graph Optimization Using T.pdf}
}

@article{sunDIFUSCOGraphbasedDiffusion2024,
  title = {{{DIFUSCO}}: {{Graph-based Diffusion Solvers}} for {{Combinatorial Optimization}}},
  shorttitle = {{{DIFUSCO}}},
  author = {Sun, Zhiqing and Yang, Yiming},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0ba520d93c3df592c83a611961314c98-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/BKW6HNRM/Sun and Yang - 2024 - DIFUSCO Graph-based Diffusion Solvers for Combina.pdf}
}

@article{sunPPGNNPretrainingPositionaware2023,
  title = {{{PP-GNN}}: {{Pretraining Position-aware Graph Neural Networks}} with the {{NP-hard}} Metric Dimension Problem},
  shorttitle = {{{PP-GNN}}},
  author = {Sun, Michael},
  date = {2023-12-07},
  journaltitle = {Neurocomputing},
  shortjournal = {Neurocomputing},
  volume = {561},
  pages = {126848},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.126848},
  url = {https://www.sciencedirect.com/science/article/pii/S0925231223009712},
  urldate = {2024-02-26},
  abstract = {On a graph G=(V,E), we call S⊂V resolving if ∀u,v∈V with u≠v, ∃w∈V such that d(u,w)≠d(v,w). The smallest possible cardinality of S is called the metric dimension, computing which is known to be NP-hard. Solving the metric dimension problem (MDP) and the associated minimal resolving set has many important applications across science and engineering. In this paper, we introduce MaskGNN, a method using a graph neural network (GNN) model to learn the minimal resolving set in a self-supervised manner by optimizing a novel surrogate objective. We provide a construction showing the global minimum of this objective coincides with the solution to the MDP. MaskGNN attains 51\%–72\% improvement over the best baseline and up to 98\% the reward of integer programming in 0.72\% of the running time. On this foundation, we introduce Pretraining Position-aware GNNs (PP-GNN) and evaluate on popular benchmark position-based tasks on graphs. PP-GNN’s strong results challenge the currently popular paradigm – heuristic-driven anchor-selection – with a new learning-based paradigm — simultaneously learning the metric basis of the graph and pretraining position-aware representations for transferring to downstream position-based tasks.},
  keywords = {Graph,Graph neural network,Graph representation learning,Metric dimension,NP-hard},
  file = {/home/krawczuk/Zotero/storage/BVZU5SRM/S0925231223009712.html}
}

@online{SynthesisSystemAnalog,
  title = {A Synthesis System for Analog Circuits Based on Evolutionary Search and Topological Reuse | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/1413261?casa_token=MvOhJbKQnnoAAAAA:eAu-H_diRTVWFMj8uTmLx55rJKDIA2AX9FdBgDPxJTGvE-D1FBdVfr1gL5Q5gUphvZ0V1io8GBqk},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/R8WLTJWA/1413261.html}
}

@article{tahmasebiExactSampleComplexity2023a,
  title = {The {{Exact Sample Complexity Gain}} from {{Invariances}} for {{Kernel Regression}}},
  author = {Tahmasebi, Behrooz and Jegelka, Stefanie},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/adf5a38a2e2e7606fbfc3eff72998afa-Abstract-Conference.html},
  urldate = {2024-02-26},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/38P7W3F6/Tahmasebi and Jegelka - 2023 - The Exact Sample Complexity Gain from Invariances .pdf}
}

@incollection{thomasChurchProblemTour2008,
  title = {Church’s {{Problem}} and a {{Tour}} through {{Automata Theory}}},
  booktitle = {Pillars of {{Computer Science}}: {{Essays Dedicated}} to {{Boris}} ({{Boaz}}) {{Trakhtenbrot}} on the {{Occasion}} of {{His}} 85th {{Birthday}}},
  author = {Thomas, Wolfgang},
  editor = {Avron, Arnon and Dershowitz, Nachum and Rabinovich, Alexander},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {635--655},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78127-1_35},
  url = {https://doi.org/10.1007/978-3-540-78127-1_35},
  urldate = {2024-02-20},
  abstract = {Church’s Problem, stated fifty years ago, asks for a finite-state machine that realizes the transformation of an infinite sequence α into an infinite sequence β such that a requirement on (α, β), expressed in monadic second-order logic, is satisfied. We explain how three fundamental techniques of automata theory play together in a solution of Church’s Problem: Determinization (starting from the subset construction), appearance records (for stratifying acceptance conditions), and reachability analysis (for the solution of games).},
  isbn = {978-3-540-78127-1},
  langid = {english},
  keywords = {Automaton Theory,Game Graph,Tree Automaton,Weak Parity,Winning Strategy}
}

@thesis{thompsonTrainingEvaluatingGraph2023,
  title = {Training and {{Evaluating Graph Generative Models}}},
  author = {Thompson, Rylee},
  date = {2023-05},
  institution = {{University of Guelph}},
  url = {https://hdl.handle.net/10214/27576},
  urldate = {2024-02-25},
  abstract = {In this thesis-by-articles we make several contributions related to graph generative models (GGMs) and their applications. In our first article, we investigate the use of GGMs for the sequential design of 3D structures. We propose LEGO as a toy problem that is complex enough to approximate real-world design and develop a method for representing simple LEGO structures as a graph to train a GGM. We extend a popular GGM approach, Deep Generative Models of Graphs (DGMG), to operate on these LEGO structures and propose several evaluation metrics originally developed for generative models of images that utilize a relevant pretrained network. In our second article, we dive deeper into the proposed metrics to identify a single metric that can be used to evaluate GGMs regardless of domain. We propose replacing the pretrained network used in evaluation with a randomly initialized one to reduce overhead requirements, and design several experiments to objectively score each metric on several criteria. Through this process, we show that pretraining the network is not required: a network with random parameters is sufficient for evaluation. We further identify several strong metrics that can be used to easily evaluate GGMs across domains.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/C6VAZ5B8/Thompson - 2023 - Training and Evaluating Graph Generative Models.pdf}
}

@inreference{TopologicalGraph2024,
  title = {Topological Graph},
  booktitle = {Wikipedia},
  date = {2024-01-11T07:43:47Z},
  url = {https://en.wikipedia.org/w/index.php?title=Topological_graph&oldid=1194890238},
  urldate = {2024-02-22},
  abstract = {In mathematics, a topological graph is a representation of a graph in the plane, where the vertices of the graph are represented by distinct points and the edges by Jordan arcs (connected pieces of Jordan curves) joining the corresponding pairs of points. The points representing the vertices of a graph and the arcs representing its edges are called the vertices and the edges of the topological graph.  It is usually assumed that any two edges of a topological graph cross a finite number of times, no edge passes through a vertex different from its endpoints, and no two edges touch each other (without crossing). A topological graph is also called a drawing of a graph. An important special class of topological graphs is the class of geometric graphs, where the edges are represented by line segments. (The term geometric graph is sometimes used in a broader, somewhat vague sense.) The theory of topological graphs is an area of graph theory, mainly concerned with combinatorial properties of topological graphs, in particular, with the crossing patterns of their edges. It is closely related to graph drawing, a field which is more application oriented, and topological graph theory, which focuses on embeddings of graphs in surfaces (that is, drawings without crossings).},
  langid = {english},
  annotation = {Page Version ID: 1194890238},
  file = {/home/krawczuk/Zotero/storage/FE8D2MFE/Topological_graph.html}
}

@online{TopologyOptimizationOperational,
  title = {Topology {{Optimization}} of {{Operational Amplifier}} in {{Continuous Space}} via {{Graph Embedding}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9774676?casa_token=SM8jM4GK_qMAAAAA:gPM11JjhdtGnORbikJKlZiaUsja3S33V-N-070gPHDeJyJDRR-U4LbhlSOy-yqm5DCBnFU6H3FL2},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/8R2XVXUP/9774676.html}
}

@online{trivediLeveragingGraphDiffusion2023a,
  title = {Leveraging {{Graph Diffusion Models}} for {{Network Refinement Tasks}}},
  author = {Trivedi, Puja and Rossi, Ryan and Arbour, David and Yu, Tong and Dernoncourt, Franck and Kim, Sungchul and Lipka, Nedim and Park, Namyong and Ahmed, Nesreen K. and Koutra, Danai},
  date = {2023-11-29},
  eprint = {2311.17856},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.17856},
  url = {http://arxiv.org/abs/2311.17856},
  urldate = {2024-02-20},
  abstract = {Most real-world networks are noisy and incomplete samples from an unknown target distribution. Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance. Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between "in-painting" and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion. Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks. In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing "style" transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/YUHFI8KJ/Trivedi et al. - 2023 - Leveraging Graph Diffusion Models for Network Refi.pdf;/home/krawczuk/Zotero/storage/LR3Y9M7M/2311.html}
}

@article{tsvetkovCONVERGENCEMETROPOLISHASTINGSMARKOV2019,
  title = {{{ON THE CONVERGENCE OF THE METROPOLIS-HASTINGS MARKOV CHAINS}}},
  author = {Tsvetkov, Dimiter and Hristov, Lyubomir and Angelova-Slavova, Ralitsa},
  date = {2019-05-11},
  journaltitle = {Serdica Mathematical Journal},
  shortjournal = {Serdica Mathematical Journal},
  volume = {43 (2017)},
  pages = {93--110},
  abstract = {In this paper we study Markov chains associated with the Metropolis-Hastings algorithm. We consider conditions under which the sequence of the successive densities of such a chain converges to the target density according to the total variation distance for any choice of the initial density. In particular we prove that the positiveness of the proposal density is enough for the chain to converge. The content of this work basically presents a stand alone proof that the reversibility along with the kernel positivity imply the convergence.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/2LQ5G9MN/Tsvetkov et al. - 2019 - ON THE CONVERGENCE OF THE METROPOLIS-HASTINGS MARK.pdf}
}

@inproceedings{uhlmannDeepReinforcementLearning2022a,
  title = {Deep {{Reinforcement Learning}} for {{Analog Circuit Sizing}} with an {{Electrical Design Space}} and {{Sparse Rewards}}},
  booktitle = {Proceedings of the 2022 {{ACM}}/{{IEEE Workshop}} on {{Machine Learning}} for {{CAD}}},
  author = {Uhlmann, Yannick and Essich, Michael and Bramlage, Lennart and Scheible, Jürgen and Curio, Cristóbal},
  date = {2022-09-12},
  series = {{{MLCAD}} '22},
  pages = {21--26},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3551901.3556474},
  url = {https://doi.org/10.1145/3551901.3556474},
  urldate = {2024-02-19},
  abstract = {There is still a great reliance on human expert knowledge during the analog integrated circuit sizing design phase due to its complexity and scale, with the result that there is a very low level of automation associated with it. Current research shows that reinforcement learning is a promising approach for addressing this issue. Similarly, it has been shown that the convergence of conventional optimization approaches can be improved by transforming the design space from the geometrical domain into the electrical domain. Here, this design space transformation is employed as an alternative action space for deep reinforcement learning agents. The presented approach is based entirely on reinforcement learning, whereby agents are trained in the craft of analog circuit sizing without explicit expert guidance. After training and evaluating agents on circuits of varying complexity, their behavior when confronted with a different technology, is examined, showing the applicability, feasibility as well as transferability of this approach.},
  isbn = {978-1-4503-9486-4},
  keywords = {analog circuit sizing,neural networks,reinforcement learning}
}

@online{US11409934B2GenerationHardware,
  title = {{{US11409934B2}} - {{Generation}} of Hardware Design Using a Constraint Solver Module for Topology Synthesis - {{Google Patents}}},
  url = {https://patents.google.com/patent/US11409934B2/en},
  urldate = {2024-02-20}
}

@inproceedings{vaswaniAttentionAllYou2017c,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2024-02-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/KINQC94K/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{vianoProximalPointImitation2022,
  title = {Proximal Point Imitation Learning},
  author = {Viano, Luca and Kamoutsi, Angeliki and Neu, Gergely and Krawczuk, Igor and Cevher, Volkan},
  date = {2022},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {24309--24326},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/9988f2c8e07c1f98af7ba9ca31ccae0b-Abstract-Conference.html},
  urldate = {2024-02-27},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/MP4IQQRJ/Viano et al. - 2022 - Proximal point imitation learning.pdf}
}

@inproceedings{vignacBuildingPowerfulEquivariant2020,
  title = {Building Powerful and Equivariant Graph Neural Networks with Structural Message-Passing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vignac, Clément and Loukas, Andreas and Frossard, Pascal},
  date = {2020},
  volume = {33},
  pages = {14143--14155},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/a32d7eeaae19821fd9ce317f3ce952a7-Abstract.html},
  urldate = {2024-02-24},
  abstract = {Message-passing has proved to be an effective way to design graph neural networks, as it is able to leverage both permutation equivariance and an inductive bias towards learning local structures in order to achieve good generalization. However, current message-passing architectures have a limited representation power and fail to learn basic topological properties of graphs. We address this problem and propose a powerful and equivariant message-passing framework based on two ideas: first, we propagate a one-hot encoding of the nodes, in addition to the features, in order to learn a local context matrix around each node. This matrix contains rich local information about both features and topology and can eventually be pooled to build node representations. Second, we propose methods for the parametrization of the message and update functions that ensure permutation equivariance. Having a representation that is independent of the specific choice of the one-hot encoding permits inductive reasoning and leads to better generalization properties. Experimentally, our model can predict various graph topological properties on synthetic data more accurately than previous methods and achieves state-of-the-art results on molecular graph regression on the ZINC dataset.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/34L3ZT5K/Vignac et al. - 2020 - Building powerful and equivariant graph neural net.pdf}
}

@online{vignacDiGressDiscreteDenoising2023b,
  title = {{{DiGress}}: {{Discrete Denoising}} Diffusion for Graph Generation},
  shorttitle = {{{DiGress}}},
  author = {Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  date = {2023-05-23},
  eprint = {2209.14734},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.14734},
  url = {http://arxiv.org/abs/2209.14734},
  urldate = {2024-02-25},
  abstract = {This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/KLZEE85X/Vignac et al. - 2023 - DiGress Discrete Denoising diffusion for graph ge.pdf;/home/krawczuk/Zotero/storage/ATJG5E24/2209.html}
}

@online{vignacDiGressDiscreteDenoising2023c,
  title = {{{DiGress}}: {{Discrete Denoising}} Diffusion for Graph Generation},
  shorttitle = {{{DiGress}}},
  author = {Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  date = {2023-05-23},
  eprint = {2209.14734},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2209.14734},
  urldate = {2024-02-27},
  abstract = {This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/W36HDHNC/Vignac et al. - 2023 - DiGress Discrete Denoising diffusion for graph ge.pdf;/home/krawczuk/Zotero/storage/QRAQ5L26/2209.html}
}

@online{vignacMiDiMixedGraph2023c,
  title = {{{MiDi}}: {{Mixed Graph}} and {{3D Denoising Diffusion}} for {{Molecule Generation}}},
  shorttitle = {{{MiDi}}},
  author = {Vignac, Clement and Osman, Nagham and Toni, Laura and Frossard, Pascal},
  date = {2023-06-05},
  eprint = {2302.09048},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.09048},
  url = {http://arxiv.org/abs/2302.09048},
  urldate = {2024-02-20},
  abstract = {This work introduces MiDi, a novel diffusion model for jointly generating molecular graphs and their corresponding 3D arrangement of atoms. Unlike existing methods that rely on predefined rules to determine molecular bonds based on the 3D conformation, MiDi offers an end-to-end differentiable approach that streamlines the molecule generation process. Our experimental results demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS dataset, MiDi generates 92\% of stable molecules, against 6\% for the previous EDM model that uses interatomic distances for bond prediction, and 40\% using EDM followed by an algorithm that directly optimize bond orders for validity. Our code is available at github.com/cvignac/MiDi.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/4AGAL9AL/Vignac et al. - 2023 - MiDi Mixed Graph and 3D Denoising Diffusion for M.pdf;/home/krawczuk/Zotero/storage/YCL72G5R/2302.html}
}

@inproceedings{vignacTopNEquivariantSet2021d,
  title = {Top-{{N}}: {{Equivariant Set}} and {{Graph Generation}} without {{Exchangeability}}},
  shorttitle = {Top-{{N}}},
  author = {Vignac, Clement and Frossard, Pascal},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=-Gk_IPJWvk},
  urldate = {2024-02-20},
  abstract = {This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. This architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation, a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15\% at SetMNIST reconstruction, by 33\% at object detection on CLEVR, generates sets that are 74\% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/4N9TGKTG/Vignac and Frossard - 2021 - Top-N Equivariant Set and Graph Generation withou.pdf}
}

@article{villarFullyCovariantMachine2023,
  title = {Towards Fully Covariant Machine Learning},
  author = {Villar, Soledad and Hogg, David W. and Yao, Weichi and Kevrekidis, George A. and Schölkopf, Bernhard},
  date = {2023-08-27},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=gllUnpYuXg},
  urldate = {2024-02-20},
  abstract = {Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. The active symmetries are those that must be established by observation and experiment. They include, for instance, translations invariances or rotation invariances of physical law. These symmetries are the subject of most of the equivariant machine learning literature. Our goal, in this conceptual contribution, is to understand the implications for machine learning of the many passive and active symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. We conjecture that the implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/LKPHC5SI/Villar et al. - 2023 - Towards fully covariant machine learning.pdf}
}

@incollection{vlassisBayesianReinforcementLearning2012a,
  title = {Bayesian {{Reinforcement Learning}}},
  booktitle = {Reinforcement {{Learning}}: {{State-of-the-Art}}},
  author = {Vlassis, Nikos and Ghavamzadeh, Mohammad and Mannor, Shie and Poupart, Pascal},
  editor = {Wiering, Marco and family=Otterlo, given=Martijn, prefix=van, useprefix=true},
  date = {2012},
  series = {Adaptation, {{Learning}}, and {{Optimization}}},
  pages = {359--386},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-27645-3_11},
  url = {https://doi.org/10.1007/978-3-642-27645-3_11},
  urldate = {2024-02-26},
  abstract = {This chapter surveys recent lines of work that use Bayesian techniques for reinforcement learning. In Bayesian learning, uncertainty is expressed by a prior distribution over unknown parameters and learning is achieved by computing a posterior distribution based on the data observed. Hence, Bayesian reinforcement learning distinguishes itself from other forms of reinforcement learning by explicitly maintaining a distribution over various quantities such as the parameters of the model, the value function, the policy or its gradient. This yields several benefits: a) domain knowledge can be naturally encoded in the prior distribution to speed up learning; b) the exploration/exploitation tradeoff can be naturally optimized; and c) notions of risk can be naturally taken into account to obtain robust policies.},
  isbn = {978-3-642-27645-3},
  langid = {english},
  keywords = {Markov Decision Process,Neural Information Processing System,Policy Parameter,Reinforcement Learning,Transfer Learning},
  file = {/home/krawczuk/Zotero/storage/I8AEYUGN/Vlassis et al. - 2012 - Bayesian Reinforcement Learning.pdf}
}

@article{wagnerRoadModularity2007,
  title = {The Road to Modularity},
  author = {Wagner, Günter P. and Pavlicev, Mihaela and Cheverud, James M.},
  date = {2007-12},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  volume = {8},
  number = {12},
  pages = {921--931},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0064},
  doi = {10.1038/nrg2267},
  url = {https://www.nature.com/articles/nrg2267},
  urldate = {2024-02-22},
  abstract = {A network of interactions is called modular if it is subdivided into relatively autonomous, internally highly connected components.Modularity has emerged as a rallying point for research in developmental and evolutionary biology, as well as in molecular systems biology.Modularity has been found on all levels of biological organization: the structure of macromolecules, protein–protein interaction networks, gene regulation and the variation of quantitative traits.Various mechanisms can lead to modularity, including gene duplication, neutral mutations and selection for robustness or selection in response to varying environmental conditions.The main challenge for future research is to provide evidence to refute some of these models, and to determine whether modularity has an impact on the rate and pattern of evolution.},
  issue = {12},
  langid = {english},
  keywords = {Agriculture,Animal Genetics and Genomics,Biomedicine,Cancer Research,Gene Function,general,Human Genetics},
  file = {/home/krawczuk/Zotero/storage/PSR29HDF/Wagner et al. - 2007 - The road to modularity.pdf}
}

@inproceedings{wangApproachTopologySynthesis2006,
  title = {An Approach to Topology Synthesis of Analog Circuits Using Hierarchical Blocks and Symbolic Analysis},
  booktitle = {Proceedings of the 2006 {{Asia}} and {{South Pacific Design Automation Conference}}},
  author = {Wang, Xiaoying and Hedrich, Lars},
  date = {2006-01-24},
  series = {{{ASP-DAC}} '06},
  pages = {700--705},
  publisher = {{IEEE Press}},
  location = {{Yokohama, Japan}},
  doi = {10.1145/1118299.1118463},
  url = {https://dl.acm.org/doi/10.1145/1118299.1118463},
  urldate = {2024-02-19},
  abstract = {This paper presents a method of design automation for analog circuits, focusing on topology generation and quick performance evaluation. First we describe mechanisms to generate circuit topologies with hierarchical blocks. Those blocks are specialized by adding terminal information. The connection between blocks is in compliance with a set of synthesis rules, which are extracted from typical schematics in the literature. Symbolic analysis has been used to select an appropriate topology quickly and to help the designer gain a better understanding of a circuit's behavior. Finally, experimental results show the creativity and efficiency of our method.},
  isbn = {978-0-7803-9451-3},
  file = {/home/krawczuk/Zotero/storage/CTJZP5WM/Wang and Hedrich - 2006 - An approach to topology synthesis of analog circui.pdf}
}

@article{wangEfficientArithmeticBlock2023,
  title = {Efficient {{Arithmetic Block Identification With Graph Learning}} and {{Network-Flow}}},
  author = {Wang, Ziyi and He, Zhuolun and Bai, Chen and Yang, Haoyu and Yu, Bei},
  date = {2023-08},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {8},
  pages = {2591--2603},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2022.3227815},
  url = {https://ieeexplore.ieee.org/abstract/document/9975800?casa_token=dcI5WrXDmQkAAAAA:2HalhgJglK4bc3PJlTs7YnN_Ob_ZIiYPkdbTBPN3YUpPXs1Flyu9mRZpt3DZtkUvBE2dHJCe2TRV},
  urldate = {2024-02-20},
  abstract = {Arithmetic block identification in gate-level netlists plays an essential role for various purposes, including malicious logic detection, functional verification, or macro-block optimization. However, current methods usually suffer from either low performance or poor scalability. To address the issue, we come up with a novel framework based on graph learning and network flow analysis, that extracts desired logic components from a complete circuit netlist. We design a novel asynchronous bidirectional graph neural network (ABGNN) dedicated to representation learning on directed acyclic graphs. In addition, we develop a convex cost network-flow-based datapath extraction approach to match the predicted block inputs with predicted block outputs. Experimental results on open-source RISC-V CPU designs demonstrate that our proposed solution significantly outperforms several state-of-the-art arithmetic block identification flows.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Arithmetic,Graph neural networks,graph neural networks (GNNs),logic gates,Logic gates,predictive models,Predictive models,representation learning,Representation learning,task analysis,Task analysis,wires,Wires},
  file = {/home/krawczuk/Zotero/storage/F7STBSFE/Wang et al. - 2023 - Efficient Arithmetic Block Identification With Gra.pdf}
}

@inproceedings{wangFunctionalityMattersNetlist2022c,
  title = {Functionality Matters in Netlist Representation Learning},
  booktitle = {Proceedings of the 59th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Wang, Ziyi and Bai, Chen and He, Zhuolun and Zhang, Guangliang and Xu, Qiang and Ho, Tsung-Yi and Yu, Bei and Huang, Yu},
  date = {2022-08-23},
  series = {{{DAC}} '22},
  pages = {61--66},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3489517.3530410},
  url = {https://dl.acm.org/doi/10.1145/3489517.3530410},
  urldate = {2024-02-20},
  abstract = {Learning feasible representation from raw gate-level netlists is essential for incorporating machine learning techniques in logic synthesis, physical design, or verification. Existing message-passing-based graph learning methodologies focus merely on graph topology while overlooking gate functionality, which often fails to capture underlying semantic, thus limiting their generalizability. To address the concern, we propose a novel netlist representation learning framework that utilizes a contrastive scheme to acquire generic functional knowledge from netlists effectively. We also propose a customized graph neural network (GNN) architecture that learns a set of independent aggregators to better cooperate with the above framework. Comprehensive experiments on multiple complex real-world designs demonstrate that our proposed solution significantly outperforms state-of-the-art netlist feature learning flows.},
  isbn = {978-1-4503-9142-9},
  file = {/home/krawczuk/Zotero/storage/5H5IGJT2/Wang et al. - 2022 - Functionality matters in netlist representation le.pdf}
}

@inproceedings{wangFunctionalityMattersNetlist2022d,
  title = {Functionality Matters in Netlist Representation Learning},
  booktitle = {Proceedings of the 59th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Wang, Ziyi and Bai, Chen and He, Zhuolun and Zhang, Guangliang and Xu, Qiang and Ho, Tsung-Yi and Yu, Bei and Huang, Yu},
  date = {2022-07-10},
  pages = {61--66},
  publisher = {{ACM}},
  location = {{San Francisco California}},
  doi = {10.1145/3489517.3530410},
  url = {https://dl.acm.org/doi/10.1145/3489517.3530410},
  urldate = {2024-02-20},
  eventtitle = {{{DAC}} '22: 59th {{ACM}}/{{IEEE Design Automation Conference}}},
  isbn = {978-1-4503-9142-9},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/MWQ8CFGA/Wang et al. - 2022 - Functionality matters in netlist representation le.pdf}
}

@inproceedings{weiHLSDatasetOpenSourceDataset2023,
  title = {{{HLSDataset}}: {{Open-Source Dataset}} for {{ML-Assisted FPGA Design}} Using {{High Level Synthesis}}},
  shorttitle = {{{HLSDataset}}},
  booktitle = {2023 {{IEEE}} 34th {{International Conference}} on {{Application-specific Systems}}, {{Architectures}} and {{Processors}} ({{ASAP}})},
  author = {Wei, Zhigang and Arora, Aman and Li, Ruihao and John, Lizy},
  date = {2023-07},
  pages = {197--204},
  issn = {2160-052X},
  doi = {10.1109/ASAP57973.2023.00040},
  url = {https://ieeexplore.ieee.org/abstract/document/10265706?casa_token=O3gTcKNewmgAAAAA:T5Ckipmxa05eMFZFX43Zq90Wv20UG7NiiNccUFDo-SmNPmo5C2767eMzp3-8K8NcuR-PB8rR6k1w},
  urldate = {2024-02-25},
  abstract = {Machine Learning (ML) has been widely adopted in design exploration using high level synthesis (HLS) for faster resource, timing and power estimation at very early stages for FPGA-based design. To perform prediction accurately, high-quality and large-volume datasets are required for training ML models. However, the current datasets used in this domain are proprietary or limited in use, and practitioners have to generate their own dataset to train HLS-related ML models. This paper presents a dataset for ML-assisted FPGA design using HLS, called HLSDataset. The dataset is generated from widely used HLS C benchmarks including Polybench, Machsuite, CHStone and Rossetta. The Verilog samples are generated with a variety of directives including loop unroll, loop pipeline, and array partition to make sure optimized and realistic designs are covered. The total number of generated Verilog samples is nearly 9,000 per FPGA type. The dataset repository includes CSV (comma separated values) files containing both HLS and implementation metrics which can be easily consumed by ML model. We also include original C source code with directives, Verilog designs, post-HLS reports, post-implementation reports for each sample in the dataset, so that any metrics not present in the CSV can be easily extracted. In order to extend the dataset for future benchmarks, generation and extraction scripts are also provided. To demonstrate the effectiveness of our dataset, we undertake case studies to perform power estimation and resource usage estimation with ML models trained with our dataset. All the code and dataset are public at our github page11https://github.com/UT-LCAIML4Accel-Dataset/tree/main/fpga\_ml\_dataset. We believe that HLSDataset can save valuable time for researchers by avoiding the tedious process of running tools, scripting and parsing files to generate the dataset, and enable them to spend more time where it counts, that is, in training ML models.},
  eventtitle = {2023 {{IEEE}} 34th {{International Conference}} on {{Application-specific Systems}}, {{Architectures}} and {{Processors}} ({{ASAP}})},
  keywords = {Benchmark testing,High level synthesis,Maximum likelihood estimation,Measurement,Predictive models,Systems architecture,Training},
  file = {/home/krawczuk/Zotero/storage/SJJ3HWKX/Wei et al. - 2023 - HLSDataset Open-Source Dataset for ML-Assisted FP.pdf;/home/krawczuk/Zotero/storage/WT435H4X/10265706.html}
}

@article{weiNetlistManufacturableLayout2023,
  title = {From {{Netlist}} to {{Manufacturable Layout}}: {{An Auto-Layout Algorithm Optimized}} for {{Radio Frequency Integrated Circuits}}},
  shorttitle = {From {{Netlist}} to {{Manufacturable Layout}}},
  author = {Wei, Yiding and Liu, Jun and Sun, Dengbao and Su, Guodong and Wang, Junchao},
  date = {2023-06},
  journaltitle = {Symmetry},
  volume = {15},
  number = {6},
  pages = {1272},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-8994},
  doi = {10.3390/sym15061272},
  url = {https://www.mdpi.com/2073-8994/15/6/1272},
  urldate = {2024-02-19},
  abstract = {Layout stitching is a repetitive and tedious task of the radio frequency integrated circuit (RFIC) design process. While academic research on layout splicing algorithms mainly focuses on analog and digital circuits, there is still a lack of well-developed algorithms for RFICs. An RFIC system usually has a symmetrical layout, such as transmitter and receiver components, low-noise amplifier (LNA), an SPDT switch, etc. This paper aims to address this gap by proposing an automated procedure for the layout of RFICs by relying on the basic device/PCell structure based on the interconnection among circuit topologies. This approach makes the in-series generation of layouts and automatic splicing based on circuit logic possible, resulting in superior stitching performance compared with related modules in Advanced Design System. To demonstrate the physical application possibilities, we implemented our algorithm on an LNA and a switch circuit.},
  issue = {6},
  langid = {english},
  keywords = {layout splicing,PCell structure,RF circuit,topology},
  file = {/home/krawczuk/Zotero/storage/K7ZPJTH8/Wei et al. - 2023 - From Netlist to Manufacturable Layout An Auto-Lay.pdf}
}

@online{winklerLearningLikelihoodsConditional2023,
  title = {Learning {{Likelihoods}} with {{Conditional Normalizing Flows}}},
  author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
  date = {2023-11-12},
  eprint = {1912.00042},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1912.00042},
  url = {http://arxiv.org/abs/1912.00042},
  urldate = {2024-02-24},
  abstract = {Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GI3XMVJN/Winkler et al. - 2023 - Learning Likelihoods with Conditional Normalizing .pdf;/home/krawczuk/Zotero/storage/Q8CD6FWI/1912.html}
}

@inproceedings{wolczykRoleForgettingFineTuning2023,
  title = {On {{The Role}} of {{Forgetting}} in {{Fine-Tuning Reinforcement Learning Models}}},
  author = {Wolczyk, Maciej and Cupiał, Bartłomiej and Zając, Michał and Pascanu, Razvan and Kuciński, Łukasz and Miłoś, Piotr},
  date = {2023-03-03},
  url = {https://openreview.net/forum?id=zmXJUKULDzh},
  urldate = {2024-02-26},
  abstract = {Recently, foundation models have achieved remarkable results in fields such as computer vision and language processing. Although there has been a significant push to introduce similar approaches in reinforcement learning, these have not yet succeeded on a comparable scale. In this paper, we take a step towards understanding and closing this gap by highlighting one of the problems specific to foundation RL models, namely the data shift occurring during fine-tuning. We show that fine-tuning on compositional tasks, where parts of the environment might only be available after a long training period, is inherently prone to catastrophic forgetting. In such a scenario, a pre-trained model might forget useful knowledge before even seeing parts of the state space it can solve. We provide examples of both a grid world and realistic robotic scenarios where catastrophic forgetting occurs. Finally, we show how this problem can be mitigated by using tools from continual learning. We discuss the potential impact of this finding and propose further research directions.},
  eventtitle = {Workshop on {{Reincarnating Reinforcement Learning}} at {{ICLR}} 2023},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/RFNSRK4R/Wolczyk et al. - 2023 - On The Role of Forgetting in Fine-Tuning Reinforce.pdf}
}

@inproceedings{wolkowiczSemidefiniteLagrangianRelaxations2000,
  title = {Semidefinite and {{Lagrangian Relaxations}} for {{Hard Combinatorial Problems}}},
  booktitle = {System {{Modelling}} and {{Optimization}}},
  author = {Wolkowicz, Henry},
  editor = {Powell, M. J. D. and Scholtes, S.},
  date = {2000},
  series = {{{IFIP}} — {{The International Federation}} for {{Information Processing}}},
  pages = {269--309},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-35514-6_13},
  abstract = {Semidefinite Programming is currently a very exciting and active area of research. Semidefinite relaxations generally provide very tight bounds for many classes of numerically hard problems. In addition, these relaxations can be solved efficiently by interior-point methods.},
  isbn = {978-0-387-35514-6},
  langid = {english},
  keywords = {Hard Combinatorial Problems.,Lagrangian Duality,Quadratic Constrained Quadratic Programs,Relaxations,Semidefinite Programming},
  file = {/home/krawczuk/Zotero/storage/5DLJB6GD/Wolkowicz - 2000 - Semidefinite and Lagrangian Relaxations for Hard C.pdf}
}

@online{wuEDGEImprovedTraining2023a,
  title = {{{EDGE}}++: {{Improved Training}} and {{Sampling}} of {{EDGE}}},
  shorttitle = {{{EDGE}}++},
  author = {Wu, Mingyang and Chen, Xiaohui and Liu, Li-Ping},
  date = {2023-10-28},
  eprint = {2310.14441},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.14441},
  url = {http://arxiv.org/abs/2310.14441},
  urldate = {2024-02-20},
  abstract = {Recently developed deep neural models like NetGAN, CELL, and Variational Graph Autoencoders have made progress but face limitations in replicating key graph statistics on generating large graphs. Diffusion-based methods have emerged as promising alternatives, however, most of them present challenges in computational efficiency and generative performance. EDGE is effective at modeling large networks, but its current denoising approach can be inefficient, often leading to wasted computational resources and potential mismatches in its generation process. In this paper, we propose enhancements to the EDGE model to address these issues. Specifically, we introduce a degree-specific noise schedule that optimizes the number of active nodes at each timestep, significantly reducing memory consumption. Additionally, we present an improved sampling scheme that fine-tunes the generative process, allowing for better control over the similarity between the synthesized and the true network. Our experimental results demonstrate that the proposed modifications not only improve the efficiency but also enhance the accuracy of the generated graphs, offering a robust and scalable solution for graph generation tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/7IBW7EXE/Wu et al. - 2023 - EDGE++ Improved Training and Sampling of EDGE.pdf;/home/krawczuk/Zotero/storage/L9HBK8R6/2310.html}
}

@online{wuGamoraGraphLearning2023,
  title = {Gamora: {{Graph Learning}} Based {{Symbolic Reasoning}} for {{Large-Scale Boolean Networks}}},
  shorttitle = {Gamora},
  author = {Wu, Nan and Li, Yingjie and Hao, Cong and Dai, Steve and Yu, Cunxi and Xie, Yuan},
  date = {2023-06-12},
  eprint = {2303.08256},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08256},
  url = {http://arxiv.org/abs/2303.08256},
  urldate = {2024-02-20},
  abstract = {Reasoning high-level abstractions from bit-blasted Boolean networks (BNs) such as gate-level netlists can significantly benefit functional verification, logic minimization, datapath synthesis, malicious logic identification, etc. Mostly, conventional reasoning approaches leverage structural hashing and functional propagation, suffering from limited scalability and inefficient usage of modern computing power. In response, we propose a novel symbolic reasoning framework exploiting graph neural networks (GNNs) and GPU acceleration to reason high-level functional blocks from gate-level netlists, namely Gamora, which offers high reasoning performance w.r.t exact reasoning algorithms, strong scalability to BNs with over 33 million nodes, and generalization capability from simple to complex designs. To further demonstrate the capability of Gamora, we also evaluate its reasoning performance after various technology mapping options, since technology-dependent optimizations are known to make functional reasoning much more challenging. Experimental results show that (1) Gamora reaches almost 100\% and over 97\% reasoning accuracy for carry-save-array (CSA) and Booth-encoded multipliers, respectively, with up to six orders of magnitude speedups compared to the state-of-the-art implementation in the ABC framework; (2) Gamora maintains high reasoning accuracy ({$>$}92\%) in finding functional modules after complex technology mapping, upon which we comprehensively analyze the impacts on Gamora reasoning from technology mapping.},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture},
  file = {/home/krawczuk/Zotero/storage/4CT8S98H/Wu et al. - 2023 - Gamora Graph Learning based Symbolic Reasoning fo.pdf;/home/krawczuk/Zotero/storage/932QZTZJ/2303.html}
}

@article{wuInapproximabilityTreewidthRelated2014,
  title = {Inapproximability of {{Treewidth}} and {{Related Problems}}},
  author = {Wu, Y. and Austrin, P. and Pitassi, T. and Liu, D.},
  date = {2014-04-06},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {49},
  pages = {569--600},
  issn = {1076-9757},
  doi = {10.1613/jair.4030},
  url = {https://www.jair.org/index.php/jair/article/view/10872},
  urldate = {2024-02-22},
  abstract = {Graphical models, such as Bayesian Networks and Markov networks play an important role in artificial intelligence and machine learning. Inference is a central problem to be solved on these networks. This, and other problems on these graph models are often known to be hard to solve in general, but tractable on graphs with bounded Treewidth. Therefore, finding or approximating the Treewidth of a graph is a fundamental problem related to inference in graphical models. In this paper, we study the approximability of a number of graph problems:   Treewidth and Pathwidth of graphs, Minimum Fill-In, One-Shot Black (and Black-White) pebbling costs of directed acyclic graphs, and a variety of different graph layout problems such as Minimum Cut Linear Arrangement and Interval Graph Completion.  We show that, assuming  the recently introduced Small Set Expansion Conjecture, all of these  problems are NP-hard to approximate to within any constant factor in polynomial time.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/S3PIZAUS/Wu et al. - 2014 - Inapproximability of Treewidth and Related Problem.pdf}
}

@online{wuProgramtoCircuitExploitingGNNs2021d,
  title = {Program-to-{{Circuit}}: {{Exploiting GNNs}} for {{Program Representation}} and {{Circuit Translation}}},
  shorttitle = {Program-to-{{Circuit}}},
  author = {Wu, Nan and He, Huake and Xie, Yuan and Li, Pan and Hao, Cong},
  date = {2021-09-13},
  eprint = {2109.06265},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.06265},
  url = {http://arxiv.org/abs/2109.06265},
  urldate = {2024-02-20},
  abstract = {Circuit design is complicated and requires extensive domain-specific expertise. One major obstacle stuck on the way to hardware agile development is the considerably time-consuming process of accurate circuit quality evaluation. To significantly expedite the circuit evaluation during the translation from behavioral languages to circuit designs, we formulate it as a Program-to-Circuit problem, aiming to exploit the representation power of graph neural networks (GNNs) by representing C/C++ programs as graphs. The goal of this work is four-fold. First, we build a standard benchmark containing 40k C/C++ programs, each of which is translated to a circuit design with actual hardware quality metrics, aiming to facilitate the development of effective GNNs targeting this high-demand circuit design area. Second, 14 state-of-the-art GNN models are analyzed on the Program-to-Circuit problem. We identify key design challenges of this problem, which should be carefully handled but not yet solved by existing GNNs. The goal is to provide domain-specific knowledge for designing GNNs with suitable inductive biases. Third, we discuss three sets of real-world benchmarks for GNN generalization evaluation, and analyze the performance gap between standard programs and the real-case ones. The goal is to enable transfer learning from limited training data to real-world large-scale circuit design problems. Fourth, the Program-to-Circuit problem is a representative within the Program-to-X framework, a set of program-based analysis problems with various downstream tasks. The in-depth understanding of strength and weaknesses in applying GNNs on Program-to-Circuit could largely benefit the entire family of Program-to-X. Pioneering in this direction, we expect more GNN endeavors to revolutionize this high-demand Program-to-Circuit problem and to enrich the expressiveness of GNNs on programs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/PJCZDW65/Wu et al. - 2021 - Program-to-Circuit Exploiting GNNs for Program Re.pdf;/home/krawczuk/Zotero/storage/2EIMJH56/2109.html}
}

@inproceedings{wuStructuralEntropyGuided2022,
  title = {Structural {{Entropy Guided Graph Hierarchical Pooling}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Wu, Junran and Chen, Xueyuan and Xu, Ke and Li, Shangzhe},
  date = {2022-06-28},
  pages = {24017--24030},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/wu22b.html},
  urldate = {2024-02-26},
  abstract = {Following the success of convolution on non-Euclidean space, the corresponding pooling approaches have also been validated on various tasks regarding graphs. However, because of the fixed compression ratio and stepwise pooling design, these hierarchical pooling methods still suffer from local structure damage and suboptimal problem. In this work, inspired by structural entropy, we propose a hierarchical pooling approach, SEP, to tackle the two issues. Specifically, without assigning the layer-specific compression ratio, a global optimization algorithm is designed to generate the cluster assignment matrices for pooling at once. Then, we present an illustration of the local structure damage from previous methods in reconstruction of ring and grid synthetic graphs. In addition to SEP, we further design two classification models, SEP-G and SEP-N for graph classification and node classification, respectively. The results show that SEP outperforms state-of-the-art graph pooling methods on graph classification benchmarks and obtains superior performance on node classifications.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/DU9YURWS/Wu et al. - 2022 - Structural Entropy Guided Graph Hierarchical Pooli.pdf}
}

@article{xiangGeneralGraphGenerators2022,
  title = {General Graph Generators: Experiments, Analyses, and Improvements},
  shorttitle = {General Graph Generators},
  author = {Xiang, Sheng and Wen, Dong and Cheng, Dawei and Zhang, Ying and Qin, Lu and Qian, Zhengping and Lin, Xuemin},
  date = {2022-09-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {31},
  number = {5},
  pages = {897--925},
  issn = {0949-877X},
  doi = {10.1007/s00778-021-00701-5},
  url = {https://doi.org/10.1007/s00778-021-00701-5},
  urldate = {2024-02-20},
  abstract = {Graph simulation is one of the most fundamental problems in graph processing and analytics. It can help users to generate new graphs on different scales to mimic observed real-life graphs in many applications such as social networks, biology networks, and information technology. In this paper, we focus on one of the most important types of graph generators: general graph generators, which aim to reproduce the properties of the observed graphs regardless of the domains. Though a variety of graph generators have been proposed in the literature, there are still several important research gaps in this area. In this paper, we first give an overview of the existing general graph generators, including recently emerged deep learning-based approaches. We classify them into four categories: simple model-based generators, complex model-based generators, autoencoder-based generators, and GAN-based generators. Then we conduct a comprehensive experimental evaluation of 20 representative graph generators based on 17 evaluation metrics and 12 real-life graphs. We provide a general roadmap of recommendations for how to select general graph generators under different settings. Furthermore, we propose a new method that can achieve a good trade-off between simulation quality and efficiency. To help researchers and practitioners apply general graph generators in their applications or make a comprehensive evaluation of their proposed general graph generators, we also implement an end-to-end platform that is publicly available.},
  langid = {english},
  keywords = {Experimental evaluation,Graph generator,Graph neural networks,Graph simulation},
  file = {/home/krawczuk/Zotero/storage/DJB6GJ4F/Xiang et al. - 2022 - General graph generators experiments, analyses, a.pdf}
}

@article{xuGeometricGraphLearning,
  title = {Geometric {{Graph Learning From Representation}} to {{Generation}}},
  author = {Xu, Minkai and Leskovec, Jure},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/5YLDJ7MT/Xu and Leskovec - Geometric Graph Learning From Representation to Ge.pdf}
}

@inproceedings{xuGlobalConvergenceLangevin2018,
  title = {Global {{Convergence}} of {{Langevin Dynamics Based Algorithms}} for {{Nonconvex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/9c19a2aa1d84e04b0bd4bc888792bd1e-Abstract.html},
  urldate = {2024-02-25},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/J8ATAQLQ/Xu et al. - 2018 - Global Convergence of Langevin Dynamics Based Algo.pdf}
}

@online{xuHowPowerfulAre2019e,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2019-02-22},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.00826},
  url = {http://arxiv.org/abs/1810.00826},
  urldate = {2024-02-24},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GRKJWLXG/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/home/krawczuk/Zotero/storage/G2WJPL6Y/1810.html}
}

@online{xuHowPowerfulAre2019f,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2019-02-22},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.00826},
  urldate = {2024-02-24},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/8HIDQ5ZG/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/home/krawczuk/Zotero/storage/ZLJ7T5I6/1810.html}
}

@inproceedings{xuSNSNotSynthesizer2022a,
  title = {{{SNS}}'s Not a Synthesizer: A Deep-Learning-Based Synthesis Predictor},
  shorttitle = {{{SNS}}'s Not a Synthesizer},
  booktitle = {Proceedings of the 49th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Xu, Ceyu and Kjellqvist, Chris and Wills, Lisa Wu},
  date = {2022-06-11},
  series = {{{ISCA}} '22},
  pages = {847--859},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3470496.3527444},
  url = {https://dl.acm.org/doi/10.1145/3470496.3527444},
  urldate = {2024-02-20},
  abstract = {The number of transistors that can fit on one monolithic chip has reached billions to tens of billions in this decade thanks to Moore's Law. With the advancement of every technology generation, the transistor counts per chip grow at a pace that brings about exponential increase in design time, including the synthesis process used to perform design space explorations. Such a long delay in obtaining synthesis results hinders an efficient chip development process, significantly impacting time-to-market. In addition, these large-scale integrated circuits tend to have larger and higher-dimension design spaces to explore, making it prohibitively expensive to obtain physical characteristics of all possible designs using traditional synthesis tools. In this work, we propose a deep-learning-based synthesis predictor called SNS (SNS's not a Synthesizer), that predicts the area, power, and timing physical characteristics of a broad range of designs at two to three orders of magnitude faster than the Synopsys Design Compiler while providing on average a 0.4998 RRSE (root relative square error). We further evaluate SNS via two representative case studies, a general-purpose out-of-order CPU case study using RISC-V Boom open-source design and an accelerator case study using an in-house Chisel implementation of DianNao, to demonstrate the capabilities and validity of SNS.},
  isbn = {978-1-4503-8610-4},
  keywords = {integrated circuits,neural networks,RTL-level synthesis},
  file = {/home/krawczuk/Zotero/storage/HBLKUQS2/Xu et al. - 2022 - SNS's not a synthesizer a deep-learning-based syn.pdf}
}

@inproceedings{yangConditionalStructureGeneration2019e,
  title = {Conditional {{Structure Generation}} through {{Graph Variational Generative Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yang, Carl and Zhuang, Peiye and Shi, Wenhan and Luu, Alan and Li, Pan},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html},
  urldate = {2024-02-20},
  abstract = {Graph embedding has been intensively studied recently, due to the advance of various neural network models. Theoretical analyses and empirical studies have pushed forward the translation of discrete graph structures into distributed representation vectors, but seldom considered the reverse direction, i.e., generation of graphs from given related context spaces. Particularly, since graphs often become more meaningful when associated with semantic contexts (e.g., social networks of certain communities, gene networks of certain diseases), the ability to infer graph structures according to given semantic conditions could be of great value. While existing graph generative models only consider graph structures without semantic contexts, we formulate the novel problem of conditional structure generation, and propose a novel unified model of graph variational generative adversarial nets (CondGen) to handle the intrinsic challenges of flexible context-structure conditioning and permutation-invariant generation. Extensive experiments on two deliberately created benchmark datasets of real-world context-enriched networks demonstrate the supreme effectiveness and generalizability of CondGen.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/ZAJ5W7QH/Yang et al. - 2019 - Conditional Structure Generation through Graph Var.pdf}
}

@online{yangScalableDiffusionMaterials2023,
  title = {Scalable {{Diffusion}} for {{Materials Generation}}},
  author = {Yang, Mengjiao and Cho, KwangHwan and Merchant, Amil and Abbeel, Pieter and Schuurmans, Dale and Mordatch, Igor and Cubuk, Ekin Dogus},
  date = {2023-10-18},
  eprint = {2311.09235},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09235},
  url = {http://arxiv.org/abs/2311.09235},
  urldate = {2024-02-20},
  abstract = {Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. Traditionally, models with explicit structures (e.g., graphs) have been used in modeling structural relationships in scientific data (e.g., atoms and bonds in crystals), but generating structures can be difficult to scale to large and complex systems. Another challenge in generating materials is the mismatch between standard generative modeling metrics and downstream applications. For instance, common metrics such as the reconstruction error do not correlate well with the downstream goal of discovering stable materials. In this work, we tackle the scalability challenge by developing a unified crystal representation that can represent any crystal structure (UniMat), followed by training a diffusion probabilistic model on these UniMat representations. Our empirical results suggest that despite the lack of explicit structure modeling, UniMat can generate high fidelity crystal structures from larger and more complex chemical systems, outperforming previous graph-based approaches under various generative modeling metrics. To better connect the generation quality of materials to downstream applications, such as discovering novel stable materials, we propose additional metrics for evaluating generative models of materials, including per-composition formation energy and stability with respect to convex hulls through decomposition energy from Density Function Theory (DFT). Lastly, we show that conditional generation with UniMat can scale to previously established crystal datasets with up to millions of crystals structures, outperforming random structure search (the current leading method for structure discovery) in discovering new stable materials.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/QYL3NASV/Yang et al. - 2023 - Scalable Diffusion for Materials Generation.pdf;/home/krawczuk/Zotero/storage/XQ6B6YKD/2311.html}
}

@article{yangVersatileMultistageGraph2022a,
  title = {Versatile {{Multi-stage Graph Neural Network}} for {{Circuit Representation}}},
  author = {Yang, Shuwen and Yang, Zhihao and Li, Dong and Zhang, Yingxueff and Zhang, Zhanguang and Song, Guojie and Hao, Jianye},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {20313--20324},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/7fa548155f40c014372146be387c4f6a-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/F6TCNA5G/Yang et al. - 2022 - Versatile Multi-stage Graph Neural Network for Cir.pdf}
}

@online{yanSwinGNNRethinkingPermutation2023b,
  title = {{{SwinGNN}}: {{Rethinking Permutation Invariance}} in {{Diffusion Models}} for {{Graph Generation}}},
  shorttitle = {{{SwinGNN}}},
  author = {Yan, Qi and Liang, Zhengyang and Song, Yang and Liao, Renjie and Wang, Lele},
  date = {2023-07-19},
  eprint = {2307.01646},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.01646},
  url = {http://arxiv.org/abs/2307.01646},
  urldate = {2024-02-25},
  abstract = {Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called \$\textbackslash textit\{SwinGNN\}\$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, \$\textbackslash textit\{i.e.\}\$, randomly permuting the generated graphs, which provably converts any graph generative model to a permutation-invariant one. Extensive experiments on synthetic and real-world protein and molecule datasets show that our SwinGNN achieves state-of-the-art performances. Our code is released at https://github.com/qiyan98/SwinGNN.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/2JZ3ML4Y/Yan et al. - 2023 - SwinGNN Rethinking Permutation Invariance in Diff.pdf;/home/krawczuk/Zotero/storage/TKCTCGPW/2307.html}
}

@article{yiGraphDenoisingDiffusion2024a,
  title = {Graph {{Denoising Diffusion}} for {{Inverse Protein Folding}}},
  author = {Yi, Kai and Zhou, Bingxin and Shen, Yiqing and Lió, Pietro and Wang, Yuguang},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/20888d00c5df685de2c09790040e0327-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/5EKY6TSP/Yi et al. - 2024 - Graph Denoising Diffusion for Inverse Protein Fold.pdf}
}

@inproceedings{youGraphRNNGeneratingRealistic2018a,
  title = {{{GraphRNN}}: {{Generating Realistic Graphs}} with {{Deep Auto-regressive Models}}},
  shorttitle = {{{GraphRNN}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {You, Jiaxuan and Ying, Rex and Ren, Xiang and Hamilton, William and Leskovec, Jure},
  date = {2018-07-03},
  pages = {5708--5717},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v80/you18a.html},
  urldate = {2024-02-25},
  abstract = {Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50 times larger than previous deep models.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/MHIK2NN7/You et al. - 2018 - GraphRNN Generating Realistic Graphs with Deep Au.pdf;/home/krawczuk/Zotero/storage/QMZPV84D/You et al. - 2018 - GraphRNN Generating Realistic Graphs with Deep Au.pdf}
}

@article{yuLearningEnergyBasedPrior2023a,
  title = {Learning {{Energy-Based Prior Model}} with {{Diffusion-Amortized MCMC}}},
  author = {Yu, Peiyu and Zhu, Yaxuan and Xie, Sirui and Ma, Xiaojian (Shawn) and Gao, Ruiqi and Zhu, Song-Chun and Wu, Ying Nian},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/85381f4549b5ddf1d48e2e287d7d3d15-Abstract-Conference.html},
  urldate = {2024-02-25},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/P9UM6DWN/Yu et al. - 2023 - Learning Energy-Based Prior Model with Diffusion-A.pdf}
}

@inproceedings{yunGraphTransformerNetworks2019b,
  title = {Graph {{Transformer Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/9d63484abb477c97640154d40595a3bb-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-call meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/4QVF66WV/Yun et al. - 2019 - Graph Transformer Networks.pdf}
}

@article{yusterTreeDecompositionGraphs1998,
  title = {Tree Decomposition of Graphs},
  author = {Yuster, Raphael},
  date = {1998},
  journaltitle = {Random Structures \& Algorithms},
  volume = {12},
  number = {3},
  pages = {237--251},
  issn = {1098-2418},
  doi = {10.1002/(SICI)1098-2418(199805)12:3<237::AID-RSA2>3.0.CO;2-W},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291098-2418%28199805%2912%3A3%3C237%3A%3AAID-RSA2%3E3.0.CO%3B2-W},
  urldate = {2024-02-20},
  abstract = {Let H be a tree on h≥2 vertices. It is shown that if G=(V, E) is a graph with \textbackslash delta (G)\textbackslash ge (|V|/2)+10h\^{}4\textbackslash sqrt|V|\textbackslash log|V|, and h−1 divides |E|, then there is a decomposition of the edges of G into copies of H. This result is asymptotically the best possible for all trees with at least three vertices. © 1998 John Wiley \& Sons, Inc. Random Struct. Alg., 12, 237–251, 1998},
  langid = {english},
  keywords = {decomposition,trees},
  file = {/home/krawczuk/Zotero/storage/6JC9E8UL/Yuster - 1998 - Tree decomposition of graphs.pdf}
}

@article{yuvarajTopologicalClusteringMultilayer2021,
  title = {Topological Clustering of Multilayer Networks},
  author = {Yuvaraj, Monisha and Dey, Asim K. and Lyubchich, Vyacheslav and Gel, Yulia R. and Poor, H. Vincent},
  date = {2021-05-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {21},
  pages = {e2019994118},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2019994118},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.2019994118},
  urldate = {2024-02-22},
  abstract = {Multilayer networks continue to gain significant attention in many areas of study, particularly due to their high utility in modeling interdependent systems such as critical infrastructures, human brain connectome, and socioenvironmental ecosystems. However, clustering of multilayer networks, especially using the information on higher-order interactions of the system entities, still remains in its infancy. In turn, higher-order connectivity is often the key in such multilayer network applications as developing optimal partitioning of critical infrastructures in order to isolate unhealthy system components under cyber-physical threats and simultaneous identification of multiple brain regions affected by trauma or mental illness. In this paper, we introduce the concepts of topological data analysis to studies of complex multilayer networks and propose a topological approach for network clustering. The key rationale is to group nodes based not on pairwise connectivity patterns or relationships between observations recorded at two individual nodes but based on how similar in shape their local neighborhoods are at various resolution scales. Since shapes of local node neighborhoods are quantified using a topological summary in terms of persistence diagrams, we refer to the approach as clustering using persistence diagrams (CPD). CPD systematically accounts for the important heterogeneous higher-order properties of node interactions within and in-between network layers and integrates information from the node neighbors. We illustrate the utility of CPD by applying it to an emerging problem of societal importance: vulnerability zoning of residential properties to weather- and climate-induced risks in the context of house insurance claim dynamics.},
  file = {/home/krawczuk/Zotero/storage/VZELHN56/Yuvaraj et al. - 2021 - Topological clustering of multilayer networks.pdf}
}

@online{zhangCategorytheoreticalMetaanalysisDefinitions2023,
  title = {A {{Category-theoretical Meta-analysis}} of {{Definitions}} of {{Disentanglement}}},
  author = {Zhang, Yivan and Sugiyama, Masashi},
  date = {2023-05-29},
  eprint = {2305.06886},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2305.06886},
  url = {http://arxiv.org/abs/2305.06886},
  urldate = {2024-02-26},
  abstract = {Disentangling the factors of variation in data is a fundamental concept in machine learning and has been studied in various ways by different researchers, leading to a multitude of definitions. Despite the numerous empirical studies, more theoretical research is needed to fully understand the defining properties of disentanglement and how different definitions relate to each other. This paper presents a meta-analysis of existing definitions of disentanglement, using category theory as a unifying and rigorous framework. We propose that the concepts of the cartesian and monoidal products should serve as the core of disentanglement. With these core concepts, we show the similarities and crucial differences in dealing with (i) functions, (ii) equivariant maps, (iii) relations, and (iv) stochastic maps. Overall, our meta-analysis deepens our understanding of disentanglement and its various formulations and can help researchers navigate different definitions and choose the most appropriate one for their specific context.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/UFIRP775/Zhang and Sugiyama - 2023 - A Category-theoretical Meta-analysis of Definition.pdf;/home/krawczuk/Zotero/storage/RFDLFLQX/2305.html}
}

@article{zhangCocoonOpenSourceInfrastructure,
  title = {Cocoon: {{An Open-Source Infrastructure}} for {{Integrated EDA}} with {{Interoperability}} and {{Interactivity}}},
  author = {Zhang, Jiaxi and Dai, Tuo and Ma, Zhengzheng and Lin, Yibo and Luo, Guojie},
  abstract = {The increasing size and complexity of integrated circuit (IC) design introduce huge design cost and put forward higher requirements for EDA tools. Improving the quality and efficiency of chip design requires the efforts of both EDA workers and IC designers. In this paper, we first put forward the concept of Integrated EDA, a system composed of EDA points tools, designs and interfaces. And we point out the key features of integrated EDA and the possible solutions. Then we propose Cocoon, an open-source infrastructure for integrated EDA with interoperability and interactivity. It contains a set of cross-tool interfaces and plays the role of EDA agent that can help IC designers choose EDA point tools to assemble a legal design flow and to produce ICs with a higher quality of results (QoR). At last, we implement two demos using Cocoon to prove that such infrastructure is feasible and flexible for integrated EDA.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/MCMM8ILZ/Zhang et al. - Cocoon An Open-Source Infrastructure for Integrat.pdf}
}

@article{zhangEfficientBatchConstrainedBayesian2022,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}}},
  author = {Zhang, Shuhan and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2021.3054811},
  url = {https://ieeexplore.ieee.org/document/9336041},
  urldate = {2024-02-19},
  abstract = {Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this article, we propose an efficient parallelizable Bayesian optimization algorithm via multiobjective acquisition function ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI), and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74\textbackslash times compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15\textbackslash times compared to the weighted EI-based Bayesian optimization (WEIBO) approach, when the batch size is 15.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Acquisition function,analog circuit synthesis,Analog circuits,batch Bayesian optimization,Bayes methods,Circuit synthesis,constrained optimization problem,Gaussian processes,Integrated circuit modeling,Optimization,Space exploration},
  file = {/home/krawczuk/Zotero/storage/JAXPWZ7N/Zhang et al. - 2022 - An Efficient Batch-Constrained Bayesian Optimizati.pdf;/home/krawczuk/Zotero/storage/Z8TYCPTR/9336041.html}
}

@article{zhangEfficientBatchConstrainedBayesian2022a,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}}},
  author = {Zhang, Shuhan and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2021.3054811},
  url = {https://ieeexplore.ieee.org/abstract/document/9336041?casa_token=fPG-7PuzrdgAAAAA:S9Y9hlsyrUarBMsLk8oVfvtisLBEAA9pVyN4KY3L9BNOOoDr4ZhBs5t5v0M1pw27rOSlhvXllF66},
  urldate = {2024-02-20},
  abstract = {Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this article, we propose an efficient parallelizable Bayesian optimization algorithm via multiobjective acquisition function ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI), and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74\textbackslash times compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15\textbackslash times compared to the weighted EI-based Bayesian optimization (WEIBO) approach, when the batch size is 15.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Acquisition function,analog circuit synthesis,Analog circuits,batch Bayesian optimization,Bayes methods,Circuit synthesis,constrained optimization problem,Gaussian processes,Integrated circuit modeling,Optimization,Space exploration},
  file = {/home/krawczuk/Zotero/storage/5J5D7W5V/Zhang et al. - 2022 - An Efficient Batch-Constrained Bayesian Optimizati.pdf}
}

@article{zhangEfficientBatchConstrainedBayesian2022b,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}}},
  author = {Zhang, Shuhan and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2021.3054811},
  url = {https://ieeexplore.ieee.org/document/9336041/},
  urldate = {2024-02-20},
  abstract = {Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this article, we propose an efficient parallelizable Bayesian optimization algorithm via multiobjective acquisition function ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI), and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74× compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15× compared to the weighted EI-based Bayesian optimization (WEIBO) approach, when the batch size is 15.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/FH9FHIIE/Zhang et al. - 2022 - An Efficient Batch-Constrained Bayesian Optimizati.pdf}
}

@article{zhangMg2vecLearningRelationshipPreserving2022,
  title = {Mg2vec: {{Learning Relationship-Preserving Heterogeneous Graph Representations}} via {{Metagraph Embedding}}},
  shorttitle = {Mg2vec},
  author = {Zhang, Wentao and Fang, Yuan and Liu, Zemin and Wu, Min and Zhang, Xinming},
  date = {2022-03},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {3},
  pages = {1317--1329},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2020.2992500},
  url = {https://ieeexplore.ieee.org/abstract/document/9089251?casa_token=4RGA3jCG5zsAAAAA:oEZwjXhBIJreFavjipIsBtilcbT4v_yGF7oi9rVPWRP40c4s3R9KfQs_2jyNrP4Nln_k7ztEjnkg},
  urldate = {2024-02-22},
  abstract = {Given that heterogeneous information networks (HIN) encompass nodes and edges belonging to different semantic types, they can model complex data in real-world scenarios. Thus, HIN embedding has received increasing attention, which aims to learn node representations in a low-dimensional space, in order to preserve the structural and semantic information on the HIN. In this regard, metagraphs, which model common and recurring patterns on HINs, emerge as a powerful tool to capture semantic-rich and often latent relationships on HINs. Although metagraphs have been employed to address several specific data mining tasks, they have not been thoroughly explored for the more general HIN embedding. In this paper, we leverage metagraphs to learn relationship-preserving HIN embedding in a self-supervised setting, to support various relationship mining tasks. In particular, we observe that most of the current approaches often under-utilize metagraphs, which are only applied in a pre-processing step and do not actively guide representation learning afterwards. Thus, we propose the novel framework of mg2vec, which learns the embeddings for metagraphs and nodes jointly. That is, metagraphs actively participates in the learning process by mapping themselves to the same embedding space as the nodes do. Moreover, metagraphs guide the learning through both first- and second-order constraints on node embeddings, to model not only latent relationships between a pair of nodes, but also individual preferences of each node. Finally, we conduct extensive experiments on three public datasets. Results show that mg2vec significantly outperforms a suite of state-of-the-art baselines in relationship mining tasks including relationship prediction, search and visualization.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Companies,Data mining,Heterogeneous information networks,network embedding,Peer-to-peer computing,relationship mining,Semantics,Task analysis,Tools,Toy manufacturing industry},
  file = {/home/krawczuk/Zotero/storage/6474M8ZH/Zhang et al. - 2022 - mg2vec Learning Relationship-Preserving Heterogen.pdf;/home/krawczuk/Zotero/storage/S6LGJG5Q/9089251.html}
}

@inproceedings{zhangPruningEdgesGradients2022b,
  title = {Pruning {{Edges}} and {{Gradients}} to {{Learn Hypergraphs From Larger Sets}}},
  booktitle = {Proceedings of the {{First Learning}} on {{Graphs Conference}}},
  author = {Zhang, David W. and Burghouts, Gertjan J. and Snoek, Cees G. M.},
  date = {2022-12-21},
  pages = {53:1-53:18},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v198/zhang22a.html},
  urldate = {2024-02-20},
  abstract = {This paper aims for set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. We address two common scaling problems encountered in set-to-hypergraph tasks that limit the size of the input set: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. We make three contributions. First, we propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. Second, we introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Third, we combine both contributions in a single set-to-hypergraph model that enables us to address problems with larger input set sizes. We provide ablations for our main technical contributions and show that our model outperforms prior state-of-the-art, especially for larger sets.},
  eventtitle = {Learning on {{Graphs Conference}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/P5EN4LFX/Zhang et al. - 2022 - Pruning Edges and Gradients to Learn Hypergraphs F.pdf}
}

@article{zhaoAutomatedTopologySynthesis2020d,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}}},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {12},
  pages = {4325--4337},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2020.2977605},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939?casa_token=3xBY3Muf444AAAAA:T1GnaWYGkpXGyr_Iyex2Z4cDkDe0zsRZlKTAp9br3AnagS94FGBj_5SpgcAeqAZc630quHfvRzHA},
  urldate = {2024-02-19},
  abstract = {This article presents an analog integrated circuit automated topology synthesis framework, where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction. Then the tree structures are decoded into circuit topologies. Our proposed method can not only handle large circuit designs but also generate creative topologies. To ensure only unique circuit topologies to be generated, two levels of isomorphism checks are performed at both tree structure level and circuit topology level. Then the generated un-sized circuit topologies are efficiently evaluated through a new method, which integrates topological symbolic analysis with gm/ID methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Circuit synthesis,Circuit topology,Circuit topology synthesis,curve-fitting,gm/ID methodology,Grammar,graph grammar,isomorphism,Libraries,linear and nonlinear programming (NLP),Reliability,symbolic analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/UMNJ8B9U/Zhao and Zhang - 2020 - An Automated Topology Synthesis Framework for Anal.pdf}
}

@article{zhaoAutomatedTopologySynthesis2020e,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}}},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {12},
  pages = {4325--4337},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2020.2977605},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939},
  urldate = {2024-02-19},
  abstract = {This article presents an analog integrated circuit automated topology synthesis framework, where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction. Then the tree structures are decoded into circuit topologies. Our proposed method can not only handle large circuit designs but also generate creative topologies. To ensure only unique circuit topologies to be generated, two levels of isomorphism checks are performed at both tree structure level and circuit topology level. Then the generated un-sized circuit topologies are efficiently evaluated through a new method, which integrates topological symbolic analysis with gm/ID methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Circuit synthesis,Circuit topology,Circuit topology synthesis,curve-fitting,gm/ID methodology,Grammar,graph grammar,isomorphism,Libraries,linear and nonlinear programming (NLP),Reliability,symbolic analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/4MI4JT5C/Zhao and Zhang - 2020 - An Automated Topology Synthesis Framework for Anal.pdf;/home/krawczuk/Zotero/storage/ZVUEI6LU/9022939.html}
}

@article{zhaoAutomatedTopologySynthesis2020f,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}}},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {12},
  pages = {4325--4337},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2020.2977605},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939?casa_token=7uxs-X0h7PsAAAAA:cb9E_EcXOv70Pyezoy0nQ_Ka-U-MpECq4dc5861LbjSvTGYzg3kUQ8itqlQFuRlbq3_bWJusNHLG},
  urldate = {2024-02-20},
  abstract = {This article presents an analog integrated circuit automated topology synthesis framework, where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction. Then the tree structures are decoded into circuit topologies. Our proposed method can not only handle large circuit designs but also generate creative topologies. To ensure only unique circuit topologies to be generated, two levels of isomorphism checks are performed at both tree structure level and circuit topology level. Then the generated un-sized circuit topologies are efficiently evaluated through a new method, which integrates topological symbolic analysis with gm/ID methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Circuit synthesis,Circuit topology,Circuit topology synthesis,curve-fitting,gm/ID methodology,Grammar,graph grammar,isomorphism,Libraries,linear and nonlinear programming (NLP),Reliability,symbolic analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/U7DHXFTD/9022939.html}
}

@thesis{zhaoAutomatedTopologySynthesis2022a,
  type = {doctoral},
  title = {Automated Topology Synthesis for Analog Integrated Circuits},
  author = {Zhao, Zhenxin},
  date = {2022-05},
  institution = {{Memorial University of Newfoundland}},
  url = {https://research.library.mun.ca/15733/},
  urldate = {2024-02-20},
  abstract = {Currently, except for circuit topology synthesis, all the other phases in the analog integrated circuit design procedure are equipped with electronic design automation (EDA) commercial tools to greatly facilitate the human laborious work and significantly improve the design productivity, even though they are still not as mature as digital EDA counterparts. This dissertation focuses on developing a circuit topology synthesis EDA tool for analog integrated circuits. In order to make the developed EDA tool commercializable, there are many challenges that have to be solved, including trustworthy solutions, innovative solutions, wide applicability, sound generalization capability, and affordable computation effort. This thesis proposes a graph-based generation method to automatically synthesize analog integrated circuits, which has partially solved some challenges. But one serious problem of this method is its unaffordable computation effort due to the time-consuming sizing process for a huge number of generated circuit structures. To address this problem, we propose a novel performance modeling method that can boost the sizing efficiency by more than 30 times with ignorable model building overhead, which is especially suitable for the circuit synthesis work that involves generating various circuit structures. With the assistance of the emerging machine learning advancement, EDA tools can be more efficient and effective. We have employed the deep reinforcement learning technique in this dissertation to synthesize analog integrated circuit structures. Its technical merits make it be able to address those pending challenges much better than the graph-based generation method. But it still suffers from a shortcoming, that is, the learning process has to be performed from scratch once the technology or design specification changes. In order to overcome this shortcoming, the transfer learning technique is applied to transfer the learned knowledge from a learning process to another in order to largely save the learning effort. The experimental results exhibit strong efficacy and great applicability of our proposed methods.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/696XPU7M/Zhao - 2022 - Automated topology synthesis for analog integrated.pdf;/home/krawczuk/Zotero/storage/IJE6F468/15733.html}
}

@inproceedings{zhaoDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} for {{Analog Circuit Structure Synthesis}}},
  booktitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2022-03},
  pages = {1157--1160},
  issn = {1558-1101},
  doi = {10.23919/DATE54114.2022.9774699},
  url = {https://ieeexplore.ieee.org/abstract/document/9774699?casa_token=6DJz5YkmihoAAAAA:yJJNEopf_gZwhkjWLDq_MMOBt-pOL0OkwWypqhPaXy_5cnNr4LG9ELx1Ylz6af9ARurxEo4go_Lc},
  urldate = {2024-02-19},
  abstract = {This paper presents a novel deep-reinforcement-learning-based method for analog circuit structure synthesis. It behaves like a designer, who learns from trials, derives design knowledge and experience, and evolves gradually to eventually figure out a way to construct circuit structures that can meet the given design specifications. Necessary design rules are defined and applied to set up the specialized environment of reinforcement learning in order to reasonably construct circuit structures. The produced circuit structures are then verified by the simulation-in-loop sizing. In addition, hash table and symbolic analysis techniques are employed to significantly promote the evaluation efficiency. Our experimental results demonstrate the sound efficiency, strong reliability, and wide applicability of the proposed method.},
  eventtitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {analog circuit synthesis,Analog circuits,Circuit synthesis,deep reinforcement learning,hash table,Integrated circuits,MIMICs,Neural networks,Reinforcement learning,Reliability},
  file = {/home/krawczuk/Zotero/storage/C5QWLXPU/Zhao and Zhang - 2022 - Deep Reinforcement Learning for Analog Circuit Str.pdf;/home/krawczuk/Zotero/storage/EH5RAFJ7/9774699.html}
}

@inproceedings{zhaoGraphGrammarBasedAnalogCircuit2019,
  title = {Graph-{{Grammar-Based Analog Circuit Topology Synthesis}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2019-05},
  pages = {1--5},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2019.8702574},
  url = {https://ieeexplore.ieee.org/document/8702574},
  urldate = {2024-02-19},
  abstract = {Automatically constructing analog circuit topology according to specifications is always a challenging task, due to the high complexity and substantial design expertise required. This paper proposes a graph-grammar-based method that can efficiently and automatically generate analog circuit topologies, which can be applied to general analog circuit synthesis frameworks for analog circuit design. The topology generation process is encoded by constructing a binary tree, in which the leaf nodes are decomposed according to a set of grammar rules. In order to guarantee only unique circuit structures to be generated, double isomorphism checks are applied at both tree structure level and circuit transistor level. Our experimental results demonstrate the high efficiency and wide applicability of the proposed method.},
  eventtitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  keywords = {analog circuit synthesis,Analog circuits,binary tree,Binary trees,Circuit synthesis,Circuit topology,Grammar,graph grammar,isomorphism,Production,Topology},
  file = {/home/krawczuk/Zotero/storage/YZS3BC2S/Zhao and Zhang - 2019 - Graph-Grammar-Based Analog Circuit Topology Synthe.pdf;/home/krawczuk/Zotero/storage/9HZUYZFJ/8702574.html}
}

@inproceedings{zhaoGraphGrammarBasedAnalogCircuit2019a,
  title = {Graph-{{Grammar-Based Analog Circuit Topology Synthesis}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2019-05},
  pages = {1--5},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2019.8702574},
  url = {https://ieeexplore.ieee.org/abstract/document/8702574?casa_token=uSk50cYGkdkAAAAA:ku0uzr88bmIkOPMFRzBdLio60QozMU4N_jqxbZXjfAZ5d81YnskNlgn565S4ZSVECDNOa1qq2xA},
  urldate = {2024-02-24},
  abstract = {Automatically constructing analog circuit topology according to specifications is always a challenging task, due to the high complexity and substantial design expertise required. This paper proposes a graph-grammar-based method that can efficiently and automatically generate analog circuit topologies, which can be applied to general analog circuit synthesis frameworks for analog circuit design. The topology generation process is encoded by constructing a binary tree, in which the leaf nodes are decomposed according to a set of grammar rules. In order to guarantee only unique circuit structures to be generated, double isomorphism checks are applied at both tree structure level and circuit transistor level. Our experimental results demonstrate the high efficiency and wide applicability of the proposed method.},
  eventtitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  keywords = {analog circuit synthesis,Analog circuits,binary tree,Binary trees,Circuit synthesis,Circuit topology,Grammar,graph grammar,isomorphism,Production,Topology},
  file = {/home/krawczuk/Zotero/storage/IUI9XMBX/Zhao and Zhang - 2019 - Graph-Grammar-Based Analog Circuit Topology Synthe.pdf;/home/krawczuk/Zotero/storage/D4AARICB/8702574.html}
}

@online{zhaoPardPermutationInvariantAutoregressive2024a,
  title = {Pard: {{Permutation-Invariant Autoregressive Diffusion}} for {{Graph Generation}}},
  shorttitle = {Pard},
  author = {Zhao, Lingxiao and Ding, Xueying and Akoglu, Leman},
  date = {2024-02-05},
  eprint = {2402.03687},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2402.03687},
  url = {http://arxiv.org/abs/2402.03687},
  urldate = {2024-02-20},
  abstract = {Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/YA3TLZZJ/Zhao et al. - 2024 - Pard Permutation-Invariant Autoregressive Diffusi.pdf;/home/krawczuk/Zotero/storage/GCLEUHII/2402.html}
}

@online{zhaoPardPermutationInvariantAutoregressive2024b,
  title = {Pard: {{Permutation-Invariant Autoregressive Diffusion}} for {{Graph Generation}}},
  shorttitle = {Pard},
  author = {Zhao, Lingxiao and Ding, Xueying and Akoglu, Leman},
  date = {2024-02-05},
  eprint = {2402.03687},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2402.03687},
  url = {http://arxiv.org/abs/2402.03687},
  urldate = {2024-02-25},
  abstract = {Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/KF2UULCD/Zhao et al. - 2024 - Pard Permutation-Invariant Autoregressive Diffusi.pdf;/home/krawczuk/Zotero/storage/39P5SZ5W/2402.html}
}

@article{zhaoSignalDivisionAwareAnalogCircuit2023,
  title = {Signal-{{Division-Aware Analog Circuit Topology Synthesis Aided}} by {{Transfer Learning}}},
  author = {Zhao, Zhenxin and Luo, Jiang and Liu, Jun and Zhang, Lihong},
  date = {2023},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  publisher = {{IEEE}},
  doi = {10.1109/TCAD.2023.3245979},
  url = {https://ieeexplore.ieee.org/abstract/document/10045726/?casa_token=pKwJBn97qqUAAAAA:IAX_s71j4QkY8MGOfPxOaCi0kZHrw4UcN1-IFIxTCZOuOg47IgDbbugxtSij33jUHwGsqbFdAMkv},
  urldate = {2024-02-19}
}

@inproceedings{zhouSPARCSelfPacedNetwork2018,
  title = {{{SPARC}}: {{Self-Paced Network Representation}} for {{Few-Shot Rare Category Characterization}}},
  shorttitle = {{{SPARC}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zhou, Dawei and He, Jingrui and Yang, Hongxia and Fan, Wei},
  date = {2018-07-19},
  series = {{{KDD}} '18},
  pages = {2807--2816},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3219819.3219968},
  url = {https://dl.acm.org/doi/10.1145/3219819.3219968},
  urldate = {2024-02-22},
  abstract = {In the era of big data, it is often the rare categories that are of great interest in many high-impact applications, ranging from financial fraud detection in online transaction networks to emerging trend detection in social networks, from network intrusion detection in computer networks to fault detection in manufacturing. As a result, rare category characterization becomes a fundamental learning task, which aims to accurately characterize the rare categories given limited label information. The unique challenge of rare category characterization, i.e., the non-separability nature of the rare categories from the majority classes, together with the availability of the multi-modal representation of the examples, poses a new research question: how can we learn a salient rare category oriented embedding representation such that the rare examples are well separated from the majority class examples in the embedding space, which facilitates the follow-up rare category characterization? To address this question, inspired by the family of curriculum learning that simulates the cognitive mechanism of human beings, we propose a self-paced framework named SPARC that gradually learns the rare category oriented network representation and the characterization model in a mutually beneficial way by shifting from the 'easy' concept to the target 'difficult' one, in order to facilitate more reliable label propagation to the large number of unlabeled examples. The experimental results on various real data demonstrate that our proposed SPARC algorithm: (1) shows a significant improvement over state-of-the-art graph embedding methods on representing the rare categories that are non-separable from the majority classes; (2) outperforms the existing methods on rare category characterization tasks.},
  isbn = {978-1-4503-5552-0},
  keywords = {network embedding,rare category analysis,self-paced learning},
  file = {/home/krawczuk/Zotero/storage/BADXTNDM/Zhou et al. - 2018 - SPARC Self-Paced Network Representation for Few-S.pdf}
}

@inproceedings{zhuEffectiveAnalogMixedsignal2020b,
  title = {Effective Analog/Mixed-Signal Circuit Placement Considering System Signal Flow},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Computer-Aided Design}}},
  author = {Zhu, Keren and Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Sun, Nan and Pan, David Z.},
  date = {2020-12-17},
  series = {{{ICCAD}} '20},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3400302.3415625},
  url = {https://dl.acm.org/doi/10.1145/3400302.3415625},
  urldate = {2024-02-20},
  abstract = {Placement is among the most critical steps in analog/mixed-signal (AMS) circuit layout synthesis. It implicitly determines the wiring topology and therefore has considerable impacts on post-layout parasitics and coupling. Existing analog placement techniques are mainly focusing on geometric constraints in analog building blocks. However, there yet lacks an effective way to consider the systemlevel signal flow for sensitive AMS circuits. Leveraging prior knowledge from schematics, we propose to consider the critical signal paths in automatic AMS placement and present an efficient framework. Experimental results demonstrate our proposed framework's efficiency and effectiveness with a 22.8\% reduction in routed wire-length compared to state-of-the-art AMS placer and 10 dB improvement in the signal-to-noise-and-distortion ratio (SNDR) for an ADC.},
  isbn = {978-1-4503-8026-3},
  file = {/home/krawczuk/Zotero/storage/MW643V64/Zhu et al. - 2020 - Effective analogmixed-signal circuit placement co.pdf}
}

@inproceedings{zhuEffectiveAnalogMixedsignal2020c,
  title = {Effective Analog/Mixed-Signal Circuit Placement Considering System Signal Flow},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Computer-Aided Design}}},
  author = {Zhu, Keren and Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Sun, Nan and Pan, David Z.},
  date = {2020-11-02},
  pages = {1--9},
  publisher = {{ACM}},
  location = {{Virtual Event USA}},
  doi = {10.1145/3400302.3415625},
  url = {https://dl.acm.org/doi/10.1145/3400302.3415625},
  urldate = {2024-02-20},
  eventtitle = {{{ICCAD}} '20: {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  isbn = {978-1-4503-8026-3},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/5NFLZPNZ/Zhu et al. - 2020 - Effective analogmixed-signal circuit placement co.pdf}
}

@article{zhuFullyautomatedLayoutSynthesis2022,
  title = {Fully-Automated Layout Synthesis for Analog and Mixed-Signal Integrated Circuits},
  author = {Zhu, Keren},
  date = {2022-08-03},
  url = {https://hdl.handle.net/2152/116451},
  urldate = {2024-02-26},
  abstract = {The performance of analog circuits is critically dependent on layout parasitics, but the layout has traditionally been a manual and time-consuming task. Analog and mixed-signal (AMS) circuits often impose specific parasitics and mismatch requirements on their layout implementation. Designers leverage their prior experience to place devices in specific patterns and configurations to reduce parasitics, the effects of local variation gradients, and layout-dependent effects. The reason behind this is from both the algorithm and software. Automated AMS layout synthesis faces challenges in developing effective place-and-route (PNR) algorithms for high-performance AMS circuits and lacks easily usable and accessible software. This dissertation covers several analog PNR algorithms to improve the quality of automated layout synthesis and the circuit learning methodology targeting further reducing human efforts. The proposed techniques have become critical parts of the open-source AMS layout synthesis software MAGICAL. This dissertation first proposes a novel analog routing methodology. The proposed framework, GeniusRoute, leverages machine learning to provide routing guidance, mimicking the sophisticated manual layout approaches. This approach allows the automatic analog router to follow the design expertise of human engineers while no additional manual effort is required to code the layout strategies. The proposed methodology obtains significant improvements over existing techniques and achieves competitive performance to manual layouts while capable of generalizing to circuits of different functionality. This dissertation also proposes a practical mixed-signal placement framework. Unlike the existing techniques, which mainly focus on geometric constraints in analog building blocks, the proposed framework formulates and effectively optimizes the system-level signal flow for sensitive mixed-signal circuits. Leveraging prior knowledge from schematics, we propose considering the critical signal paths in automatic AMS placement and presenting an efficient framework. The proposed framework shows efficiency and effectiveness with a reduced routed wirelength compared to a state-of-the-art AMS placer and improved post-layout performance. Furthermore, the well generation in the analog layout synthesis flow is revisited. Instead of treating well generation as an isolated process, we propose a new methodology of well-aware placement. We formulate the well-aware placement problem and propose a machine learning-guided placement framework. By allowing well sharing between transistors and explicitly considering wells in placement, the proposed framework achieves more than 74\% improvement in the area and more than 26\% reduction in half-perimeter wirelength over existing placement methodologies in experimental results. Finally, this dissertation revisits and explores the fundamental problem of analog circuit learning. A novel unsupervised circuit learning framework is proposed to leverage the human layout as a training label. The machine learning model is pre-trained with automatically extracted labels and then transferred to other downstream tasks. The transferrable circuit representation model demonstrates the possibility of a machine learning model to understand the circuits.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/EIPU3UGC/Zhu - 2022 - Fully-automated layout synthesis for analog and mi.pdf}
}

@inproceedings{zhuTAGLearningCircuit2022b,
  title = {{{TAG}}: {{Learning Circuit Spatial Embedding}} from {{Layouts}}},
  shorttitle = {{{TAG}}},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Zhu, Keren and Chen, Hao and Turner, Walker J. and Kokai, George F. and Wei, Po-Hsuan and Pan, David Z. and Ren, Haoxing},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3549384},
  url = {https://dl.acm.org/doi/10.1145/3508352.3549384},
  urldate = {2024-02-26},
  abstract = {Analog and mixed-signal (AMS) circuit designs still rely on human design expertise. Machine learning has been assisting circuit design automation by replacing human experience with artificial intelligence. This paper presents TAG, a new paradigm of learning the circuit representation from layouts leveraging Text, self Attention and Graph. The embedding network model learns spatial information without manual labeling. We introduce text embedding and a self-attention mechanism to AMS circuit learning. Experimental results demonstrate the ability to predict layout distances between instances with industrial FinFET technology benchmarks. The effectiveness of the circuit representation is verified by showing the transferability to three other learning tasks with limited data in the case studies: layout matching prediction, wirelength estimation, and net parasitic capacitance prediction.},
  isbn = {978-1-4503-9217-4},
  file = {/home/krawczuk/Zotero/storage/D89SXDMN/Zhu et al. - 2022 - TAG Learning Circuit Spatial Embedding from Layou.pdf}
}
