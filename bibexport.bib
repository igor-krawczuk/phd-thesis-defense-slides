@online{230914322Smallscale,
  title = {[2309.14322] {{Small-scale}} Proxies for Large-Scale {{Transformer}} Training Instabilities},
  url = {https://arxiv.org/abs/2309.14322},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/PHFUUQU5/2309.html}
}

@article{abbeNonuniversalityDeepLearning2022c,
  title = {On the Non-Universality of Deep Learning: Quantifying the Cost of Symmetry},
  shorttitle = {On the Non-Universality of Deep Learning},
  author = {Abbe, Emmanuel and Boix-Adsera, Enric},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {17188--17201},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/6d9aac9407bcb1a5957401fa0b8de693-Abstract-Conference.html},
  urldate = {2024-02-24},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YDQJETRJ/Abbe and Boix-Adsera - 2022 - On the non-universality of deep learning quantify.pdf}
}

@article{abelFUBOCOStructureSynthesis2022,
  title = {{{FUBOCO}}: {{Structure Synthesis}} of {{Basic Op-Amps}} by {{FUnctional BlOck COmposition}}},
  shorttitle = {{{FUBOCO}}},
  author = {Abel, Inga and Graeb, Helmut},
  date = {2022-06-27},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {27},
  number = {6},
  pages = {63:1--63:27},
  issn = {1084-4309},
  doi = {10.1145/3522738},
  url = {https://dl.acm.org/doi/10.1145/3522738},
  urldate = {2024-02-19},
  abstract = {This article presents a method to automatically synthesize the structure and initial sizing of an operational amplifier. It is positioned between approaches with fixed design plans and a small search space of structures and approaches with generic structural production rules and a large search space with technically impractical structures. The presented approach develops a hierarchical composition graph based on functional blocks that spans a search space of thousands of technically meaningful structure variants for single-output, fully differential, and complementary operational amplifiers. The search algorithm is a combined heuristic and enumerative process. The evaluation is based on circuit sizing with a library of behavioral equations of functional blocks. Formalizing the knowledge of functional blocks in op-amps for structural synthesis and sizing inherently reduces the search space and lessens the number of created topologies not fulfilling the specifications. Experimental results for the three op-amp classes are presented. An outlook how this method can be extended to multi-stage op-amps is given.},
  keywords = {Analog circuit design,CMOS,operational amplifiers},
  file = {/home/krawczuk/Zotero/storage/I3K3EPJL/Abel and Graeb - 2022 - FUBOCO Structure Synthesis of Basic Op-Amps by FU.pdf}
}

@online{AdaptiveLayoutDecomposition,
  title = {Adaptive {{Layout Decomposition With Graph Embedding Neural Networks}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9672139?casa_token=H7aXYoXqoRsAAAAA:FV_lv-zfdfH0hasodGZR4fYBmowWv7vRcyeYF5q8oJCYfI_5_HAH-UaEFLsFAqxVzSHRw3xTiTuQ},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/SMEZUMPA/9672139.html}
}

@article{afacanReviewMachineLearning2021a,
  title = {Review: {{Machine}} Learning Techniques in Analog/{{RF}} Integrated Circuit Design, Synthesis, Layout, and Test},
  shorttitle = {Review},
  author = {Afacan, Engin and Lourenço, Nuno and Martins, Ricardo and Dündar, Günhan},
  date = {2021-03-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {77},
  pages = {113--130},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2020.11.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926020302947},
  urldate = {2024-02-20},
  abstract = {Rapid developments in semiconductor technology have substantially increased the computational capability of computers. As a result of this and recent developments in theory, machine learning (ML) techniques have become attractive in many new applications. This trend has also inspired researchers working on integrated circuit (IC) design and optimization. ML-based design approaches have gained importance to challenge/aid conventional design methods since they can be employed at different design levels, from modeling to test, to learn any nonlinear input-output relationship of any analog and radio frequency (RF) device or circuit; thus, providing fast and accurate responses to the task that they have learned. Furthermore, employment of ML techniques in analog/RF electronic design automation (EDA) tools boosts the performance of such tools. In this paper, we summarize the recent research and present a comprehensive review on ML techniques for analog/RF circuit modeling, design, synthesis, layout, and test.},
  keywords = {Analog and radio frequency,Artificial intelligence,Artificial neural network,Deep learning,Integrated circuits,Machine learning,Optimization,Synthesis},
  file = {/home/krawczuk/Zotero/storage/EVIQXSDB/Afacan et al. - 2021 - Review Machine learning techniques in analogRF i.pdf}
}

@article{afacanReviewMachineLearning2021b,
  title = {Review: {{Machine}} Learning Techniques in Analog/{{RF}} Integrated Circuit Design, Synthesis, Layout, and Test},
  shorttitle = {Review},
  author = {Afacan, Engin and Lourenço, Nuno and Martins, Ricardo and Dündar, Günhan},
  date = {2021-03-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {77},
  pages = {113--130},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2020.11.006},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926020302947},
  urldate = {2024-02-20},
  abstract = {Rapid developments in semiconductor technology have substantially increased the computational capability of computers. As a result of this and recent developments in theory, machine learning (ML) techniques have become attractive in many new applications. This trend has also inspired researchers working on integrated circuit (IC) design and optimization. ML-based design approaches have gained importance to challenge/aid conventional design methods since they can be employed at different design levels, from modeling to test, to learn any nonlinear input-output relationship of any analog and radio frequency (RF) device or circuit; thus, providing fast and accurate responses to the task that they have learned. Furthermore, employment of ML techniques in analog/RF electronic design automation (EDA) tools boosts the performance of such tools. In this paper, we summarize the recent research and present a comprehensive review on ML techniques for analog/RF circuit modeling, design, synthesis, layout, and test.},
  keywords = {Analog and radio frequency,Artificial intelligence,Artificial neural network,Deep learning,Integrated circuits,Machine learning,Optimization,Synthesis},
  file = {/home/krawczuk/Zotero/storage/J6UP9WKN/Afacan et al. - 2021 - Review Machine learning techniques in analogRF i.pdf}
}

@inproceedings{aggarwalMultilayerGridEmbeddings1985,
  title = {Multi-Layer Grid Embeddings},
  booktitle = {26th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1985)},
  author = {Aggarwal, Alok and Klawe, Maria and Lichtentein, David and Linial, Nathan and Wigderson, Avi},
  date = {1985},
  pages = {186--196},
  publisher = {{IEEE}},
  location = {{Portland, OR, USA}},
  doi = {10.1109/SFCS.1985.37},
  url = {http://ieeexplore.ieee.org/document/4568142/},
  urldate = {2024-02-22},
  abstract = {In this paper we propose two new multi-layer grid models for VLSI layout, both of which take into account the number of contact cuts used. For the first model in which nodes "exist" only on one layer, we prove a tight area x (number of contact = cuts) 8(n2) trade-off for embedding any degree 4 n-node planar graph in two layers. For the second model in which nodes "exist" simultaneously on all layers, we prove a number of bounds on the area needed to embed graphs using no contact cuts. For example we prove that any n-node graph which is the union of two planar subgraphs can be embedded on two layers in 0(n2) area without contact cuts. This bound is tight even if more layers and an unbounded number of contact cuts are allowed. We also show that planar graphs of bounded degree can be embedded on two layers in O(n1.6) area without contact cuts.},
  eventtitle = {26th {{Annual Symposium}} on {{Foundations}} of {{Computer Science}} (Sfcs 1985)},
  isbn = {978-0-8186-0644-1},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/IKERI2Q4/Aggarwal et al. - 1985 - Multi-layer grid embeddings.pdf}
}

@article{aggarwalMultilayerGridEmbeddings1991,
  title = {Multilayer Grid Embeddings for {{VLSI}}},
  author = {Aggarwal, Alok and Klawe, Maria and Shor, Peter},
  date = {1991-06-01},
  journaltitle = {Algorithmica},
  shortjournal = {Algorithmica},
  volume = {6},
  number = {1},
  pages = {129--151},
  issn = {1432-0541},
  doi = {10.1007/BF01759038},
  url = {https://doi.org/10.1007/BF01759038},
  urldate = {2024-02-22},
  abstract = {In this paper we propose two new multilayer grid models for VLSI layout, both of which take into account the number of contact cuts used. For the first model in which nodes “exist” only on one layer, we prove a tight area × (number of contact cuts) = Θ(n2) tradeoff for embeddingn-node planar graphs of bounded degree in two layers. For the second model in which nodes “exist” simultaneously on all layers, we give a number of upper bounds on the area needed to embed groups using no contact cuts. We show that anyn-node graph of thickness 2 can be embedded on two layers inO(n2) area. This bound is tight even if more layers and any number of contact cuts are allowed. We also show that planar graphs of bounded degree can be embedded on two layers inO(n3/2(logn)2) area.},
  langid = {english},
  keywords = {Embedding,Grid,Planar graph,Thickness,VLSI},
  file = {/home/krawczuk/Zotero/storage/Q86EPHLT/Aggarwal et al. - 1991 - Multilayer grid embeddings for VLSI.pdf}
}

@article{agnesinaGeneralFrameworkVLSIa,
  title = {A {{General Framework For VLSI Tool Parameter Optimization}} with {{Deep Reinforcement Learning}}},
  author = {Agnesina, Anthony and Pentapati, Sai and Lim, Sung Kyu},
  abstract = {Electronic design automation (EDA) tools and flows have steadily increased in complexity over the years, with modern tools offering more than 10,000 parameter settings, rendering the optimum tuning of such tools possible for only expert users. Automating this parameter setting for power-performance-area optimization would democratize modern EDA tools and VLSI physical design. In this paper, we present a general way of casting the parameter optimization problem into a reinforcement learning task. The resulting agent is then assigned to optimize a 2D VLSI placement step with proof-of-concept results. We conclude with a discussion of our ongoing work and how the methodology can be applied to 3D partitioning.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/ZCD2I2M7/Agnesina et al. - A General Framework For VLSI Tool Parameter Optimi.pdf}
}

@article{albeattieBetterSPQRtreeDecomposition2024,
  title = {A Better {{SPQR-tree}} Decomposition of Electrical Circuits Containing Multiports and Its Application to Wave Digital Emulation},
  author = {Al Beattie, Bakr and Ochs, Karlheinz},
  date = {2024},
  journaltitle = {International Journal of Circuit Theory and Applications},
  volume = {52},
  number = {2},
  pages = {536--550},
  issn = {1097-007X},
  doi = {10.1002/cta.3781},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cta.3781},
  urldate = {2024-02-20},
  abstract = {We report on a systematic method for the port-wise decomposition of electrical networks. Our method exploits the SPQR-tree decomposition as an algorithmic way for decomposing a given circuit into a maximal set of one-port and multiports. We provide a solution for representing multiports by suitable replacement graphs, such that they are encapsulated not decomposed or processed by the SPQR-algorithm. Contrary to current methods, our replacement graphs are not sophisticated but rather simple, which, in general, enhances the performance of the algorithm, when it comes to decomposing large electrical networks containing multiports. Some electrical devices, such as transistors or op-amps, are commonly described in terms of their terminals rather than their ports. Thus, we cover the application of the template-based approach to three-terminal devices. Lastly, we make use of the new decomposition method for the emulation of a wave digital filter.},
  langid = {english},
  keywords = {graph theory,port-wise decomposition,SPQR-tree,wave digital structures},
  file = {/home/krawczuk/Zotero/storage/KD6YHJJQ/Al Beattie and Ochs - 2024 - A better SPQR-tree decomposition of electrical cir.pdf}
}

@online{alemohammadSelfConsumingGenerativeModels2023,
  title = {Self-{{Consuming Generative Models Go MAD}}},
  author = {Alemohammad, Sina and Casco-Rodriguez, Josue and Luzi, Lorenzo and Humayun, Ahmed Imtiaz and Babaei, Hossein and LeJeune, Daniel and Siahkoohi, Ali and Baraniuk, Richard G.},
  date = {2023-07-04},
  eprint = {2307.01850},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2307.01850},
  url = {http://arxiv.org/abs/2307.01850},
  urldate = {2024-02-20},
  abstract = {Seismic advances in generative AI algorithms for imagery, text, and other data types has led to the temptation to use synthetic data to train next-generation models. Repeating this process creates an autophagous (self-consuming) loop whose properties are poorly understood. We conduct a thorough analytical and empirical analysis using state-of-the-art generative image models of three families of autophagous loops that differ in how fixed or fresh real training data is available through the generations of training and in whether the samples from previous generation models have been biased to trade off data quality versus diversity. Our primary conclusion across all scenarios is that without enough fresh real data in each generation of an autophagous loop, future generative models are doomed to have their quality (precision) or diversity (recall) progressively decrease. We term this condition Model Autophagy Disorder (MAD), making analogy to mad cow disease.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/J7UZ2BNV/Alemohammad et al. - 2023 - Self-Consuming Generative Models Go MAD.pdf;/home/krawczuk/Zotero/storage/4BPQEZH6/2307.html}
}

@online{AlgorithmsTopologySynthesis,
  title = {Algorithms for Topology Synthesis of Analog Circuits - {{ProQuest}}},
  url = {https://www.proquest.com/openview/633d818e7e92de309b764391a0186637/1?pq-origsite=gscholar&cbl=18750},
  urldate = {2024-02-20},
  abstract = {Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/LSIK3EHX/1.html}
}

@online{AlgorithmsVLSICircuit,
  title = {Algorithms for {{VLSI}} Circuit Optimization and {{GPU-based}} Parallelization - {{ProQuest}}},
  url = {https://www.proquest.com/docview/739030016?pq-origsite=gscholar&fromopenview=true&sourcetype=Dissertations%20&%20Theses},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/ZMXGX3WM/739030016.html}
}

@article{alvianoAnytimeAnswerSet2016,
  title = {Anytime Answer Set Optimization via Unsatisfiable Core Shrinking},
  author = {Alviano, Mario and Dodaro, Carmine},
  date = {2016-09},
  journaltitle = {Theory and Practice of Logic Programming},
  volume = {16},
  number = {5-6},
  pages = {533--551},
  issn = {1471-0684, 1475-3081},
  doi = {10.1017/S147106841600020X},
  url = {https://www.cambridge.org/core/journals/theory-and-practice-of-logic-programming/article/abs/anytime-answer-set-optimization-via-unsatisfiable-core-shrinking/55F4305D2BAAD203E8177F3955C9DEEA},
  urldate = {2024-02-20},
  abstract = {Unsatisfiable core analysis can boost the computation of optimum stable models for logic programs with weak constraints. However, current solvers employing unsatisfiable core analysis either run to completion, or provide no suboptimal stable models but the one resulting from the preliminary disjoint cores analysis. This drawback is circumvented here by introducing a progression based shrinking of the analyzed unsatisfiable cores. In fact, suboptimal stable models are possibly found while shrinking unsatisfiable cores, hence resulting into an anytime algorithm. Moreover, as confirmed empirically, unsatisfiable core analysis also benefits from the shrinking process in terms of solved instances.},
  langid = {english},
  keywords = {answer set programming,unsatisfiable cores,weak constraints},
  file = {/home/krawczuk/Zotero/storage/9V877HW5/Alviano and Dodaro - 2016 - Anytime answer set optimization via unsatisfiable .pdf}
}

@online{AnalogCircuitSynthesis,
  title = {Analog Circuit Synthesis by Superimposing of Sub-Circuits | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/922076?casa_token=VQJfMEDi324AAAAA:fsCB-LbGJT0PZzx36ih_aGXXzfb4pX0_MIju9rpAyxQGBxJee3_nw5bP38DD7bPSJCTza_PvukFJ},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/PPQ2LP28/922076.html}
}

@online{AnalogIntegratedCircuitc,
  title = {Analog {{Integrated Circuit Topology Synthesis With Deep Reinforcement Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9718525?casa_token=dVivchwUEd0AAAAA:wVIv3ieX3FMg40vdPpBqhIj2Ak4TY6zQiwvDmX-Tjk52V40x9NueALgwluqFmmA3IbT3JT-vk1HW},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/X7QUN29V/9718525.html}
}

@online{AnalogIntegratedCircuitd,
  title = {Analog {{Integrated Circuit Topology Synthesis With Deep Reinforcement Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9718525?casa_token=HRVQ9BNU-cMAAAAA:UrmxLS-X4kLj-UTwjoqpwWbbu0-dJoGANMFbAuRtBrg5WmGceBA8a0FziiNJTgoHYJelutqhIbeU},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/CD2JI95B/9718525.html}
}

@online{AnalogMixedSignalCircuit,
  title = {Analog/{{Mixed-Signal Circuit Synthesis Enabled}} by the {{Advancements}} of {{Circuit Architectures}} and {{Machine Learning Algorithms}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9712577?casa_token=i6LpwT0sCxcAAAAA:sPuySgP4jlPu4cfSF2ywRxIwxFN8LgoZp22RSavexDOR5fin8Eh8ivfnkJcL80wluwk3kNzkcYmt},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/PRAEZ6RQ/9712577.html}
}

@book{aptNewPerspectivesGames2008,
  title = {New Perspectives on Games and Interaction},
  editor = {Apt, Krzysztof R. and Van Rooij, Robert},
  date = {2008},
  series = {Texts in Logic and Games},
  number = {4},
  publisher = {{Amsterdam University Press}},
  location = {{Amsterdam}},
  isbn = {978-90-8964-057-4},
  langid = {english},
  pagetotal = {328},
  file = {/home/krawczuk/Zotero/storage/229PSJ6U/Apt and Van Rooij - 2008 - New perspectives on games and interaction.pdf}
}

@inproceedings{austinStructuredDenoisingDiffusion2021e,
  title = {Structured {{Denoising Diffusion Models}} in {{Discrete State-Spaces}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Austin, Jacob and Johnson, Daniel D. and Ho, Jonathan and Tarlow, Daniel and family=Berg, given=Rianne, prefix=van den, useprefix=true},
  date = {2021},
  volume = {34},
  pages = {17981--17993},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/958c530554f78bcd8e97125b70e6973d-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Denoising diffusion probabilistic models (DDPMs) [Ho et al. 2021] have shown impressive results on image and waveform generation in continuous state spaces. Here, we introduce Discrete Denoising Diffusion Probabilistic Models (D3PMs), diffusion-like generative models for discrete data that generalize the multinomial diffusion model of Hoogeboom et al. [2021], by going beyond corruption processes with uniform transition probabilities. This includes corruption with transition matrices that mimic Gaussian kernels in continuous space, matrices based on nearest neighbors in embedding space, and matrices that introduce absorbing states. The third allows us to draw a connection between diffusion models and autoregressive and mask-based generative models. We show that the choice of transition matrix is an important design decision that leads to improved results in image and text domains. We also introduce a new loss function that combines the variational lower bound with an auxiliary cross entropy loss.  For text, this model class achieves strong results on character-level text generation while scaling to large vocabularies on LM1B. On the image dataset CIFAR-10, our models approach the sample quality and exceed the log-likelihood of the continuous-space DDPM model.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/VPZZMZW9/Austin et al. - 2021 - Structured Denoising Diffusion Models in Discrete .pdf}
}

@online{AutomatedDesignAnalog,
  title = {Automated {{Design}} of {{Analog Circuits Using Reinforcement Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9576505?casa_token=3fLtOocO4noAAAAA:bNHmff4veiE13woBo4vZGsl3Pfs4OifW9zp7ecaMGJWq9vM3FRjCEGBbsLL2FrFuNhcD2mRH2gCc},
  urldate = {2024-02-20}
}

@online{AutomatedTopologySynthesisa,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939?casa_token=R5QoZc365Q8AAAAA:owOf8EzUqPGYmwhhu9gwcPgTm3LWL_W40JAyhtml33m6YrV12b6YU7s_4tVzCwP0NaVir_U6a0I},
  urldate = {2024-02-24},
  file = {/home/krawczuk/Zotero/storage/SFDIINX4/9022939.html}
}

@online{AutomaticGenerationSynthetic,
  title = {Automatic Generation of Synthetic Sequential Benchmark Circuits | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/1020350?casa_token=_xZfijPDv2kAAAAA:AzZcTFlVzzkSVUCb-P77q6Jg_VA3leMyffgFkCSWXmLugdY5bzZ1soIq4-9C-AGvLCwBl2XVajPl},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/IG65UUQG/1020350.html}
}

@online{AutomaticOpAmpGeneration,
  title = {Automatic {{Op-Amp Generation From Specification}} to {{Layout}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10185586?casa_token=m4V1oATbeB8AAAAA:NiuECNoipkEm_0-KAZPn2Iye-EZOOhwO5X4Ox-ExnKAraEfSN6yzplpv1ZsB_b6aTV-p-X674ghJ},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/A44ZQAVN/10185586.html}
}

@inproceedings{balcilarBreakingLimitsMessage2021b,
  title = {Breaking the {{Limits}} of {{Message Passing Graph Neural Networks}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Balcilar, Muhammet and Heroux, Pierre and Gauzere, Benoit and Vasseur, Pascal and Adam, Sebastien and Honeine, Paul},
  date = {2021-07-01},
  pages = {599--608},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/balcilar21a.html},
  urldate = {2024-02-24},
  abstract = {Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in O(n3)O(n3)\textbackslash mathcal\{O\}(n\^{}3) with memory usage in O(n2)O(n2)\textbackslash mathcal\{O\}(n\^{}2), consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/IC2ZGHKK/Balcilar et al. - 2021 - Breaking the Limits of Message Passing Graph Neura.pdf;/home/krawczuk/Zotero/storage/NNPNZ27K/Balcilar et al. - 2021 - Breaking the Limits of Message Passing Graph Neura.pdf}
}

@online{balcilarBreakingLimitsMessage2021c,
  title = {Breaking the {{Limits}} of {{Message Passing Graph Neural Networks}}},
  author = {Balcilar, Muhammet and Héroux, Pierre and Gaüzère, Benoit and Vasseur, Pascal and Adam, Sébastien and Honeine, Paul},
  date = {2021-06-08},
  eprint = {2106.04319},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2106.04319},
  url = {http://arxiv.org/abs/2106.04319},
  urldate = {2024-02-24},
  abstract = {Since the Message Passing (Graph) Neural Networks (MPNNs) have a linear complexity with respect to the number of nodes when applied to sparse graphs, they have been widely implemented and still raise a lot of interest even though their theoretical expressive power is limited to the first order Weisfeiler-Lehman test (1-WL). In this paper, we show that if the graph convolution supports are designed in spectral-domain by a non-linear custom function of eigenvalues and masked with an arbitrary large receptive field, the MPNN is theoretically more powerful than the 1-WL test and experimentally as powerful as a 3-WL existing models, while remaining spatially localized. Moreover, by designing custom filter functions, outputs can have various frequency components that allow the convolution process to learn different relationships between a given input graph signal and its associated properties. So far, the best 3-WL equivalent graph neural networks have a computational complexity in \$\textbackslash mathcal\{O\}(n\^{}3)\$ with memory usage in \$\textbackslash mathcal\{O\}(n\^{}2)\$, consider non-local update mechanism and do not provide the spectral richness of output profile. The proposed method overcomes all these aforementioned problems and reaches state-of-the-art results in many downstream tasks.},
  pubstate = {preprint},
  keywords = {68T07,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.5.0},
  file = {/home/krawczuk/Zotero/storage/K44HVA6W/Balcilar et al. - 2021 - Breaking the Limits of Message Passing Graph Neura.pdf;/home/krawczuk/Zotero/storage/Z2KY3MN2/2106.html}
}

@article{basuEfficientEquivariantTransfer2024,
  title = {Efficient {{Equivariant Transfer Learning}} from {{Pretrained Models}}},
  author = {Basu, Sourya and Katdare, Pulkit and Sattigeri, Prasanna and Chenthamarakshan, Vijil and Driggs-Campbell, Katherine and Das, Payel and Varshney, Lav R.},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0d02892a0055c94584f6394f8d069c8e-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/CWM84MRY/Basu et al. - 2024 - Efficient Equivariant Transfer Learning from Pretr.pdf}
}

@online{battagliaRelationalInductiveBiases2018k,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and Sanchez-Gonzalez, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  date = {2018-10-17},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1806.01261},
  url = {http://arxiv.org/abs/1806.01261},
  urldate = {2024-02-24},
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences--a hallmark of human intelligence from infancy--remains a formidable challenge for modern AI. The following is part position paper, part review, and part unification. We argue that combinatorial generalization must be a top priority for AI to achieve human-like abilities, and that structured representations and computations are key to realizing this objective. Just as biology uses nature and nurture cooperatively, we reject the false choice between "hand-engineering" and "end-to-end" learning, and instead advocate for an approach which benefits from their complementary strengths. We explore how using relational inductive biases within deep learning architectures can facilitate learning about entities, relations, and rules for composing them. We present a new building block for the AI toolkit with a strong relational inductive bias--the graph network--which generalizes and extends various approaches for neural networks that operate on graphs, and provides a straightforward interface for manipulating structured knowledge and producing structured behaviors. We discuss how graph networks can support relational reasoning and combinatorial generalization, laying the foundation for more sophisticated, interpretable, and flexible patterns of reasoning. As a companion to this paper, we have released an open-source software library for building graph networks, with demonstrations of how to use them in practice.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/3QVCGNQI/Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf;/home/krawczuk/Zotero/storage/HVRS9Z3A/1806.html}
}

@online{BayesianOptimizationApproach,
  title = {Bayesian {{Optimization Approach}} for {{Analog Circuit Synthesis Using Neural Network}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/8714788},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/QL6XW7YM/8714788.html}
}

@online{BayesianOptimizationApproacha,
  title = {Bayesian {{Optimization Approach}} for {{RF Circuit Synthesis}} via {{Multitask Neural Network Enhanced Gaussian Process}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9852008?casa_token=-1QQmC7K-EcAAAAA:RV2ho8BpTRzIUNbsA27gCdQGYOXPyOBJBA-ZCvhZHQyDFRn5fVYAX_Fijuc_OrMU5eS1Z2IuXemw},
  urldate = {2024-02-20}
}

@online{belcakNeuralCombinatorialLogic2022a,
  title = {Neural {{Combinatorial Logic Circuit Synthesis}} from {{Input-Output Examples}}},
  author = {Belcak, Peter and Wattenhofer, Roger},
  date = {2022-10-29},
  eprint = {2210.16606},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.16606},
  url = {http://arxiv.org/abs/2210.16606},
  urldate = {2024-02-24},
  abstract = {We propose a novel, fully explainable neural approach to synthesis of combinatorial logic circuits from input-output examples. The carrying advantage of our method is that it readily extends to inductive scenarios, where the set of examples is incomplete but still indicative of the desired behaviour. Our method can be employed for a virtually arbitrary choice of atoms - from logic gates to FPGA blocks - as long as they can be formulated in a differentiable fashion, and consistently yields good results for synthesis of practical circuits of increasing size. In particular, we succeed in learning a number of arithmetic, bitwise, and signal-routing operations, and even generalise towards the correct behaviour in inductive scenarios. Our method, attacking a discrete logical synthesis problem with an explainable neural approach, hints at a wider promise for synthesis and reasoning-related tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  file = {/home/krawczuk/Zotero/storage/XADZ3TZX/Belcak and Wattenhofer - 2022 - Neural Combinatorial Logic Circuit Synthesis from .pdf;/home/krawczuk/Zotero/storage/B9YGIRBV/2210.html}
}

@inproceedings{bhanjaGraphBasedSynthesis2017b,
  title = {A Graph Based Synthesis Procedure for Linear Analog Function},
  booktitle = {2017 30th {{IEEE International System-on-Chip Conference}} ({{SOCC}})},
  author = {Bhanja, Mousumi and Ray, Baidyanath},
  date = {2017-09},
  pages = {328--333},
  publisher = {{IEEE}},
  location = {{Munich}},
  doi = {10.1109/SOCC.2017.8226071},
  url = {http://ieeexplore.ieee.org/document/8226071/},
  urldate = {2024-02-20},
  abstract = {This paper presents a graph based synthesis procedure for reconfigurable linear analog function. A n-level weighted binary tree structure has been used to represent nth order linear network. Root of the binary tree has two children nodes with weights of first order lowpass filter (LPF) and first order highpass filter (HPF). Traversing through each possible path in the tree implements one filter type. The level 2 binary tree has been transformed to a hexagonal closed graph. This conversion has been done to map the proposed synthesis procedure into field programmable analog array (FPAA), which demonstrates the reusuability and programmability. First order LPF and HPF has been used as basic building blocks, whereas a hexagonal structure is denoted as a configurable analog block (CAB) of the FPAA. The hexagonal topology of the FPAA gives the versatile connectivity between two adjacent CABS of the FPAA. Performance has been verified through SPICE simulations.},
  eventtitle = {2017 30th {{IEEE International System-on-Chip Conference}} ({{SOCC}})},
  isbn = {978-1-5386-4034-0},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/449LB5KI/Bhanja and Ray - 2017 - A graph based synthesis procedure for linear analo.pdf}
}

@inproceedings{bhanjaGraphBasedSystematic2023,
  title = {Graph {{Based Systematic Synthesis Procedure}} of Gm-{{C Filter}}},
  booktitle = {2023 7th {{International Conference}} on {{Electronics}}, {{Communication}} and {{Aerospace Technology}} ({{ICECA}})},
  author = {Bhanja, Mousumi and Ganguly, Anirban and Parija, Smita Rani},
  date = {2023},
  pages = {407--412},
  publisher = {{IEEE}},
  doi = {10.1109/ICECA58529.2023.10395319},
  url = {https://ieeexplore.ieee.org/abstract/document/10395319/?casa_token=Ci_nxVLq0woAAAAA:IbOCUVI_rDxU78Kk4PN08JF2dFYyHh-tXY5FyipkaE5K4tE4pYpUx2N6ibwQGMF3FbLxAEjt_Uzq},
  urldate = {2024-02-19}
}

@article{bianchiExpressivePowerPooling2023a,
  title = {The Expressive Power of Pooling in {{Graph Neural Networks}}},
  author = {Bianchi, Filippo Maria and Lachi, Veronica},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/e26f31de8b13ec569bf507e6ae2cd952-Abstract-Conference.html},
  urldate = {2024-02-24},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/89T6775T/Bianchi and Lachi - 2023 - The expressive power of pooling in Graph Neural Ne.pdf}
}

@inproceedings{biedlPointsetEmbeddabilityProblem2012,
  title = {The Point-Set Embeddability Problem for Plane Graphs},
  booktitle = {Proceedings of the Twenty-Eighth Annual Symposium on {{Computational}} Geometry},
  author = {Biedl, Therese and Vatshelle, Martin},
  date = {2012-06-17},
  series = {{{SoCG}} '12},
  pages = {41--50},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2261250.2261257},
  url = {https://dl.acm.org/doi/10.1145/2261250.2261257},
  urldate = {2024-02-22},
  abstract = {In this paper, we study the point-set-embeddability-problem, i.e., given a planar graph and a set of points, is there a mapping of the vertices to the points such that the resulting straight-line drawing is planar? It was known that this problem is NP-hard if the embedding can be chosen, but becomes polynomial for triangulated graphs of treewidth 3. We show here that in fact it can be answered for all planar graphs with a fixed combinatorial embedding that have constant treewidth and constant face-degree. We also prove that as soon as one of the conditions is dropped (i.e., either the treewidth is unbounded or some faces have large degrees), point-set-embeddability with a fixed embedding becomes NP-hard. The NP-hardness holds even for a 3-connected planar graph with constant treewidth, triangulated planar graphs, or 2-connected outer-planar graphs.},
  isbn = {978-1-4503-1299-8},
  keywords = {carving width,graph drawing,point-set embedding},
  file = {/home/krawczuk/Zotero/storage/YB9HI3YN/Biedl and Vatshelle - 2012 - The point-set embeddability problem for plane grap.pdf}
}

@inproceedings{biedlPointsetEmbeddabilityProblem2012a,
  title = {The Point-Set Embeddability Problem for Plane Graphs},
  booktitle = {Proceedings of the Twenty-Eighth Annual Symposium on {{Computational}} Geometry},
  author = {Biedl, Therese and Vatshelle, Martin},
  date = {2012-06-17},
  series = {{{SoCG}} '12},
  pages = {41--50},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2261250.2261257},
  url = {https://dl.acm.org/doi/10.1145/2261250.2261257},
  urldate = {2024-02-22},
  abstract = {In this paper, we study the point-set-embeddability-problem, i.e., given a planar graph and a set of points, is there a mapping of the vertices to the points such that the resulting straight-line drawing is planar? It was known that this problem is NP-hard if the embedding can be chosen, but becomes polynomial for triangulated graphs of treewidth 3. We show here that in fact it can be answered for all planar graphs with a fixed combinatorial embedding that have constant treewidth and constant face-degree. We also prove that as soon as one of the conditions is dropped (i.e., either the treewidth is unbounded or some faces have large degrees), point-set-embeddability with a fixed embedding becomes NP-hard. The NP-hardness holds even for a 3-connected planar graph with constant treewidth, triangulated planar graphs, or 2-connected outer-planar graphs.},
  isbn = {978-1-4503-1299-8},
  keywords = {carving width,graph drawing,point-set embedding},
  file = {/home/krawczuk/Zotero/storage/GY5KXWMI/Biedl and Vatshelle - 2012 - The point-set embeddability problem for plane grap.pdf}
}

@article{bokerFinegrainedExpressivityGraph2023,
  title = {Fine-Grained {{Expressivity}} of {{Graph Neural Networks}}},
  author = {Böker, Jan and Levie, Ron and Huang, Ningyuan and Villar, Soledad and Morris, Christopher},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9200d97ca2bf3a26db7b591844014f00-Abstract-Conference.html},
  urldate = {2024-02-24},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/MHZYYDT7/Böker et al. - 2023 - Fine-grained Expressivity of Graph Neural Networks.pdf}
}

@article{bollaSpectralPropertiesModularity2015a,
  title = {Spectral Properties of Modularity Matrices},
  author = {Bolla, Marianna and Bullins, Brian and Chaturapruek, Sorathan and Chen, Shiwen and Friedl, Katalin},
  date = {2015-05},
  journaltitle = {Linear Algebra and its Applications},
  shortjournal = {Linear Algebra and its Applications},
  volume = {473},
  pages = {359--376},
  issn = {00243795},
  doi = {10.1016/j.laa.2014.10.039},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0024379514007186},
  urldate = {2024-02-22},
  abstract = {There is an exact relation between the spectra of modularity matrices introduced in social network analysis and the χ2 statistic. We investigate a weighted graph with the main interest being when the hypothesis of independent attachment of the vertices is rejected, and we look for clusters of vertices with higher inter-cluster relations than expected under the hypothesis of independence. In this context, we give a sufficient condition for a weighted, and a sufficient and necessary condition for an unweighted graph to have at least one positive eigenvalue in its modularity or normalized modularity spectrum, which guarantees a community structure with more than one cluster. This property has important implications for the isoperimetric inequality, the symmetric maximal correlation, and the Newman–Girvan modularity.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/R8LT2LCT/Bolla et al. - 2015 - Spectral properties of modularity matrices.pdf}
}

@unpublished{bommasaniGeneralizedOptimalLinear2020,
  title = {Generalized {{Optimal Linear Orders}}},
  author = {Bommasani, Rishi},
  date = {2020},
  eprint = {2108.10692},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.7298/5x0j-me63},
  url = {http://arxiv.org/abs/2108.10692},
  urldate = {2024-02-22},
  abstract = {The sequential structure of language, and the order of words in a sentence specifically, plays a central role in human language processing. Consequently, in designing computational models of language, the de facto approach is to present sentences to machines with the words ordered in the same order as in the original human-authored sentence. The very essence of this work is to question the implicit assumption that this is desirable and inject theoretical soundness into the consideration of word order in natural language processing. In this thesis, we begin by uniting the disparate treatments of word order in cognitive science, psycholinguistics, computational linguistics, and natural language processing under a flexible algorithmic framework. We proceed to use this heterogeneous theoretical foundation as the basis for exploring new word orders with an undercurrent of psycholinguistic optimality. In particular, we focus on notions of dependency length minimization given the difficulties in human and computational language processing in handling long-distance dependencies. We then discuss algorithms for finding optimal word orders efficiently in spite of the combinatorial space of possibilities. We conclude by addressing the implications of these word orders on human language and their downstream impacts when integrated in computational models.},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/Z7Q8JXYX/Bommasani - 2020 - Generalized Optimal Linear Orders.pdf;/home/krawczuk/Zotero/storage/D9M28Z5W/2108.html}
}

@article{bond-taylorDeepGenerativeModelling2022,
  title = {Deep {{Generative Modelling}}: {{A Comparative Review}} of {{VAEs}}, {{GANs}}, {{Normalizing Flows}}, {{Energy-Based}} and {{Autoregressive Models}}},
  shorttitle = {Deep {{Generative Modelling}}},
  author = {Bond-Taylor, Sam and Leach, Adam and Long, Yang and Willcocks, Chris G.},
  date = {2022-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {44},
  number = {11},
  pages = {7327--7347},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2021.3116668},
  url = {https://ieeexplore.ieee.org/abstract/document/9555209},
  urldate = {2024-02-22},
  abstract = {Deep generative models are a class of techniques that train deep neural networks to model the distribution of training samples. Research has fragmented into various interconnected approaches, each of which make trade-offs including run-time, diversity, and architectural restrictions. In particular, this compendium covers energy-based models, variational autoencoders, generative adversarial networks, autoregressive models, normalizing flows, in addition to numerous hybrid approaches. These techniques are compared and contrasted, explaining the premises behind each and how they are interrelated, while reviewing current state-of-the-art advances and implementations.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Analytical models,autoregressive models,Computational modeling,Data models,Deep learning,energy-based models,generative adversarial networks,Generative adversarial networks,generative models,Neurons,normalizing flows,Predictive models,Training,variational autoencoders},
  file = {/home/krawczuk/Zotero/storage/TC44WUWB/Bond-Taylor et al. - 2022 - Deep Generative Modelling A Comparative Review of.pdf;/home/krawczuk/Zotero/storage/TZWGDWKL/9555209.html}
}

@online{bousquetOptimalTransportGenerative2017a,
  title = {From Optimal Transport to Generative Modeling: The {{VEGAN}} Cookbook},
  shorttitle = {From Optimal Transport to Generative Modeling},
  author = {Bousquet, Olivier and Gelly, Sylvain and Tolstikhin, Ilya and Simon-Gabriel, Carl-Johann and Schoelkopf, Bernhard},
  date = {2017-05-22},
  eprint = {1705.07642},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1705.07642},
  url = {http://arxiv.org/abs/1705.07642},
  urldate = {2024-02-20},
  abstract = {We study unsupervised generative modeling in terms of the optimal transport (OT) problem between true (but unknown) data distribution \$P\_X\$ and the latent variable model distribution \$P\_G\$. We show that the OT problem can be equivalently written in terms of probabilistic encoders, which are constrained to match the posterior and prior distributions over the latent space. When relaxed, this constrained optimization problem leads to a penalized optimal transport (POT) objective, which can be efficiently minimized using stochastic gradient descent by sampling from \$P\_X\$ and \$P\_G\$. We show that POT for the 2-Wasserstein distance coincides with the objective heuristically employed in adversarial auto-encoders (AAE) (Makhzani et al., 2016), which provides the first theoretical justification for AAEs known to the authors. We also compare POT to other popular techniques like variational auto-encoders (VAE) (Kingma and Welling, 2014). Our theoretical results include (a) a better understanding of the commonly observed blurriness of images generated by VAEs, and (b) establishing duality between Wasserstein GAN (Arjovsky and Bottou, 2017) and POT for the 1-Wasserstein distance.},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/ZXVBVCJ2/Bousquet et al. - 2017 - From optimal transport to generative modeling the.pdf;/home/krawczuk/Zotero/storage/GDK9IQDU/1705.html}
}

@online{brucknerSPQRTreeLikeEmbeddingRepresentation2019,
  title = {An {{SPQR-Tree-Like Embedding Representation}} for {{Upward Planarity}}},
  author = {Brückner, Guido and Himmel, Markus and Rutter, Ignaz},
  date = {2019-08-01},
  eprint = {1908.00352},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1908.00352},
  url = {http://arxiv.org/abs/1908.00352},
  urldate = {2024-02-20},
  abstract = {The SPQR-tree is a data structure that compactly represents all planar embeddings of a biconnected planar graph. It plays a key role in constrained planarity testing. We develop a similar data structure, called the UP-tree, that compactly represents all upward planar embeddings of a biconnected single-source directed graph. We demonstrate the usefulness of the UP-tree by solving the upward planar embedding extension problem for biconnected single-source directed graphs.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/krawczuk/Zotero/storage/82ZLH3BK/Brückner et al. - 2019 - An SPQR-Tree-Like Embedding Representation for Upw.pdf;/home/krawczuk/Zotero/storage/6Z42IVTC/1908.html}
}

@online{caiLatentGraphDiffusion2024a,
  title = {Latent {{Graph Diffusion}}: {{A Unified Framework}} for {{Generation}} and {{Prediction}} on {{Graphs}}},
  shorttitle = {Latent {{Graph Diffusion}}},
  author = {Cai, Zhou and Wang, Xiyuan and Zhang, Muhan},
  date = {2024-02-04},
  eprint = {2402.02518},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2402.02518},
  url = {http://arxiv.org/abs/2402.02518},
  urldate = {2024-02-20},
  abstract = {In this paper, we propose the first framework that enables solving graph learning tasks of all levels (node, edge and graph) and all types (generation, regression and classification) with one model. We first propose Latent Graph Diffusion (LGD), a generative model that can generate node, edge, and graph-level features of all categories simultaneously. We achieve this goal by embedding the graph structures and features into a latent space leveraging a powerful encoder which can also be decoded, then training a diffusion model in the latent space. LGD is also capable of conditional generation through a specifically designed cross-attention mechanism. Then we formulate prediction tasks including regression and classification as (conditional) generation, which enables our LGD to solve tasks of all levels and all types with provable guarantees. We verify the effectiveness of our framework with extensive experiments, where our models achieve state-of-the-art or highly competitive results across generation and regression tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/LHD27RHX/Cai et al. - 2024 - Latent Graph Diffusion A Unified Framework for Ge.pdf;/home/krawczuk/Zotero/storage/TEG7YCQA/2402.html}
}

@inproceedings{campilho-gomesAutomaticFlatLevelCircuit2020,
  title = {Automatic {{Flat-Level Circuit Generation}} with {{Genetic Algorithms}}},
  booktitle = {Technological {{Innovation}} for {{Life Improvement}}},
  author = {Campilho-Gomes, Miguel and Tavares, Rui and Goes, João},
  editor = {Camarinha-Matos, Luis M. and Farhadi, Nastaran and Lopes, Fábio and Pereira, Helena},
  date = {2020},
  series = {{{IFIP Advances}} in {{Information}} and {{Communication Technology}}},
  pages = {101--108},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-45124-0_9},
  abstract = {This paper describes a novel methodology to generate analog and digital circuits, autonomously, using the transistor (or other elementary device, e.g. resistor) as the basic elementary block – flat-level. A genetic algorithm is employed as the generation engine and variable length chromosomes are used to describe the circuit topology that evolves during the search. The circuit devices type and sizing are described by each gene of genetic algorithm. The automatic process starts with the circuit input and output specifications, and proceeds with the circuit topology and sizing evolution to meet those specifications, eventually, ending up with a novel topology. During the evolution, each generated circuit is electrically evaluated by a spice-like circuit simulator, i.e. Ngspice, using full model specifications - like BSIM3 for transistors - in a highly parallelized architecture built over a multi-thread model.},
  isbn = {978-3-030-45124-0},
  langid = {english},
  keywords = {Amplifier,Analog circuit,Automatic topology generation,Digital circuit,Genetic algorithm,Ngspice,Variable Length Chromosome},
  file = {/home/krawczuk/Zotero/storage/ED65I72T/Campilho-Gomes et al. - 2020 - Automatic Flat-Level Circuit Generation with Genet.pdf}
}

@inproceedings{chenAutoCRAFTLayoutAutomation2022,
  title = {{{AutoCRAFT}}: {{Layout Automation}} for {{Custom Circuits}} in {{Advanced FinFET Technologies}}},
  shorttitle = {{{AutoCRAFT}}},
  booktitle = {Proceedings of the 2022 {{International Symposium}} on {{Physical Design}}},
  author = {Chen, Hao and Turner, Walker J. and Song, Sanquan and Zhu, Keren and Kokai, George F. and Zimmer, Brian and Gray, C. Thomas and Khailany, Brucek and Pan, David Z. and Ren, Haoxing},
  date = {2022-04-13},
  series = {{{ISPD}} '22},
  pages = {175--183},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3505170.3511044},
  url = {https://dl.acm.org/doi/10.1145/3505170.3511044},
  urldate = {2024-02-19},
  abstract = {Despite continuous efforts in layout automation for full-custom circuits, including analog/mixed-signal (AMS) designs, automated layout tools have not yet been widely adopted in current industrial full-custom design flows due to the high circuit complexity and sensitivity to layout parasitics. Nevertheless, the strict design rules and grid-based restrictions in nanometer-scale FinFET nodes limit the degree of freedom in full-custom layout design and thus reduce the gap between automation tools and human experts. This paper presents AutoCRAFT, an automatic layout generator targeting region-based layouts for advanced FinFET-based full-custom circuits. AutoCRAFT uses specialized place-and-route (P\&R) algorithms to handle various design constraints while adhering to typical FinFET layout styles. Verified by comprehensive post-layout analyses, AutoCRAFT has achieved promising preliminary results in generating sign-off quality layouts for industrial benchmarks.},
  isbn = {978-1-4503-9210-5},
  keywords = {analog/mixed-signal,finfet,full-custom layout,physical design},
  file = {/home/krawczuk/Zotero/storage/ZC4UHS3J/Chen et al. - 2022 - AutoCRAFT Layout Automation for Custom Circuits i.pdf}
}

@article{chenChallengesOpportunitiesFully2020d,
  title = {Challenges and Opportunities toward Fully Automated Analog Layout Design},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Sun, Nan and Pan, David Z.},
  date = {2020-11},
  journaltitle = {Journal of Semiconductors},
  shortjournal = {J. Semicond.},
  volume = {41},
  number = {11},
  pages = {111407},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1674-4926},
  doi = {10.1088/1674-4926/41/11/111407},
  url = {https://dx.doi.org/10.1088/1674-4926/41/11/111407},
  urldate = {2024-02-20},
  abstract = {Realizing the layouts of analog/mixed-signal (AMS) integrated circuits (ICs) is a complicated task due to the high design flexibility and sensitive circuit performance. Compared with the advancements of digital IC layout automation, analog IC layout design is still heavily manual, which leads to a more time-consuming and error-prone process. In recent years, significant progress has been made in automated analog layout design with emerging of several open-source frameworks. This paper firstly reviews the existing state-of-the art AMS layout synthesis frameworks with focus on the different approaches and their individual challenges. We then present recent research trends and opportunities in the field. Finally, we summaries the paper with open questions and future directions for fully-automating the analog IC layout.},
  langid = {english}
}

@article{chenChallengesOpportunitiesFully2020e,
  title = {Challenges and Opportunities toward Fully Automated Analog Layout Design},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Sun, Nan and Pan, David Z.},
  date = {2020-11},
  journaltitle = {Journal of Semiconductors},
  shortjournal = {J. Semicond.},
  volume = {41},
  number = {11},
  pages = {111407},
  publisher = {{Chinese Institute of Electronics}},
  issn = {1674-4926},
  doi = {10.1088/1674-4926/41/11/111407},
  url = {https://dx.doi.org/10.1088/1674-4926/41/11/111407},
  urldate = {2024-02-20},
  abstract = {Realizing the layouts of analog/mixed-signal (AMS) integrated circuits (ICs) is a complicated task due to the high design flexibility and sensitive circuit performance. Compared with the advancements of digital IC layout automation, analog IC layout design is still heavily manual, which leads to a more time-consuming and error-prone process. In recent years, significant progress has been made in automated analog layout design with emerging of several open-source frameworks. This paper firstly reviews the existing state-of-the art AMS layout synthesis frameworks with focus on the different approaches and their individual challenges. We then present recent research trends and opportunities in the field. Finally, we summaries the paper with open questions and future directions for fully-automating the analog IC layout.},
  langid = {english}
}

@article{chenChallengesOpportunitiesFully2020f,
  title = {Challenges and Opportunities toward Fully Automated Analog Layout Design},
  author = {Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Sun, Nan and Pan, David Z.},
  date = {2020-11-01},
  journaltitle = {Journal of Semiconductors},
  shortjournal = {J. Semicond.},
  volume = {41},
  number = {11},
  pages = {111407},
  issn = {1674-4926, 2058-6140},
  doi = {10.1088/1674-4926/41/11/111407},
  url = {https://iopscience.iop.org/article/10.1088/1674-4926/41/11/111407},
  urldate = {2024-02-20},
  abstract = {Realizing the layouts of analog/mixed-signal (AMS) integrated circuits (ICs) is a complicated task due to the high design flexibility and sensitive circuit performance. Compared with the advancements of digital IC layout automation, analog IC layout design is still heavily manual, which leads to a more time-consuming and error-prone process. In recent years, significant progress has been made in automated analog layout design with emerging of several open-source frameworks. This paper firstly reviews the existing state-of-the art AMS layout synthesis frameworks with focus on the different approaches and their individual challenges. We then present recent research trends and opportunities in the field. Finally, we summaries the paper with open questions and future directions for fully-automating the analog IC layout.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/U2SIPBX4/Chen et al. - 2020 - Challenges and opportunities toward fully automate.pdf}
}

@article{chenComponentsConvertersFundamental2024,
  title = {From {{Components}} to {{Converters}}: {{A Fundamental Topology Derivation Method}} for {{Nonresonant DC}}–{{DC Converters Based}} on {{Graph Theory}}},
  shorttitle = {From {{Components}} to {{Converters}}},
  author = {Chen, Guipeng and Mo, Liping and Jiang, Chaoqiang and Qing, Xinlin},
  date = {2024-01},
  journaltitle = {IEEE Transactions on Power Electronics},
  volume = {39},
  number = {1},
  pages = {1028--1045},
  issn = {1941-0107},
  doi = {10.1109/TPEL.2023.3323597},
  url = {https://ieeexplore.ieee.org/abstract/document/10278481?casa_token=g7L2SJVcFBgAAAAA:GY1iv85vEejRHF_TB3RX-m9b38k1-ND2ZPOubf1iLvWlFzr7gfTM66Oe_qXEWhRtvNnyPoeCV4XU},
  urldate = {2024-02-19},
  abstract = {The past 20 years witnessed the invention of numerous converters by utilizing various topology derivation methods. Unfortunately, most of these methods are limited by pre-existing topologies or specific cells, causing the omission of some potentially valuable topologies. To break the limitations, a fundamental topology derivation method, namely components to converters (C2C), is proposed for nonresonant dc–dc converters. The basic idea of C2C is intuitively to derive topologies by combining separate components and filtering out valid combinations. Theoretically, C2C can derive converters more comprehensively since its results are not restricted by firm connections of the existing topologies or cells. However, C2C faces a heavy computing load caused by the massive combinations of components. Hence, a two-stage C2C topology derivation strategy is designed to alleviate the computing load. Furthermore, graph theory and dynamic programming are applied to computerize and optimize the above two-stage C2C. The two-stage C2C is utilized to derive single-switch two-port converters and single-inductor multiple-port converters. The derivation results show that all existing topologies with given components and numerous new topologies are derived automatically and simultaneously. Compared with the existing topology derivation methods, the proposed two-stage C2C is more thorough and automatic, facilitating more converters to meet various demands in practical applications.},
  eventtitle = {{{IEEE Transactions}} on {{Power Electronics}}},
  keywords = {Dynamic programming,Graph theory,Heuristic algorithms,Network topology,nonresonant dc–dc converter,Switches,Topology,topology derivation,Voltage},
  file = {/home/krawczuk/Zotero/storage/TFL7P93H/Chen et al. - 2024 - From Components to Converters A Fundamental Topol.pdf;/home/krawczuk/Zotero/storage/EUZ4WZL3/10278481.html}
}

@inproceedings{chenGeneratingCellLibrary2023,
  title = {On {{Generating Cell Library}} in {{Advanced Nodes}}: {{Efforts}} and {{Challenges}}},
  shorttitle = {On {{Generating Cell Library}} in {{Advanced Nodes}}},
  booktitle = {2023 {{International VLSI Symposium}} on {{Technology}}, {{Systems}} and {{Applications}} ({{VLSI-TSA}}/{{VLSI-DAT}})},
  author = {Chen, Hung-Ming and Hsiao, Cheng-Li and Chao, Wei-Tung and Hsieh, I-Chun},
  date = {2023-04-17},
  pages = {1--4},
  publisher = {{IEEE}},
  location = {{HsinChu, Taiwan}},
  doi = {10.1109/VLSI-TSA/VLSI-DAT57221.2023.10134126},
  url = {https://ieeexplore.ieee.org/document/10134126/},
  urldate = {2024-02-22},
  eventtitle = {2023 {{International VLSI Symposium}} on {{Technology}}, {{Systems}} and {{Applications}} ({{VLSI-TSA}}/{{VLSI-DAT}})},
  isbn = {9798350334166},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/YHY9SW5F/Chen et al. - 2023 - On Generating Cell Library in Advanced Nodes Effo.pdf}
}

@article{chenMachineLearningAdvanced2022,
  title = {Machine Learning in Advanced {{IC}} Design: {{A}} Methodological Survey},
  shorttitle = {Machine Learning in Advanced {{IC}} Design},
  author = {Chen, Tinghuan and Zhang, Grace Li and Yu, Bei and Li, Bing and Schlichtmann, Ulf},
  date = {2022},
  journaltitle = {IEEE Design \& Test},
  volume = {40},
  number = {1},
  pages = {17--33},
  publisher = {{IEEE}},
  doi = {10.1109/MDAT.2022.3216799},
  url = {https://ieeexplore.ieee.org/document/9927393/?casa_token=T9748pCZBRcAAAAA:GeXAatfUFxNH7kYG1jlXmgiTv_ivV3EukpRBXgMMdlsKVjxBNs42wtCSqLdruMqOK-u8FrB4KuIm},
  urldate = {2024-02-19}
}

@inproceedings{chenTopologicalRelationalLearning2021,
  title = {Topological {{Relational Learning}} on {{Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Yuzhou and Coskunuzer, Baris and Gel, Yulia},
  date = {2021},
  volume = {34},
  pages = {27029--27042},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/e334fd9dac68f13fa1a57796148cf812-Abstract.html},
  urldate = {2024-02-20},
  abstract = {Graph neural networks (GNNs) have emerged as a powerful tool for graph classification and representation learning. However, GNNs tend to suffer from over-smoothing problems and are vulnerable to graph perturbations. To address these challenges, we propose a novel topological neural framework of topological relational inference (TRI) which allows for integrating higher-order graph information to GNNs and for systematically learning a local graph structure. The key idea is to rewire the original graph by using the persistent homology of the small neighborhoods of the nodes and then to incorporate the extracted topological summaries as the side information into the local algorithm. As a result, the new framework enables us to harness both the conventional information on the graph structure and information on higher order topological properties of the graph. We derive theoretical properties on stability of the new local topological representation of the graph and discuss its implications on the graph algebraic connectivity. The experimental results on node classification tasks demonstrate that the new TRI-GNN outperforms all 14 state-of-the-art baselines on 6 out 7 graphs and exhibit higher robustness to perturbations, yielding up to 10\textbackslash\% better performance under noisy scenarios.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/S7IN62SA/Chen et al. - 2021 - Topological Relational Learning on Graphs.pdf}
}

@inproceedings{chenTOTALTopologyOptimization2023,
  title = {{{TOTAL}}: {{Topology Optimization}} of {{Operational Amplifier}} via {{Reinforcement Learning}}},
  shorttitle = {{{TOTAL}}},
  booktitle = {2023 24th {{International Symposium}} on {{Quality Electronic Design}} ({{ISQED}})},
  author = {Chen, Zihao and Meng, Songlei and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2023},
  pages = {1--8},
  publisher = {{IEEE}},
  url = {https://ieeexplore.ieee.org/abstract/document/10129336/?casa_token=jWG_R5SWFfYAAAAA:IkVSx9R6WervPNZLg8YaLlvx4ZlZ7P1W5Xn20044DWb7EfxxaNU-BbI3MoVFl3SCurN1zezZPzjN},
  urldate = {2024-02-19},
  keywords = {❓ Multiple DOI}
}

@article{chungEmbeddingGraphsBooks1987,
  title = {Embedding {{Graphs}} in {{Books}}: {{A Layout Problem}} with {{Applications}} to {{VLSI Design}}},
  shorttitle = {Embedding {{Graphs}} in {{Books}}},
  author = {Chung, Fan R. K. and Leighton, Frank Thomson and Rosenberg, Arnold L.},
  date = {1987-01},
  journaltitle = {SIAM Journal on Algebraic Discrete Methods},
  shortjournal = {SIAM. J. on Algebraic and Discrete Methods},
  volume = {8},
  number = {1},
  pages = {33--58},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0196-5212},
  doi = {10.1137/0608002},
  url = {https://epubs.siam.org/doi/abs/10.1137/0608002},
  urldate = {2024-02-22},
  abstract = {The relative powers of queues and stacks are compared as mechanisms for laying out the edges of a graph. In a k-queue layout, vertices of the graph are placed in some linear order (also called a linear arrangement), and each edge is assigned to exactly one of the k queues, so that the edges assigned to each queue obey a first-in/first-out (FIFO) discipline. As the vertices are scanned left to right, an edge is enqueued on its assigned queue when its left endpoint is encountered and is dequeued from its queue when its right endpoint is encountered. In a k-stack layout, vertices of the graph are placed in some linear order, and each edge is assigned to exactly one of the k stacks so that the edges assigned to each stack obey a last-in/first-out discipline. As the vertices are scanned left to right, an edge is pushed on its assigned stack when its left endpoint is encountered and is popped from its stack when its right endpoint is encountered. The paper has three main results. First, a tradeoff between queuenumber and stacknumber is shown for a fixed linear order of the vertices of G. In particular, for a fixed-order layout of a graph G, \textbackslash [ \{\textbackslash text\{queuenumber \}\} \textbackslash times \{\textbackslash text\{ stacknumber \}\} \textbackslash geq \{\textbackslash text\{ cutwidth/valence \}\}( G ).\textbackslash ] Second, it is shown that every 1-queue graph has a 2-stack layout and that every 1-stack graph has a 2-queue layout. Third, in a surprising display of the power of queues, it is shown that the ternary hypercube requires exponentially more stacks than queues. More precisely, an N-vertex ternary hypercube has a \$( 2\textbackslash log \_3 N )\$-queue layout but requires \$\textbackslash Omega ( N\^{}\{1/9 - \textbackslash epsilon \}  )\$ stacks, \$\textbackslash epsilon  {$>$} 0\$, in any stack layout. Also, some asymptotic bounds for the queuenumber of bounded-valence graphs are derived.},
  file = {/home/krawczuk/Zotero/storage/TNGH5ZYJ/Chung et al. - 1987 - Embedding Graphs in Books A Layout Problem with A.pdf}
}

@article{churchApplicationRecursiveArithmetic1963,
  title = {Application of {{Recursive Arithmetic}} to the {{Problem}} of {{Circuit Synthesis}}},
  author = {Church, Alonzo},
  date = {1963},
  journaltitle = {Journal of Symbolic Logic},
  volume = {28},
  number = {4},
  pages = {289--290},
  publisher = {{Association for Symbolic Logic}},
  doi = {10.2307/2271310},
  file = {/home/krawczuk/Zotero/storage/BXU6AFEL/Church - 1963 - Application of Recursive Arithmetic to the Problem.pdf}
}

@inproceedings{daiMarginalDistributionAdaptation2022b,
  title = {Marginal {{Distribution Adaptation}} for {{Discrete Sets}} via {{Module-Oriented Divergence Minimization}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Dai, Hanjun and Yang, Mengjiao and Xue, Yuan and Schuurmans, Dale and Dai, Bo},
  date = {2022-06-28},
  pages = {4605--4617},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/dai22c.html},
  urldate = {2024-02-20},
  abstract = {Distributions over discrete sets capture the essential statistics including the high-order correlation among elements. Such information provides powerful insight for decision making across various application domains, e.g., product assortment based on product distribution in shopping carts. While deep generative models trained on pre-collected data can capture existing distributions, such pre-trained models are usually not capable of aligning with a target domain in the presence of distribution shift due to reasons such as temporal shift or the change in the population mix. We develop a general framework to adapt a generative model subject to a (possibly counterfactual) target data distribution with both sampling and computation efficiency. Concretely, instead of re-training a full model from scratch, we reuse the learned modules to preserve the correlations between set elements, while only adjusting corresponding components to align with target marginal constraints. We instantiate the approach for three commonly used forms of discrete set distribution—latent variable, autoregressive, and energy based models—and provide efficient solutions for marginal-constrained optimization in either primal or dual forms. Experiments on both synthetic and real-world e-commerce and EHR datasets show that the proposed framework is able to practically align a generative model to match marginal constraints under distribution shift.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/HNPCWBKR/Dai et al. - 2022 - Marginal Distribution Adaptation for Discrete Sets.pdf}
}

@article{dana74AttorneyAgent,
  title = {(74) {{Attorney}}, {{Agent}},or {{Firm}} - {{Dana Legal Services}};},
  author = {Dana, Jubin},
  abstract = {In accordance with various embodiments and aspects of the invention, systems and methods are disclosed that can automatically find the best legal configuration that will be optimal with respect to a given set of requirements or metrics, such as: area, timing, and power. A designer defines the metrics or requirements, which represent the functional needs. A designer typically selects a set of parameters from a group of parameters available to user, which are user selectable parameters. The best parameters, from which the user can select parameters, are identified, and provided to the user. A constraint solver module ensures all rules are enforced and finds all legal parameters that fulfil the intent. The constraint solver module generates configura tions that meet the requirements and are legal configura tions .},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/M4Z7XGSE/Dana - (74) Attorney, Agent,or Firm - Dana Legal Services.pdf}
}

@article{dasilvaAutoTGReinforcementLearningBased2023,
  title = {{{AutoTG}}: {{Reinforcement Learning-Based Symbolic Optimization}} for {{AI-Assisted Power Converter Design}}},
  shorttitle = {{{AutoTG}}},
  author = {family=Silva, given=Felipe Leno, prefix=da, useprefix=true and Glatt, Ruben and Su, Wencong and Bui, Van-Hai and Chang, Fangyuan and Chaturvedi, Shivam and Wang, Mengqi and Murphey, Yi Lu and Huang, Can and Xue, Lingxiao and Zeng, Rong},
  date = {2023},
  journaltitle = {IEEE Journal of Emerging and Selected Topics in Industrial Electronics},
  pages = {1--10},
  issn = {2687-9743},
  doi = {10.1109/JESTIE.2023.3303836},
  url = {https://ieeexplore.ieee.org/abstract/document/10215513?casa_token=96l5LfY-AMcAAAAA:euKy9S6j6PHMeezQ52jY7vQGtR6vgoBqfb1p1bCTL5wXkgsNVQAsEIrnefOyEB9bRdyxHNXWCwyJ},
  urldate = {2024-02-19},
  abstract = {Power converters are pervasive in modern electronic component design. They can be found in all electronic devices from household appliances and cellphone chargers to vehicles. Currently, designing new circuit topologies is hard because it requires human expertise based on experience and is difficult to automate. However, artificial-intelligence-assisted design can significantly facilitate the development of new power converters and/or improve the final result. Intelligently designed highly efficient power converters can have a significant effect on many important attributes, such as power efficiency, layout size, cost, heat dissemination, energy requirements, etc. We propose Autonomous Topology Generator (AutoTG), a reinforcement-learning-based framework that generates power converter topology candidates based on user specifications, optimized for user preferences. By modeling power converter design as a symbolic optimization problem, we sequentially sample components in an autoregressive manner until new topologies are formed, providing both the topology specification and the sizing (magnitude of each component parameter) of the proposed power converter. We provide an empirical evaluation and show that AutoTG is able to generate varied high-efficiency topologies within component restrictions based on user input and show that previously unknown topologies can be found for further evaluation.},
  eventtitle = {{{IEEE Journal}} of {{Emerging}} and {{Selected Topics}} in {{Industrial Electronics}}},
  keywords = {(AI)-based design,Capacitors,Integrated circuit modeling,Mathematical models,Optimization,power converter design,Reinforcement learning,symbolic optimization applications,Topology,Voltage},
  file = {/home/krawczuk/Zotero/storage/NBNV78XX/da Silva et al. - 2023 - AutoTG Reinforcement Learning-Based Symbolic Opti.pdf;/home/krawczuk/Zotero/storage/U3WILZ5I/10215513.html}
}

@article{dastidarSynthesisSystemAnalog2005,
  title = {A Synthesis System for Analog Circuits Based on Evolutionary Search and Topological Reuse},
  author = {Dastidar, T.R. and Chakrabarti, P.P. and Ray, P.},
  date = {2005-04},
  journaltitle = {IEEE Transactions on Evolutionary Computation},
  volume = {9},
  number = {2},
  pages = {211--224},
  issn = {1941-0026},
  doi = {10.1109/TEVC.2004.841308},
  url = {https://ieeexplore.ieee.org/abstract/document/1413261},
  urldate = {2024-02-20},
  abstract = {We present a method for automated synthesis of analog circuits using evolutionary search and a set of circuit design rules based on topological reuse. The system requires only moderate expert knowledge on part of the user. It allows circuit size, circuit topology, and device values to evolve. The circuit representation scheme employs a topological reuse-based approach-it uses commonly used subcircuits for analog design as inputs and utilizes these to create the final circuit. The connectivity between these blocks is governed by a well-defined set of rules and the scheme is capable of representing most standard analog circuit topologies. The system operation consists of two phases-in the first phase, the circuit size and topology are evolved. A limited amount of device sizing also occurs in this phase. The second phase consists entirely of device value optimization. The design of the evaluation function-which evaluates each generated circuit using SPICE simulations-has also been automated to a great extent. The evaluation function is generated automatically depending on a behavioral description of the circuit. We present several experimental results obtained using this scheme, including two types of comparators, two types of oscillators, and an XOR logic gate. The generated circuits closely resemble hand designed circuits. The computational needs of the system are modest.},
  eventtitle = {{{IEEE Transactions}} on {{Evolutionary Computation}}},
  keywords = {Analog circuits,Bioinformatics,circuit design,Circuit simulation,Circuit synthesis,Circuit topology,Design automation,evolutionary search,Genetic programming,Genomics,Process design,SPICE,SPICE simulation,topological reuse},
  file = {/home/krawczuk/Zotero/storage/EMHC2NGZ/1413261.html}
}

@inproceedings{dasTopologySynthesisAnalog2008,
  title = {Topology Synthesis of Analog Circuits Based on Adaptively Generated Building Blocks},
  booktitle = {Proceedings of the 45th Annual {{Design Automation Conference}}},
  author = {Das, Angan and Vemuri, Ranga},
  date = {2008-06-08},
  series = {{{DAC}} '08},
  pages = {44--49},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/1391469.1391483},
  url = {https://dl.acm.org/doi/10.1145/1391469.1391483},
  urldate = {2024-02-19},
  abstract = {This paper presents an automated analog synthesis tool for topology generation and subsequent circuit sizing. Though sizing is indispensable, the paper mainly concentrates on topology generation. A new kind of GA is developed, where a fraction of the offsprings in each generation is built from building blocks or cells obtained from previous generations. The cells are stored in a hierarchically arranged library that also contains information on the preferred neighborhood of each cell. The adaptively formed cell library starts only with basic elements and gradually includes functionally useful and bigger blocks, pertinent to the design. The techniques have been applied to synthesize an operational amplifier and a ring oscillator design. Results show that with reasonable computational effort, topologies have evolved that are designer understandable.},
  isbn = {978-1-60558-115-6},
  keywords = {automated design,genetic algorithm,topology generation},
  file = {/home/krawczuk/Zotero/storage/N7PPL6U3/Das and Vemuri - 2008 - Topology synthesis of analog circuits based on ada.pdf}
}

@online{decaoMolGANImplicitGenerative2022b,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  date = {2022-09-27},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1805.11973},
  url = {http://arxiv.org/abs/1805.11973},
  urldate = {2024-02-24},
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse. Code at https://github.com/nicola-decao/MolGAN},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/EMX5CAXR/De Cao and Kipf - 2022 - MolGAN An implicit generative model for small mol.pdf;/home/krawczuk/Zotero/storage/RKRWRQZH/1805.html}
}

@online{DefiningCommandsOptional,
  title = {8.1 {{Defining Commands}} with an {{Optional Argument}}},
  url = {https://www.dickimaw-books.com/latex/novices/html/newcomopt.html},
  urldate = {2024-02-23},
  file = {/home/krawczuk/Zotero/storage/65EZM6L7/newcomopt.html}
}

@article{dibartolomeoSTRATISFIMALLAYOUTModular2022,
  title = {{{STRATISFIMAL LAYOUT}}: {{A}} Modular Optimization Model for Laying out Layered Node-Link Network Visualizations},
  shorttitle = {{{STRATISFIMAL LAYOUT}}},
  author = {family=Bartolomeo, given=Sara, prefix=di, useprefix=true and Riedewald, Mirek and Gatterbauer, Wolfgang and Dunne, Cody},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {1},
  pages = {324--334},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3114756},
  url = {https://ieeexplore.ieee.org/abstract/document/9556579?casa_token=rpAO5UT0J9oAAAAA:yZQZinEONYZWld1DiK49IFMVf3GhoRMvMvFiAX7HxH8w0USq5XcT6uMAWTOO6Z6SR-CKy-5s8HFT},
  urldate = {2024-02-22},
  abstract = {Node-link visualizations are a familiar and powerful tool for displaying the relationships in a network. The readability of these visualizations highly depends on the spatial layout used for the nodes. In this paper, we focus on computing layered layouts, in which nodes are aligned on a set of parallel axes to better expose hierarchical or sequential relationships. Heuristic-based layouts are widely used as they scale well to larger networks and usually create readable, albeit sub-optimal, visualizations. We instead use a layout optimization model that prioritizes optimality - as compared to scalability - because an optimal solution not only represents the best attainable result, but can also serve as a baseline to evaluate the effectiveness of layout heuristics. We take an important step towards powerful and flexible network visualization by proposing Stratisfimal Layout, a modular integer-linear-programming formulation that can consider several important readability criteria simultaneously — crossing reduction, edge bendiness, and nested and multi-layer groups. The layout can be adapted to diverse use cases through its modularity. Individual features can be enabled and customized depending on the application. We provide open-source and documented implementations of the layout, both for web-based and desktop visualizations. As a proof-of-concept, we apply it to the problem of visualizing complicated SQL queries, which have features that we believe cannot be addressed by existing layout optimization models. We also include a benchmark network generator and the results of an empirical evaluation to assess the performance trade-offs of our design choices. A full version of this paper with all appendices, data, and source code is available at osf.io/qdyt9 with live examples at https://visdunneright.github.io/stratisfimal/.},
  eventtitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  keywords = {bendiness reduction,Computational modeling,crossing reduction,integer linear programming,Integer linear programming,Layered node-link visualization,Layout,nested groups,Optimization,Scalability,Structured Query Language,Visualization},
  file = {/home/krawczuk/Zotero/storage/FC2QISNY/di Bartolomeo et al. - 2022 - STRATISFIMAL LAYOUT A modular optimization model .pdf;/home/krawczuk/Zotero/storage/F6HYYYSY/9556579.html}
}

@online{dielemanContinuousDiffusionCategorical2022,
  title = {Continuous Diffusion for Categorical Data},
  author = {Dieleman, Sander and Sartran, Laurent and Roshannai, Arman and Savinov, Nikolay and Ganin, Yaroslav and Richemond, Pierre H. and Doucet, Arnaud and Strudel, Robin and Dyer, Chris and Durkan, Conor and Hawthorne, Curtis and Leblond, Rémi and Grathwohl, Will and Adler, Jonas},
  date = {2022-12-15},
  eprint = {2211.15089},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2211.15089},
  url = {http://arxiv.org/abs/2211.15089},
  urldate = {2024-02-25},
  abstract = {Diffusion models have quickly become the go-to paradigm for generative modelling of perceptual signals (such as images and sound) through iterative refinement. Their success hinges on the fact that the underlying physical phenomena are continuous. For inherently discrete and categorical data such as language, various diffusion-inspired alternatives have been proposed. However, the continuous nature of diffusion models conveys many benefits, and in this work we endeavour to preserve it. We propose CDCD, a framework for modelling categorical data with diffusion models that are continuous both in time and input space. We demonstrate its efficacy on several language modelling tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/6SVC9A4E/Dieleman et al. - 2022 - Continuous diffusion for categorical data.pdf;/home/krawczuk/Zotero/storage/CVLUB4T3/2211.html}
}

@online{DiffusionCNFLearningDenoising,
  title = {{{DiffusionCNF}}: {{Learning Denoising Diffusion Models}} via {{Conditional Normalizing Flows}} - {{University}} of {{Georgia}}},
  url = {https://esploro.libs.uga.edu/esploro/outputs/9949575129702959?institution=01GALI_UGA&skipUsageReporting=true&recordUsage=false},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/RNEJG3YT/9949575129702959.html}
}

@article{dingResultsTreeDecomposition1995,
  title = {Some Results on Tree Decomposition of Graphs},
  author = {Ding, Guoli and Oporowski, Bogdan},
  date = {1995},
  journaltitle = {Journal of Graph Theory},
  volume = {20},
  number = {4},
  pages = {481--499},
  issn = {1097-0118},
  doi = {10.1002/jgt.3190200412},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/jgt.3190200412},
  urldate = {2024-02-20},
  abstract = {We investigate tree decompositions (T,(Xt)tϵV(T)) whose width is “close to optimal” and such that all the subtrees of T induced by the vertices of the graph are “small.” We prove the existence of such decompositions for various interpretations of “close to optimal” and “small.” As a corollary of these results, we prove that the dilation of a graph is bounded by a logarithmic function of the congestion of the graph thereby settling a generalization of a conjecture of Bienstock. © 1995 John Wiley \& Sons, Inc.},
  langid = {english}
}

@online{dongCktGNNCircuitGraph2024,
  title = {{{CktGNN}}: {{Circuit Graph Neural Network}} for {{Electronic Design Automation}}},
  shorttitle = {{{CktGNN}}},
  author = {Dong, Zehao and Cao, Weidong and Zhang, Muhan and Tao, Dacheng and Chen, Yixin and Zhang, Xuan},
  date = {2024-02-09},
  eprint = {2308.16406},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2308.16406},
  url = {http://arxiv.org/abs/2308.16406},
  urldate = {2024-02-20},
  abstract = {The electronic design automation of analog circuits has been a longstanding challenge in the integrated circuit field due to the huge design space and complex design trade-offs among circuit specifications. In the past decades, intensive research efforts have mostly been paid to automate the transistor sizing with a given circuit topology. By recognizing the graph nature of circuits, this paper presents a Circuit Graph Neural Network (CktGNN) that simultaneously automates the circuit topology generation and device sizing based on the encoder-dependent optimization subroutines. Particularly, CktGNN encodes circuit graphs using a two-level GNN framework (of nested GNN) where circuits are represented as combinations of subgraphs in a known subgraph basis. In this way, it significantly improves design efficiency by reducing the number of subgraphs to perform message passing. Nonetheless, another critical roadblock to advancing learning-assisted circuit design automation is a lack of public benchmarks to perform canonical assessment and reproducible research. To tackle the challenge, we introduce Open Circuit Benchmark (OCB), an open-sourced dataset that contains \$10\$K distinct operational amplifiers with carefully-extracted circuit specifications. OCB is also equipped with communicative circuit generation and evaluation capabilities such that it can help to generalize CktGNN to design various analog circuits by producing corresponding datasets. Experiments on OCB show the extraordinary advantages of CktGNN through representation-based optimization frameworks over other recent powerful GNN baselines and human experts' manual designs. Our work paves the way toward a learning-based open-sourced design automation for analog circuits. Our source code is available at \textbackslash url\{https://github.com/zehao-dong/CktGNN\}.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/NALLKYJH/Dong et al. - 2024 - CktGNN Circuit Graph Neural Network for Electroni.pdf;/home/krawczuk/Zotero/storage/83W8FGXJ/2308.html}
}

@book{downeyParameterizedComplexity2012,
  title = {Parameterized {{Complexity}}},
  author = {Downey, Rodney G. and Fellows, M. R.},
  date = {2012-12-06},
  eprint = {HyTjBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The idea for this book was conceived over the second bottle of Villa Maria's Caber net Medot '89, at the dinner of the Australasian Combinatorics Conference held at Palmerston North, New Zealand in December 1990, where the authors first met and discovered they had a number of interests in common. Initially, we embarked on a small project to try to formulate reductions to address the apparent parame terized intractability of DOMINATING SET, and to introduce a structure in which to frame our answers. Having spent several months trying to get the definitions for the reductions right (they now seem so obvious), we turned to our tattered copies of Garey and Johnson's work [239]. We were stunned to find that virtually none of the classical reductions worked in the parameterized setting. We then wondered if we'd be able to find any interesting reductions. Several years, many more bottles, so many papers, and reductions later it [3] seemed that we had unwittingly stumbled upon what we believe is a truly central and new area of complexity theory. It seemed to us that the material would be of great interest to people working in areas where exact algorithms for a small range of parameters are natural and useful (e. g. , Molecular Biology, VLSI design). The tractability theory was rich with distinctive and powerful techniques. The intractability theory seemed to have a deep structure and techniques all of its own.},
  isbn = {978-1-4612-0515-9},
  langid = {english},
  pagetotal = {538},
  keywords = {Computers / Computer Science,Computers / Information Technology,Computers / Programming / Algorithms,Mathematics / Applied,Mathematics / Combinatorics,Mathematics / History \& Philosophy,Mathematics / Logic}
}

@online{duDeepReinforcementLearning2023a,
  title = {Beyond {{Deep Reinforcement Learning}}: {{A Tutorial}} on {{Generative Diffusion Models}} in {{Network Optimization}}},
  shorttitle = {Beyond {{Deep Reinforcement Learning}}},
  author = {Du, Hongyang and Zhang, Ruichen and Liu, Yinqiu and Wang, Jiacheng and Lin, Yijing and Li, Zonghang and Niyato, Dusit and Kang, Jiawen and Xiong, Zehui and Cui, Shuguang and Ai, Bo and Zhou, Haibo and Kim, Dong In},
  date = {2023-08-10},
  eprint = {2308.05384},
  eprinttype = {arxiv},
  eprintclass = {cs, eess},
  doi = {10.48550/arXiv.2308.05384},
  url = {http://arxiv.org/abs/2308.05384},
  urldate = {2024-02-20},
  abstract = {Generative Diffusion Models (GDMs) have emerged as a transformative force in the realm of Generative Artificial Intelligence (GAI), demonstrating their versatility and efficacy across a variety of applications. The ability to model complex data distributions and generate high-quality samples has made GDMs particularly effective in tasks such as image generation and reinforcement learning. Furthermore, their iterative nature, which involves a series of noise addition and denoising steps, is a powerful and unique approach to learning and generating data. This paper serves as a comprehensive tutorial on applying GDMs in network optimization tasks. We delve into the strengths of GDMs, emphasizing their wide applicability across various domains, such as vision, text, and audio generation.We detail how GDMs can be effectively harnessed to solve complex optimization problems inherent in networks. The paper first provides a basic background of GDMs and their applications in network optimization. This is followed by a series of case studies, showcasing the integration of GDMs with Deep Reinforcement Learning (DRL), incentive mechanism design, Semantic Communications (SemCom), Internet of Vehicles (IoV) networks, etc. These case studies underscore the practicality and efficacy of GDMs in real-world scenarios, offering insights into network design. We conclude with a discussion on potential future directions for GDM research and applications, providing major insights into how they can continue to shape the future of network optimization.},
  pubstate = {preprint},
  keywords = {Computer Science - Networking and Internet Architecture,Electrical Engineering and Systems Science - Signal Processing},
  file = {/home/krawczuk/Zotero/storage/35EQLGKK/Du et al. - 2023 - Beyond Deep Reinforcement Learning A Tutorial on .pdf;/home/krawczuk/Zotero/storage/ZMK6EKAR/2308.html}
}

@inproceedings{duvenaudConvolutionalNetworksGraphs2015,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and Aspuru-Guzik, Alan and Adams, Ryan P},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/f9be311e65d81a9ad8150a60844bb94c-Abstract.html},
  urldate = {2024-02-24},
  abstract = {We introduce a convolutional neural network that operates directly on graphs.These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape.The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints.We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/QC3NCDBB/Duvenaud et al. - 2015 - Convolutional Networks on Graphs for Learning Mole.pdf}
}

@online{EfficientArithmeticBlock,
  title = {Efficient {{Arithmetic Block Identification With Graph Learning}} and {{Network-Flow}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9975800?casa_token=dcI5WrXDmQkAAAAA:2HalhgJglK4bc3PJlTs7YnN_Ob_ZIiYPkdbTBPN3YUpPXs1Flyu9mRZpt3DZtkUvBE2dHJCe2TRV},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/A485QCGW/9975800.html}
}

@online{EfficientBatchConstrainedBayesian,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9336041?casa_token=fPG-7PuzrdgAAAAA:S9Y9hlsyrUarBMsLk8oVfvtisLBEAA9pVyN4KY3L9BNOOoDr4ZhBs5t5v0M1pw27rOSlhvXllF66},
  urldate = {2024-02-20}
}

@online{EfficientlyFindingBest,
  title = {Efficiently Finding the ‘Best’ Solution with Multi-Objectives from Multiple Topologies in Topology Library of Analog Circuit | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/4796529},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/D877Y7G4/4796529.html}
}

@online{EfficientPerformanceModelinga,
  title = {Efficient {{Performance Modeling}} for {{Automated CMOS Analog Circuit Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9528897?casa_token=ZBgdjluZh7sAAAAA:yF2LZzHwypFCVoDhIZjEr1XzBtaVFJMY4uvOcbPqNq0tQd21Igb4fmz-1MYaFQUWzeoOIMjCuIJY},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/AI476698/9528897.html}
}

@inproceedings{elesedyProvablyStrictGeneralisation2021b,
  title = {Provably {{Strict Generalisation Benefit}} for {{Equivariant Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Elesedy, Bryn and Zaidi, Sheheryar},
  date = {2021-07-01},
  pages = {2959--2969},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/elesedy21a.html},
  urldate = {2024-02-22},
  abstract = {It is widely believed that engineering a model to be invariant/equivariant improves generalisation. Despite the growing popularity of this approach, a precise characterisation of the generalisation benefit is lacking. By considering the simplest case of linear models, this paper provides the first provably non-zero improvement in generalisation for invariant/equivariant models when the target distribution is invariant/equivariant with respect to a compact group. Moreover, our work reveals an interesting relationship between generalisation, the number of training examples and properties of the group action. Our results rest on an observation of the structure of function spaces under averaging operators which, along with its consequences for feature averaging, may be of independent interest.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/GXZ74F9P/Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf;/home/krawczuk/Zotero/storage/H5YXFAXX/Elesedy and Zaidi - 2021 - Provably Strict Generalisation Benefit for Equivar.pdf}
}

@inproceedings{eleyanSemicustomDesignFlow2009,
  title = {Semi-Custom Design Flow: {{Leveraging Place}} and Route Tools in {{Custom Circuit}} Design},
  shorttitle = {Semi-Custom Design Flow},
  booktitle = {2009 {{IEEE International Conference}} on {{IC Design}} and {{Technology}}},
  author = {Eleyan, Nadeem N. and Lin, Ken and Kamal, Masud and Mohammad, Baker and Bassett, Paul},
  date = {2009-05},
  pages = {143--147},
  issn = {2381-3555},
  doi = {10.1109/ICICDT.2009.5166283},
  url = {https://ieeexplore.ieee.org/document/5166283},
  urldate = {2024-02-19},
  abstract = {There are generally two options available to integrated circuit (IC) designers to physically implement their designs: synthesis/place and route design and custom circuit design. Each design approach has its advantages and draw backs. This paper will cover a hybrid design flow using concepts from both areas to give us a quick design turn around time while allowing control on custom placement and routing.},
  eventtitle = {2009 {{IEEE International Conference}} on {{IC Design}} and {{Technology}}},
  keywords = {Circuit synthesis,CMOS logic circuits,Design automation,Design optimization,Digital signal processing,Integrated circuit design,Integrated circuit noise,Integrated circuit synthesis,Logic design,Random access memory,Routing,Timing},
  file = {/home/krawczuk/Zotero/storage/8C2K4VZ8/5166283.html}
}

@online{EMGraphFastLearningBased,
  title = {{{EMGraph}}: {{Fast Learning-Based Electromigration Analysis}} for {{Multi-Segment Interconnect Using Graph Convolution Networks}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9586239?casa_token=aPtfgBTKzREAAAAA:dWCqyGFU9oJLp2WomDQDeX_SSivmmB0zTFY9nPzO2I7PjEGCKXEwvSh89lwPi4KBHbZauYItMAoi},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/DYYBZ2PE/9586239.html}
}

@article{fanPowerConverterCircuit2022a,
  title = {Power {{Converter Circuit Design Automation Using Parallel Monte Carlo Tree Search}}},
  author = {Fan, Shaoze and Zhang, Shun and Liu, Jianbo and Cao, Ningyuan and Guo, Xiaoxiao and Li, Jing and Zhang, Xin},
  date = {2022-12-24},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {28},
  number = {2},
  pages = {17:1--17:33},
  issn = {1084-4309},
  doi = {10.1145/3549538},
  url = {https://dl.acm.org/doi/10.1145/3549538},
  urldate = {2024-02-19},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using Monte Carlo Tree Search. Specifically, the framework embraces the upper-confidence-bound-tree (UCT), a variant of Monte Carlo Tree Search, to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy and can run in parallel to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduce design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient—the sequential version can generate topologies with the same quality while being up to 67\% faster. The parallelization schemes can further achieve high speedups compared to the sequential version.},
  keywords = {circuit synthesis,circuit topology design,Design automation,Monte Carlo Tree Search (MCTS),power converter,upper-confidence-bound tree (UCT)},
  file = {/home/krawczuk/Zotero/storage/3VAPTTCJ/Fan et al. - 2022 - Power Converter Circuit Design Automation Using Pa.pdf}
}

@inproceedings{fanSpecificationTopologyAutomatic2021d,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}}},
  shorttitle = {From {{Specification}} to {{Topology}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Fan, Shaoze and Cao, Ningyuan and Zhang, Shun and Li, Jing and Guo, Xiaoxiao and Zhang, Xin},
  date = {2021-11},
  pages = {1--9},
  issn = {1558-2434},
  doi = {10.1109/ICCAD51958.2021.9643552},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=a9WE7fzBkhUAAAAA:uF7KocWq5SpT4JvweG-3L54Dv12Mok8KiHNsHc5sdVKaVe45kdmEkH7hNIClQ6IMX6HGapaSvMdB},
  urldate = {2024-02-19},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient - it can generate topologies with the same quality while being up to 67\% faster. Additionally, we discussed some interesting circuits discovered by our framework.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  keywords = {Circuit topology,Computational efficiency,design automation,Energy efficiency,Manuals,power converter topology design,reinforcement learning,Reinforcement learning,Space exploration,Topology,upper-confidence-bound tree (UCT)},
  file = {/home/krawczuk/Zotero/storage/HQ7VFUGM/Fan et al. - 2021 - From Specification to Topology Automatic Power Co.pdf;/home/krawczuk/Zotero/storage/WS866L76/9643552.html}
}

@inproceedings{fanSpecificationTopologyAutomatic2021e,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}}},
  shorttitle = {From {{Specification}} to {{Topology}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Fan, Shaoze and Cao, Ningyuan and Zhang, Shun and Li, Jing and Guo, Xiaoxiao and Zhang, Xin},
  date = {2021-11-01},
  pages = {1--9},
  publisher = {{IEEE}},
  location = {{Munich, Germany}},
  doi = {10.1109/ICCAD51958.2021.9643552},
  url = {https://ieeexplore.ieee.org/document/9643552/},
  urldate = {2024-02-20},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous applicationspecific power converters. A conventional manual design procedure of such power converters is computation- and laborintensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient —it can generate topologies with the same quality while being up to 67\% faster. Additionally, we discussed some interesting circuits discovered by our framework.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  isbn = {978-1-66544-507-8},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/576QF55G/Fan et al. - 2021 - From Specification to Topology Automatic Power Co.pdf}
}

@inproceedings{fanSpecificationTopologyAutomatic2021f,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}}},
  shorttitle = {From {{Specification}} to {{Topology}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Fan, Shaoze and Cao, Ningyuan and Zhang, Shun and Li, Jing and Guo, Xiaoxiao and Zhang, Xin},
  date = {2021-11},
  pages = {1--9},
  issn = {1558-2434},
  doi = {10.1109/ICCAD51958.2021.9643552},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=OfMFhpoMuAQAAAAA:AfACosBN6oY2Vpsbk8_cjU86UjZRF_Ea-T_xy-llZuNF7C5ZRvRRToOmxHA4g0vG5tvB5wt09S5x},
  urldate = {2024-02-20},
  abstract = {The tidal waves of modern electronic/electrical devices have led to increasing demands for ubiquitous application-specific power converters. A conventional manual design procedure of such power converters is computation- and labor-intensive, which involves selecting and connecting component devices, tuning component-wise parameters and control schemes, and iteratively evaluating and optimizing the design. To automate and speed up this design process, we propose an automatic framework that designs custom power converters from design specifications using reinforcement learning. Specifically, the framework embraces upper-confidence-bound-tree-based (UCT-based) reinforcement learning to automate topology space exploration with circuit design specification-encoded reward signals. Moreover, our UCT-based approach can exploit small offline data via the specially designed default policy to accelerate topology space exploration. Further, it utilizes a hybrid circuit evaluation strategy to substantially reduces design evaluation costs. Empirically, we demonstrated that our framework could generate energy-efficient circuit topologies for various target voltage conversion ratios. Compared to existing automatic topology optimization strategies, the proposed method is much more computationally efficient - it can generate topologies with the same quality while being up to 67\% faster. Additionally, we discussed some interesting circuits discovered by our framework.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  keywords = {Circuit topology,Computational efficiency,design automation,Energy efficiency,Manuals,power converter topology design,reinforcement learning,Reinforcement learning,Space exploration,Topology,upper-confidence-bound tree (UCT)},
  file = {/home/krawczuk/Zotero/storage/IP2RU9EW/Fan et al. - 2021 - From Specification to Topology Automatic Power Co.pdf}
}

@article{fayaziAnGeLFullyAutomatedAnalog2023a,
  title = {{{AnGeL}}: {{Fully-Automated Analog Circuit Generator Using}} a {{Neural Network Assisted Semi-Supervised Learning Approach}}},
  shorttitle = {{{AnGeL}}},
  author = {Fayazi, Morteza and Taba, Morteza Tavakoli and Afshari, Ehsan and Dreslinski, Ronald},
  date = {2023-11},
  journaltitle = {IEEE Transactions on Circuits and Systems I: Regular Papers},
  volume = {70},
  number = {11},
  pages = {4516--4529},
  issn = {1558-0806},
  doi = {10.1109/TCSI.2023.3295737},
  url = {https://ieeexplore.ieee.org/abstract/document/10190116?casa_token=W4xAyjAsivMAAAAA:4e4PlPkYtzIr3vRUNkwyR78lbRvfkNBE1ywx3WA_MAYg-QtyL-XR9DIO20jdxR4HY6QqXKiPJD5E},
  urldate = {2024-02-19},
  abstract = {Machine Learning (ML) has shown promising results in predicting the behavior of analog circuits. However, in order to completely cover the design space for today’s complicated circuits, supervised ML requires a large number of labeled samples which is time-consuming to provide. Furthermore, a separate dataset must be collected for each circuit topology making all other previously gathered datasets useless. In this paper, we first present a database including labeled and unlabeled data. We use neural networks to determine the behavior of complicated topologies by combining the more simple ones. By generating such unlabeled data, the time for providing the training set is significantly reduced compared to the conventional approaches. Using this database, we propose a fully-automated analog circuit generator framework, AnGeL. AnGeL performs all the schematic circuit design steps from deciding the circuit topology to determining the circuit parameters i.e. sizing. Our results show that for multiple circuit topologies, in comparison to the state-of-the-art works while maintaining the same accuracy, the required labeled data is reduced by 4.7x - 1090x. Also, the runtime of AnGeL is 2.9x - 75x faster.},
  eventtitle = {{{IEEE Transactions}} on {{Circuits}} and {{Systems I}}: {{Regular Papers}}},
  keywords = {Analog circuit design automation,Analog circuits,circuit sizing,Circuit synthesis,Circuit topology,Databases,Integrated circuit modeling,neural network,semi-supervised learning,Topology,topology selection,Training},
  file = {/home/krawczuk/Zotero/storage/EVNWLPZI/Fayazi et al. - 2023 - AnGeL Fully-Automated Analog Circuit Generator Us.pdf;/home/krawczuk/Zotero/storage/RELTEWK9/10190116.html}
}

@online{FEATSFrameworkExplorativea,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=Q7MqnOUlxuQAAAAA:AzXEteK3Y7AbcL5SV3ko3-aD8fUBJCvXV75FkslpJrfLg2RAseaiNNRII611beTwmAL2HDP_30I9},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/M6U2YDWA/6980087.html}
}

@online{FEATSFrameworkExplorativeb,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=7W00nHRy4joAAAAA:-NpwTCtaXxFtwCYJSnhNC1ZDlMRv_6NxpQFZLRE9OlqvgScAq5z60SejgPlV3fs_nRW4pm7RSSuT},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/ILE25UIW/6980087.html}
}

@online{FEATSFrameworkExplorativec,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=AupKaZ1Ni9MAAAAA:s6GSVExX1r_PejIk3CYI_X64YVSG4ls8AcDoa9F7QO33jCx7hbSkZYJ2-2dI5vbLj0_VZTKZerY},
  urldate = {2024-02-24},
  file = {/home/krawczuk/Zotero/storage/QUEYYDCM/6980087.html}
}

@online{fichteDynASP2DynamicProgramming2017,
  title = {{{DynASP2}}.5: {{Dynamic Programming}} on {{Tree Decompositions}} in {{Action}}},
  shorttitle = {{{DynASP2}}.5},
  author = {Fichte, Johannes K. and Hecher, Markus and Morak, Michael and Woltran, Stefan},
  date = {2017-06-28},
  eprint = {1706.09370},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.1706.09370},
  url = {http://arxiv.org/abs/1706.09370},
  urldate = {2024-02-20},
  abstract = {A vibrant theoretical research area are efficient exact parameterized algorithms. Very recent solving competitions such as the PACE challenge show that there is also increasing practical interest in the parameterized algorithms community. An important research question is whether dedicated parameterized exact algorithms exhibit certain practical relevance and one can even beat well-established problem solvers. We consider the logic-based declarative modeling language and problem solving framework Answer Set Programming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based algorithms. An ASP solver (DynASP2), which is based on a classical dynamic programming on tree decompositions, has been published very recently. Unfortunately, DynASP2 can outperform modern ASP solvers on programs of small treewidth only if the question of interest is to count the number of solutions. In this paper, we describe underlying concepts of our new implementation (DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers even for finding just one solution when solving problems as the Steiner tree problem that have been modeled in ASP on graphs with low treewidth. Our implementation is based on a novel approach that we call multi-pass dynamic programming (M-DPSINC).},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Complexity,Computer Science - Logic in Computer Science},
  file = {/home/krawczuk/Zotero/storage/9P4WHLHN/Fichte et al. - 2017 - DynASP2.5 Dynamic Programming on Tree Decompositi.pdf;/home/krawczuk/Zotero/storage/826FFN38/1706.html}
}

@online{franklEmbeddingGraphsEuclidean,
  title = {Embedding Graphs in {{Euclidean}} Space},
  author = {Frankl, Nóra and Kupavskii, Andrey and Swanepoel, Konrad J.},
  eprint = {1802.03092},
  eprinttype = {arxiv},
  eprintclass = {math},
  doi = {10.1016/j.jcta.2019.105146},
  url = {http://arxiv.org/abs/1802.03092},
  urldate = {2024-02-22},
  abstract = {The dimension of a graph \$G\$ is the smallest \$d\$ for which its vertices can be embedded in \$d\$-dimensional Euclidean space in the sense that the distances between endpoints of edges equal \$1\$ (but there may be other unit distances). Answering a question of Erd\textbackslash H\{o\}s and Simonovits [Ars Combin. 9 (1980) 229--246], we show that any graph with less than \$\textbackslash binom\{d+2\}\{2\}\$ edges has dimension at most \$d\$. Improving their result, we prove that that the dimension of a graph with maximum degree \$d\$ is at most \$d\$. We show the following Ramsey result: if each edge of the complete graph on \$2d\$ vertices is coloured red or blue, then either the red graph or the blue graph can be embedded in Euclidean \$d\$-space. We also derive analogous results for embeddings of graphs into the \$(d-1)\$-dimensional sphere of radius \$1/\textbackslash sqrt\{2\}\$.},
  pubstate = {preprint},
  keywords = {52C10,Mathematics - Combinatorics,Mathematics - Metric Geometry},
  file = {/home/krawczuk/Zotero/storage/L7DMQ4DE/Frankl et al. - Embedding graphs in Euclidean space.pdf;/home/krawczuk/Zotero/storage/W9SYS3E8/1802.html}
}

@online{fulber-garciaHowDrawFlowcharts2022,
  title = {How to {{Draw Flowcharts With LaTeX}} | {{Baeldung}} on {{Computer Science}}},
  author = {Fulber-Garcia, Vinicius},
  date = {2022-01-19T08:05:14+00:00},
  url = {https://www.baeldung.com/cs/latex-flowcharts},
  urldate = {2024-02-19},
  abstract = {Learn how to draw flowcharts with LaTeX/TikZ.},
  langid = {american},
  file = {/home/krawczuk/Zotero/storage/5MPRAJFQ/latex-flowcharts.html}
}

@online{FundamentalsLayoutDesign,
  title = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}} | {{SpringerLink}}},
  url = {https://link.springer.com/book/10.1007/978-3-030-39284-0},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/9PWSQLIF/978-3-030-39284-0.html}
}

@online{GANAGraphConvolutional,
  title = {{{GANA}}: {{Graph Convolutional Network Based Automated Netlist Annotation}} for {{Analog Circuits}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9116329?casa_token=pmDFJicmP08AAAAA:1rCal6CTB7-Eamcy3AvSogNocH1GGEo4qlxNM7-egouIg952ujAHMwdEnWLA-IPDRraV4uaxiUeh},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/IJYBLPVS/9116329.html}
}

@book{gebotysOptimalVLSIArchitectural2012,
  title = {Optimal {{VLSI Architectural Synthesis}}: {{Area}}, {{Performance}} and {{Testability}}},
  shorttitle = {Optimal {{VLSI Architectural Synthesis}}},
  author = {Gebotys, Catherine H. and Elmasry, Mohamed I.},
  date = {2012-12-06},
  eprint = {njrUBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Although research in architectural synthesis has been conducted for over ten years it has had very little impact on industry. This in our view is due to the inability of current architectural synthesizers to provide area-delay competitive (or "optimal") architectures, that will support interfaces to analog, asynchronous, and other complex processes. They also fail to incorporate testability. The OASIC (optimal architectural synthesis with interface constraints) architectural synthesizer and the CATREE (computer aided trees) synthesizer demonstrate how these problems can be solved. Traditionally architectural synthesis is viewed as NP hard and there fore most research has involved heuristics. OASIC demonstrates by using an IP approach (using polyhedral analysis), that most input algo rithms can be synthesized very fast into globally optimal architectures. Since a mathematical model is used, complex interface constraints can easily be incorporated and solved. Research in test incorporation has in general been separate from syn thesis research. This is due to the fact that traditional test research has been at the gate or lower level of design representation. Nevertheless as technologies scale down, and complexity of design scales up, the push for reducing testing times is increased. On way to deal with this is to incorporate test strategies early in the design process. The second half of this text examines an approach for integrating architectural synthesis with test incorporation. Research showed that test must be considered during synthesis to provide good architectural solutions which minimize Xlll area delay cost functions.},
  isbn = {978-1-4615-4018-2},
  langid = {english},
  pagetotal = {293},
  keywords = {{Computers / Design, Graphics \& Media / CAD-CAM},Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Circuits / General}
}

@online{GenericTopologySelection,
  title = {A Generic Topology Selection Method for Analog Circuits with Embedded Circuit Sizing Demonstrated on the {{OTA}} Example | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/7927115?casa_token=ICCzPz3hLuEAAAAA:VeKxL84Rvqjsw44k7Y-0Wr-wLzB9ObE8Xn84V4EOfdSpxj6zjwrGnL2JhOFRAt7O6gQUnlQWJ2C3},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/IJVE8Z6S/7927115.html}
}

@online{genevayGANVAEOptimal2017e,
  title = {{{GAN}} and {{VAE}} from an {{Optimal Transport Point}} of {{View}}},
  author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
  date = {2017-06-06},
  eprint = {1706.01807},
  eprinttype = {arxiv},
  eprintclass = {stat},
  doi = {10.48550/arXiv.1706.01807},
  url = {http://arxiv.org/abs/1706.01807},
  urldate = {2024-02-20},
  abstract = {This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE).},
  pubstate = {preprint},
  keywords = {Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/QWXUSBCN/Genevay et al. - 2017 - GAN and VAE from an Optimal Transport Point of Vie.pdf;/home/krawczuk/Zotero/storage/Y72YXKWA/1706.html}
}

@inreference{GeometricGraphTheory2024,
  title = {Geometric Graph Theory},
  booktitle = {Wikipedia},
  date = {2024-01-16T22:33:01Z},
  url = {https://en.wikipedia.org/w/index.php?title=Geometric_graph_theory&oldid=1196243069},
  urldate = {2024-02-22},
  abstract = {Geometric graph theory in the broader sense is a large and amorphous subfield of graph theory, concerned with graphs defined by geometric means. In a stricter sense, geometric graph theory studies combinatorial and geometric properties of geometric graphs, meaning graphs drawn in the Euclidean plane with possibly intersecting straight-line edges, and topological graphs, where the edges are allowed to be arbitrary continuous curves connecting the vertices; thus, it can be described as "the theory of geometric and topological graphs" (Pach 2013). Geometric graphs are also known as spatial networks.},
  langid = {english},
  annotation = {Page Version ID: 1196243069},
  file = {/home/krawczuk/Zotero/storage/7XVHPA7E/Geometric_graph_theory.html}
}

@online{GeometricRepresentationsGraphs,
  title = {Geometric {{Representations}} of {{Graphs}} - {{Chair}} of {{Computer Science I}} - {{Algorithms}} and {{Complexity}}},
  url = {https://www.informatik.uni-wuerzburg.de/en/algo/projects/geometric-representations-of-graphs/},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/KJFZQ2Z6/geometric-representations-of-graphs.html}
}

@incollection{gilmerMessagePassingNeural2020,
  title = {Message {{Passing Neural Networks}}},
  booktitle = {Machine {{Learning Meets Quantum Physics}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  editor = {Schütt, Kristof T. and Chmiela, Stefan and family=Lilienfeld, given=O. Anatole, prefix=von, useprefix=true and Tkatchenko, Alexandre and Tsuda, Koji and Müller, Klaus-Robert},
  date = {2020},
  series = {Lecture {{Notes}} in {{Physics}}},
  pages = {199--214},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-40245-7_10},
  url = {https://doi.org/10.1007/978-3-030-40245-7_10},
  urldate = {2024-02-24},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. In this chapter, we describe a general common framework for learning representations on graph data called message passing neural networks (MPNNs) and show how several prior neural network models for graph data fit into this framework. This chapter contains large overlap with Gilmer et al. (International Conference on Machine Learning, pp. 1263–1272, 2017), and has been modified to highlight more recent extensions to the MPNN framework.},
  isbn = {978-3-030-40245-7},
  langid = {english}
}

@inproceedings{gilmerNeuralMessagePassing2017a,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  date = {2017-07-17},
  pages = {1263--1272},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v70/gilmer17a.html},
  urldate = {2024-02-24},
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/3USUCFH4/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf}
}

@online{GNNCapChipScaleInterconnect,
  title = {{{GNN-Cap}}: {{Chip-Scale Interconnect Capacitance Extraction Using Graph Neural Network}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10314730?casa_token=b_LSbuO_O6cAAAAA:IqWNu4n9DoqhTPEMEIX9Ruj7du-yaEuWAjJnMwHPiqQ-FbY4QIp4XXm3WD9iinBJl6flelYhshzt},
  urldate = {2024-02-22}
}

@inproceedings{graebAnalogSynthesisDeterministic2022,
  title = {Analog {{Synthesis}} - {{The Deterministic Way}}},
  booktitle = {Proceedings of the 2022 {{International Symposium}} on {{Physical Design}}},
  author = {Graeb, Helmut},
  date = {2022-04-13},
  series = {{{ISPD}} '22},
  pages = {167--174},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3505170.3511043},
  url = {https://dl.acm.org/doi/10.1145/3505170.3511043},
  urldate = {2024-02-20},
  abstract = {While the majority of research in design automation for analog circuits has been relying on statistical solution approaches, deterministic approaches are an attractive alternative. This paper gives a few examples of deterministic methods for sizing, structural synthesis and layout synthesis of analog circuits, which have been developed over the past decades. It starts from the so-called characteristic boundary curve for interactive parameter optimization, and ends at recent approaches for structural synthesis of operational amplifiers based on functional block composition. A deterministic approach to analog placement and to yield optimization will also be described. The central role of structural analysis of circuit netlists in these approaches will be explained. A summary of the underlying mindset of analog design automation and an outlook on future opportunities for deterministic sizing and layout synthesis concludes the paper.},
  isbn = {978-1-4503-9210-5},
  keywords = {analog,circuit,design centering,layout,optimization,sizing,synthesis},
  file = {/home/krawczuk/Zotero/storage/AVWMFWHZ/Graeb - 2022 - Analog Synthesis - The Deterministic Way.pdf}
}

@inproceedings{graebLearningImplicitFunctional2023,
  title = {Learning from the {{Implicit Functional Hierarchy}} in an {{Analog Netlist}}},
  booktitle = {Proceedings of the 2023 {{International Symposium}} on {{Physical Design}}},
  author = {Graeb, Helmut and Leibl, Markus},
  date = {2023-03-26},
  series = {{{ISPD}} '23},
  pages = {93--100},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3569052.3578921},
  url = {https://dl.acm.org/doi/10.1145/3569052.3578921},
  urldate = {2024-02-19},
  abstract = {Analog circuit design is characterized by a plethora of implicit design and technology aspects available to the experienced designer. In order to create useful computer-aided design methods, this implicit knowledge has to be captured in a systematic and hierarchical way. A key approach to this goal is to "learn" the knowledge from the netlist of an analog circuit. This requires a library of structural and functional blocks for analog circuits together with their individual constraints and performance equations, graph homomorphism techniques to recognize blocks that can have different structural implementations and I/O pins, as well as synthesis methods that exploit the learned knowledge. In this contribution, we will present how to make use of the functional and structural hierarchy of operational amplifiers. As an application, we explore the capabilities of machine learning in the context of structural and functional properties and show that the results can be substantially improved by pre-processing data with traditional methods for functional block analysis. This claim is validated on a data set of roughly 100,000 readily sized and simulated operational amplifiers.},
  isbn = {978-1-4503-9978-4},
  keywords = {analog circuits,cad,design automation,neural networks,operational amplifier},
  file = {/home/krawczuk/Zotero/storage/XKU5CZMK/Graeb and Leibl - 2023 - Learning from the Implicit Functional Hierarchy in.pdf}
}

@inproceedings{grusAutomaticPlacerAnalog2023,
  title = {Automatic {{Placer}} for {{Analog Circuits Using Integer Linear Programming Warm Started}} by {{Graph Drawing}}:},
  shorttitle = {Automatic {{Placer}} for {{Analog Circuits Using Integer Linear Programming Warm Started}} by {{Graph Drawing}}},
  booktitle = {Proceedings of the 12th {{International Conference}} on {{Operations Research}} and {{Enterprise Systems}}},
  author = {Grus, Josef and Hanzálek, Zdeněk and Barri, Dalibor and Vacula, Patrik},
  date = {2023},
  pages = {106--116},
  publisher = {{SCITEPRESS - Science and Technology Publications}},
  location = {{Lisbon, Portugal}},
  doi = {10.5220/0011789300003396},
  url = {https://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0011789300003396},
  urldate = {2024-02-20},
  eventtitle = {12th {{International Conference}} on {{Operations Research}} and {{Enterprise Systems}}},
  isbn = {978-989-758-627-9},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/L9VSMMIP/Grus et al. - 2023 - Automatic Placer for Analog Circuits Using Integer.pdf}
}

@article{gruverProteinDesignGuided2024,
  title = {Protein {{Design}} with {{Guided Discrete Diffusion}}},
  author = {Gruver, Nate and Stanton, Samuel and Frey, Nathan and Rudner, Tim G. J. and Hotzel, Isidro and Lafrance-Vanasse, Julien and Rajpal, Arvind and Cho, Kyunghyun and Wilson, Andrew G.},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/29591f355702c3f4436991335784b503-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/9D4HD67M/Gruver et al. - 2024 - Protein Design with Guided Discrete Diffusion.pdf}
}

@online{GtGeometricTopology,
  title = {Gt.Geometric Topology - {{Graph}} Embedding in {{3D}} Grid Minimizing Edge Length - {{MathOverflow}}},
  url = {https://mathoverflow.net/questions/158460/graph-embedding-in-3d-grid-minimizing-edge-length},
  urldate = {2024-02-22}
}

@inproceedings{gulrajaniImprovedTrainingWasserstein2017d,
  title = {Improved {{Training}} of {{Wasserstein GANs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/892c3b1c6dccd52936e27cbd0ff683d6-Abstract.html},
  urldate = {2024-02-25},
  abstract = {Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only poor samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models with continuous generators. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/HHKIRMR4/Gulrajani et al. - 2017 - Improved Training of Wasserstein GANs.pdf}
}

@online{gulrajaniLikelihoodBasedDiffusionLanguage2023,
  title = {Likelihood-{{Based Diffusion Language Models}}},
  author = {Gulrajani, Ishaan and Hashimoto, Tatsunori B.},
  date = {2023-05-30},
  eprint = {2305.18619},
  eprinttype = {arxiv},
  eprintclass = {cs},
  url = {http://arxiv.org/abs/2305.18619},
  urldate = {2024-02-22},
  abstract = {Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms GPT-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.},
  pubstate = {preprint},
  version = {1},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GZZD4AKY/Gulrajani and Hashimoto - 2023 - Likelihood-Based Diffusion Language Models.pdf;/home/krawczuk/Zotero/storage/P75JPSPM/2305.html}
}

@inproceedings{guptaGRAFENNELearningGraphs2023,
  title = {{{GRAFENNE}}: {{Learning}} on {{Graphs}} with {{Heterogeneous}} and {{Dynamic Feature Sets}}},
  shorttitle = {{{GRAFENNE}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Gupta, Shubham and Manchanda, Sahil and Ranu, Sayan and Bedathur, Srikanta J.},
  date = {2023-07-03},
  pages = {12165--12181},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/gupta23b.html},
  urldate = {2024-02-20},
  abstract = {Graph neural networks (GNNs), in general, are built on the assumption of a static set of features characterizing each node in a graph. This assumption is often violated in practice. Existing methods partly address this issue through feature imputation. However, these techniques (i) assume uniformity of feature set across nodes, (ii) are transductive by nature, and (iii) fail to work when features are added or removed over time. In this work, we address these limitations through a novel GNN framework called GRAFENNE. GRAFENNE performs a novel allotropic transformation on the original graph, wherein the nodes and features are decoupled through a bipartite encoding. Through a carefully chosen message passing framework on the allotropic transformation, we make the model parameter size independent of the number of features and thereby inductive to both unseen nodes and features. We prove that GRAFENNE is at least as expressive as any of the existing message-passing GNNs in terms of Weisfeiler-Leman tests, and therefore, the additional inductivity to unseen features does not come at the cost of expressivity. In addition, as demonstrated over four real-world graphs, GRAFENNE empowers the underlying GNN with high empirical efficacy and the ability to learn in continual fashion over streaming feature sets.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/76HYCMSE/Gupta et al. - 2023 - GRAFENNE Learning on Graphs with Heterogeneous an.pdf}
}

@online{guptaStructuringRepresentationGeometry2023,
  title = {Structuring {{Representation Geometry}} with {{Rotationally Equivariant Contrastive Learning}}},
  author = {Gupta, Sharut and Robinson, Joshua and Lim, Derek and Villar, Soledad and Jegelka, Stefanie},
  date = {2023-06-24},
  eprint = {2306.13924},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2306.13924},
  url = {http://arxiv.org/abs/2306.13924},
  urldate = {2024-02-20},
  abstract = {Self-supervised learning converts raw perceptual data such as images to a compact space where simple Euclidean distances measure meaningful variations in data. In this paper, we extend this formulation by adding additional geometric structure to the embedding space by enforcing transformations of input space to correspond to simple (i.e., linear) transformations of embedding space. Specifically, in the contrastive learning setting, we introduce an equivariance objective and theoretically prove that its minima forces augmentations on input space to correspond to rotations on the spherical embedding space. We show that merely combining our equivariant loss with a non-collapse term results in non-trivial representations, without requiring invariance to data augmentations. Optimal performance is achieved by also encouraging approximate invariance, where input augmentations correspond to small rotations. Our method, CARE: Contrastive Augmentation-induced Rotational Equivariance, leads to improved performance on downstream tasks, and ensures sensitivity in embedding space to important variations in data (e.g., color) that standard contrastive methods do not achieve. Code is available at https://github.com/Sharut/CARE.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/54EQDGJM/Gupta et al. - 2023 - Structuring Representation Geometry with Rotationa.pdf;/home/krawczuk/Zotero/storage/BLRMI2GL/2306.html}
}

@inproceedings{gusmaoDeepLearningToolbox2021,
  title = {A {{Deep Learning Toolbox}} for {{Analog Integrated Circuit Placement}}},
  booktitle = {{{SMACD}} / {{PRIME}} 2021; {{International Conference}} on {{SMACD}} and 16th {{Conference}} on {{PRIME}}},
  author = {Gusmao, Antonio and Canelas, Antonio and Horta, Nuno and Lourenco, Nuno and Martins, Ricardo},
  date = {2021-07},
  pages = {1--4},
  url = {https://ieeexplore.ieee.org/abstract/document/9547914},
  urldate = {2024-02-20},
  abstract = {This paper presents a deep learning toolbox, DEEPPLACER, to assist designers during the layout design of analog integrated circuits. DEEPPLACER relies on a simple pair-wise device interaction circuit description, i.e., the circuits’ topological constraints, to propose valid floorplan solutions for block-level structures, including topologies and deep technology nodes not used for its training, at push-button speed. Despite its automatic functionalities, the toolbox is focused on explainable artificial intelligence, involving the designer in the synthesis flow via filtering and editing options over the candidate floorplan solutions. This constant state of human-machine feedback environment turns the designer aware of the impact of each device’s position change and inherent tradeoffs while suggesting subsequent moves, ultimately increasing the designers’ productivity in this time-consuming and iterative task. Finally, DEEPPLACER is shown to instantly generate a floorplan with 61\% better constraint fulfilment than a human designed solution.},
  eventtitle = {{{SMACD}} / {{PRIME}} 2021; {{International Conference}} on {{SMACD}} and 16th {{Conference}} on {{PRIME}}},
  file = {/home/krawczuk/Zotero/storage/TESS7RSF/Gusmao et al. - 2021 - A Deep Learning Toolbox for Analog Integrated Circ.pdf;/home/krawczuk/Zotero/storage/4R3YSPCR/9547914.html}
}

@article{gusmaoDifferentiableConstraintsEncoding2023,
  title = {Differentiable {{Constraints}}’ {{Encoding}} for {{Gradient-Based Analog Integrated Circuit Placement Optimization}}},
  author = {Gusmão, António and Alves, Pedro and Horta, Nuno and Lourenço, Nuno and Martins, Ricardo},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {1},
  pages = {110},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics12010110},
  url = {https://www.mdpi.com/2079-9292/12/1/110},
  urldate = {2024-02-20},
  abstract = {Analog IC design is characterized by non-systematic re-design iterations, often requiring partial or complete layout re-design. The layout task usually starts with device placement, where the several performance figures and constraints to be met escalate its complexity immensely, and, due to the inherent tradeoffs, an “optimal” floorplan solution does not usually exist. Deep learning models are now establishing for the automation of the placement task of analog integrated circuit layout design, promising to bypass the limitations of existing approaches based on: time-consuming optimization processes with several constraints; or placement retargeting from legacy designs/templates, which rely heavily on legacy layout data. However, as the complexity of analog design cases tackled by these methodologies increases, a broader set of topological constraints must be supported to cover the different layout styles and circuit classes. Here, model-independent differentiable encodings for regularity, boundary, proximity, and symmetry island constraints are formulated for the first time in the literature, and an unsupervised loss function is used for the artificial neural network model to learn how to generate placements that follow them. The use of a deep learning model makes push-button speed placement generation possible, additionally, as only sizing data are required for its training, it discards the need to acquire legacy layouts containing insights into this vast set of, often neglected, constraints. The model is ultimately used to produce floorplans from scratch at push-button speed for real state-of-the-art analog structures, including technology nodes not used for training. A case-study comparison with a floorplan design made by a human-expert presents improvements in the fulfillment of every constraint, reaching an overall improvement of around 70\%, demonstrating the approach’s value in placement design.},
  issue = {1},
  langid = {english},
  keywords = {automatic placement,deep learning,electronic design automation,physical synthesis,topological constraints},
  file = {/home/krawczuk/Zotero/storage/TUG7TDXT/Gusmão et al. - 2023 - Differentiable Constraints’ Encoding for Gradient-.pdf}
}

@inproceedings{gusmaoLateBreakingResults2021,
  title = {Late {{Breaking Results}}: {{Attention}} in {{Graph2Seq Neural Networks}} towards {{Push-Button Analog IC Placement}}},
  shorttitle = {Late {{Breaking Results}}},
  booktitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Gusmao, Antonio and Horta, Nuno and Lourenco, Nuno and Martins, Ricardo},
  date = {2021-12-05},
  pages = {1360--1361},
  publisher = {{IEEE}},
  location = {{San Francisco, CA, USA}},
  doi = {10.1109/DAC18074.2021.9586177},
  url = {https://ieeexplore.ieee.org/document/9586177/},
  urldate = {2024-02-20},
  abstract = {In this paper, disruptive research using modern embedding techniques and an attention-based encoder-decoder deep learning (DL) model is conducted to automate analog layout synthesis. Unlike previous legacy-based placement automation mechanisms, the attention-based Graph2Seq model is inherently independent of the number of devices within a circuit topology and their order. Moreover, its unsupervised training does not rely on expensive legacy layout data but only on sizing solutions. Experimental results show that the proposed model generates placement solutions at push-button speed and can generalize to circuit topologies and technological nodes not used in training. Moreover, while being scalable, the model produces placement solutions that compete with highly optimized analog placements and other, order-dependent and non-scalable, DL models.},
  eventtitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  isbn = {978-1-66543-274-0},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/WIZ4XQXT/Gusmao et al. - 2021 - Late Breaking Results Attention in Graph2Seq Neur.pdf}
}

@online{haefeliDiffusionModelsGraphs2023a,
  title = {Diffusion {{Models}} for {{Graphs Benefit From Discrete State Spaces}}},
  author = {Haefeli, Kilian Konstantin and Martinkus, Karolis and Perraudin, Nathanaël and Wattenhofer, Roger},
  date = {2023-08-15},
  eprint = {2210.01549},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2210.01549},
  url = {http://arxiv.org/abs/2210.01549},
  urldate = {2024-02-25},
  abstract = {Denoising diffusion probabilistic models and score-matching models have proven to be very powerful for generative tasks. While these approaches have also been applied to the generation of discrete graphs, they have, so far, relied on continuous Gaussian perturbations. Instead, in this work, we suggest using discrete noise for the forward Markov process. This ensures that in every intermediate step the graph remains discrete. Compared to the previous approach, our experimental results on four datasets and multiple architectures show that using a discrete noising process results in higher quality generated samples indicated with an average MMDs reduced by a factor of 1.5. Furthermore, the number of denoising steps is reduced from 1000 to 32 steps, leading to a 30 times faster sampling procedure.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/3BI6QX46/Haefeli et al. - 2023 - Diffusion Models for Graphs Benefit From Discrete .pdf;/home/krawczuk/Zotero/storage/KWZIZH4I/2210.html}
}

@inproceedings{hakhamaneshiBagNetBerkeleyAnalog2019d,
  title = {{{BagNet}}: {{Berkeley Analog Generator}} with {{Layout Optimizer Boosted}} with {{Deep Neural Networks}}},
  shorttitle = {{{BagNet}}},
  booktitle = {2019 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}} ({{ICCAD}})},
  author = {Hakhamaneshi, Kourosh and Werblun, Nick and Abbeel, Pieter and Stojanović, Vladimir},
  date = {2019-11},
  pages = {1--8},
  issn = {1558-2434},
  doi = {10.1109/ICCAD45719.2019.8942062},
  url = {https://ieeexplore.ieee.org/abstract/document/8942062?casa_token=MmLlHOUsuEcAAAAA:PE70fQr7gDO_ggoIy96mlHokcVIK6qh4jdCCEAsJAudbyJpck8DrLxbSOuEbOda4xQU4DaWeJAk},
  urldate = {2024-02-24},
  abstract = {The discrepancy between post-layout and schematic simulation results continues to widen in analog design due in part to the domination of layout parasitics. This paradigm shift is forcing designers to adopt design methodologies that seamlessly integrate layout effects into the standard design flow. Hence, any simulation-based optimization framework should take into account time-consuming post-layout simulation results. This work presents a learning framework that learns to reduce the number of simulations of evolutionary-based combinatorial optimizers, using a DNN that discriminates against generated samples, before running simulations. Using this approach, the discriminator achieves at least two orders of magnitude improvement on sample efficiency for several large circuit examples including an optical link receiver layout.},
  eventtitle = {2019 {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}} ({{ICCAD}})},
  keywords = {Integrated circuit modeling,Layout,Measurement,Optimization,Sociology,Solid modeling,Statistics},
  file = {/home/krawczuk/Zotero/storage/NWYCDHQ2/Hakhamaneshi et al. - 2019 - BagNet Berkeley Analog Generator with Layout Opti.pdf;/home/krawczuk/Zotero/storage/QJL9Z99Q/8942062.html}
}

@article{hakhamaneshiPretrainingGraphNeural2023a,
  title = {Pretraining {{Graph Neural Networks}} for {{Few-Shot Analog Circuit Modeling}} and {{Design}}},
  author = {Hakhamaneshi, Kourosh and Nassar, Marcel and Phielipp, Mariano and Abbeel, Pieter and Stojanovic, Vladimir},
  date = {2023-07},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {7},
  pages = {2163--2173},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2022.3217421},
  url = {https://ieeexplore.ieee.org/abstract/document/9930855?casa_token=oEJI3VPE4T4AAAAA:_-NFX46KfVpWBpHgIGlFh450V-DbZxLtt0ImmW0KM2Yp6p459WUxUp8_u3mLpPoF68Yj1zhNNbs-},
  urldate = {2024-02-20},
  abstract = {Being able to predict the performance of circuits without running expensive simulations is a desired capability that can catalyze automated design. In this article, we present a supervised pretraining approach to learn circuit representations that can be adapted to new circuit topologies or unseen prediction tasks. We hypothesize that if we train a neural network (NN) that can predict the output direct current (dc) voltages of a wide range of circuit instances it will be forced to learn generalizable knowledge about the role of each circuit element and how they interact with each other. The dataset for this supervised learning objective can be easily collected at scale since the required dc simulation to get ground truth labels is relatively cheap. This representation would then be helpful for few-shot generalization to unseen circuit metrics that require more time-consuming simulations for obtaining the ground-truth labels. To cope with the variable topological structure of different circuits we describe each circuit as a graph and use graph NNs (GNNs) to learn node embeddings. We show that pretraining GNNs on prediction of output node voltages can encourage learning representations that can be adapted to new unseen topologies or prediction of new circuit-level properties with up to 10x more sample efficiency compared to a randomly initialized model. We further show that we can improve the sample efficiency of prior SoTA model-based optimization methods by 2\textbackslash times (almost as good as using an oracle model) via fintuning pretrained GNNs as the feature extractor of the learned models.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Adaptation models,Analog circuits,Circuit design,Feature extraction,graph neural networks (GNNs),Integrated circuit modeling,knowledge transfer,machine learning,Predictive models,pretraining,Task analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/6PE4IZT5/Hakhamaneshi et al. - 2023 - Pretraining Graph Neural Networks for Few-Shot Ana.pdf}
}

@article{hakhamaneshiPretrainingGraphNeural2023b,
  title = {Pretraining {{Graph Neural Networks}} for {{Few-Shot Analog Circuit Modeling}} and {{Design}}},
  author = {Hakhamaneshi, Kourosh and Nassar, Marcel and Phielipp, Mariano and Abbeel, Pieter and Stojanovic, Vladimir},
  date = {2023-07},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  volume = {42},
  number = {7},
  pages = {2163--2173},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2022.3217421},
  url = {https://ieeexplore.ieee.org/document/9930855/},
  urldate = {2024-02-20},
  abstract = {Being able to predict the performance of circuits without running expensive simulations is a desired capability that can catalyze automated design. In this article, we present a supervised pretraining approach to learn circuit representations that can be adapted to new circuit topologies or unseen prediction tasks. We hypothesize that if we train a neural network (NN) that can predict the output direct current (dc) voltages of a wide range of circuit instances it will be forced to learn generalizable knowledge about the role of each circuit element and how they interact with each other. The dataset for this supervised learning objective can be easily collected at scale since the required dc simulation to get ground truth labels is relatively cheap. This representation would then be helpful for few-shot generalization to unseen circuit metrics that require more time-consuming simulations for obtaining the ground-truth labels. To cope with the variable topological structure of different circuits we describe each circuit as a graph and use graph NNs (GNNs) to learn node embeddings. We show that pretraining GNNs on prediction of output node voltages can encourage learning representations that can be adapted to new unseen topologies or prediction of new circuit-level properties with up to 10x more sample efficiency compared to a randomly initialized model. We further show that we can improve the sample efficiency of prior SoTA model-based optimization methods by 2× (almost as good as using an oracle model) via fintuning pretrained GNNs as the feature extractor of the learned models.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/HKLTFE5U/Hakhamaneshi et al. - 2023 - Pretraining Graph Neural Networks for Few-Shot Ana.pdf}
}

@online{hanGeometricGraphRepresentation2022,
  title = {Geometric {{Graph Representation Learning}} via {{Maximizing Rate Reduction}}},
  author = {Han, Xiaotian and Jiang, Zhimeng and Liu, Ninghao and Song, Qingquan and Li, Jundong and Hu, Xia},
  date = {2022-02-13},
  eprint = {2202.06241},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2202.06241},
  url = {http://arxiv.org/abs/2202.06241},
  urldate = {2024-02-22},
  abstract = {Learning discriminative node representations benefits various downstream tasks in graph analysis such as community detection and node classification. Existing graph representation learning methods (e.g., based on random walk and contrastive learning) are limited to maximizing the local similarity of connected nodes. Such pair-wise learning schemes could fail to capture the global distribution of representations, since it has no explicit constraints on the global geometric properties of representation space. To this end, we propose Geometric Graph Representation Learning (G2R) to learn node representations in an unsupervised manner via maximizing rate reduction. In this way, G2R maps nodes in distinct groups (implicitly stored in the adjacency matrix) into different subspaces, while each subspace is compact and different subspaces are dispersedly distributed. G2R adopts a graph neural network as the encoder and maximizes the rate reduction with the adjacency matrix. Furthermore, we theoretically and empirically demonstrate that rate reduction maximization is equivalent to maximizing the principal angles between different subspaces. Experiments on real-world datasets show that G2R outperforms various baselines on node classification and community detection tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/YFN2KS5N/Han et al. - 2022 - Geometric Graph Representation Learning via Maximi.pdf;/home/krawczuk/Zotero/storage/XYCSEUM5/2202.html}
}

@thesis{hanICPhysicalDesign2018,
  title = {{{IC Physical Design Methodologies}} for {{Advanced Process Nodes}}},
  author = {Han, Kwangsoo},
  date = {2018},
  institution = {{UC San Diego}},
  url = {https://escholarship.org/uc/item/5rn717w9},
  urldate = {2024-02-22},
  abstract = {Next-generation applications in mobile, automotive, internet of things, robotic, artificial intelligence, etc. domains require the development and integration of advanced systems-on-chip (SOCs) that deliver ever-higher performance with much lower power. Thus, Moore's Law continues to be necessary, and innovations are needed beyond this law to help manage performance, power, area and cost (PPAC) for integrated-circuit (IC) design. Among the steps in the typical IC design flow, physical design implementation critically impacts PPAC. However, in concert with continuation of the Moore's Law trajectory, IC physical design encounters new challenges such as patterning restrictions due to manufacturing limits, severe process variation, and escalating interconnect RC delay. This thesis presents techniques to mitigate these challenges, grouped into three main thrusts: (i) manufacturing-aware design methodologies, (ii) process-aware design methodologies, and (iii) interconnect-aware design methodologies.Multiple-patterning techniques play a key role in the quest to print ever-smaller features for continued technology scaling in advanced nodes. However, the use of multiple-patterning significantly raises the number of extra steps for patterning as well as layout constraints needed for patternability; this causes an explosion of design rules and a loss of achievable layout density. To manage the onslaught of complex design rules arising from multiple-patterning, the manufacturing-aware design methodologies thrust of this thesis proposes approaches to optimize 2D block mask layout for minimum timing degradation, perform detailed placement to fix complex front-end-of-line (FEOL) design rule violations, and evaluate complex back-end-of-line (BEOL) design rule impact.Design variability due to manufacturing process variations has significant impact on the quality and yield of modern IC designs. Escalating process variation with new device architectures and manufacturing techniques (e.g., FinFET, multiple-patterning, etc.) required for node scaling results in the rapid increase of pessimism and overdesign. To mitigate the impact of severe process variation, the process-aware design methodologies thrust of this thesis presents approaches to optimize top-level clock tree for OCV minimization, reduce skew variation in the clock network, and perform partitioning in 3DIC that leverages a priori knowledge of inter-die variation.In advanced technology nodes, interconnect RC delay becomes more and more dominant. The continuous rapid increase of interconnect RC leads to not only performance loss from interconnect delay increase, but circuit power and area degradation as well, due to exponential increase in the number of buffers and drivers. To mitigate the escalating interconnect RC delay, the interconnect-aware design methodologies thrust of this thesis proposes approaches to co-optimize wirelength and pathlength in routing, studies optimal wirelength-skew tradeoff and remaining suboptimality in interconnect tree constructions, and performs optimal generalized H-tree construction with buffering.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CFBEU759/Han - 2018 - IC Physical Design Methodologies for Advanced Proc.pdf}
}

@article{harjaniOASYSFrameworkAnalog1989a,
  title = {{{OASYS}}: A Framework for Analog Circuit Synthesis},
  shorttitle = {{{OASYS}}},
  author = {Harjani, R. and Rutenbar, R.A. and Carley, L.R.},
  date = {1989-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {8},
  number = {12},
  pages = {1247--1266},
  issn = {1937-4151},
  doi = {10.1109/43.44506},
  url = {https://ieeexplore.ieee.org/abstract/document/44506?casa_token=lTwqQXBnIH0AAAAA:0FaLAkXT-oJOA1nktVcV9mJRtTxomg5HGxBugj6Pym51WPrJbu6lyi5C0v8YgfNBHKO5LQ_vHQx7},
  urldate = {2024-02-20},
  abstract = {A hierarchically structured framework for analog circuit synthesis is described. This hierarchical structure has two important features: it decomposes the design task into a sequence of smaller tasks with uniform structure, and it simplifies the reuse of design knowledge. Mechanisms are described that select from among alternate design styles and translate performance specifications from one level in the hierarchy to the next lower, more concrete level. A prototype implementation, OASYS, synthesizes sized transistor schematics for CMOS operational amplifiers from performance specifications and process parameters. Measurements from detailed circuit simulation and from actual fabricated analog ICs based on OASYS-synthesized designs demonstrate that OASYS is capable of synthesizing functional circuits.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Analog integrated circuits,Circuit synthesis,Circuit topology,CMOS process,Design automation,Digital signal processing,Process design,Prototypes,Software libraries},
  file = {/home/krawczuk/Zotero/storage/YEUNP8FI/Harjani et al. - 1989 - OASYS a framework for analog circuit synthesis.pdf;/home/krawczuk/Zotero/storage/EPW4MXHN/44506.html}
}

@article{haslerProgrammableAnalogSystem2024a,
  title = {Programmable {{Analog System Benchmarks Leading}} to {{Efficient Analog Computation Synthesis}}},
  author = {Hasler, Jennifer and Hao, Cong},
  date = {2024-01-27},
  journaltitle = {ACM Transactions on Reconfigurable Technology and Systems},
  shortjournal = {ACM Trans. Reconfigurable Technol. Syst.},
  volume = {17},
  number = {1},
  pages = {12:1--12:25},
  issn = {1936-7406},
  doi = {10.1145/3625298},
  url = {https://dl.acm.org/doi/10.1145/3625298},
  urldate = {2024-02-20},
  abstract = {This effort develops the first rich suite of analog and mixed-signal benchmark of various sizes and domains, intended for use with contemporary analog and mixed-signal designs and synthesis tools. Benchmarking enables analog-digital co-design exploration as well as extensive evaluation of analog synthesis tools and the generated analog/mixed-signal circuit or device. The goals of this effort are defining analog computation system benchmarks, developing the required concepts for higher-level analog and mixed-signal tools to utilize these benchmarks, and enabling future automated architectural design space exploration (DSE) to determine the best configurable architecture (e.g., a new FPAA) for a certain family of applications. The benchmarks comprise multiple levels of an acoustic, a vision, a communications, and an analog filter system that must be simultaneously satisfied for a complete system.},
  keywords = {Analog benchmarks,analog computing,analog system synthesis,mixed-signal HLS},
  file = {/home/krawczuk/Zotero/storage/K3AV5RYC/Hasler and Hao - 2024 - Programmable Analog System Benchmarks Leading to E.pdf}
}

@article{hawashReversibleCircuitSynthesis2020,
  title = {Reversible {{Circuit Synthesis Time Reduction Based}} on {{Subtree-Circuit Mapping}}},
  author = {Hawash, Amjad and Awad, Ahmed and Abdalhaq, Baker},
  date = {2020-01},
  journaltitle = {Applied Sciences},
  volume = {10},
  number = {12},
  pages = {4147},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-3417},
  doi = {10.3390/app10124147},
  url = {https://www.mdpi.com/2076-3417/10/12/4147},
  urldate = {2024-02-20},
  abstract = {Several works have been conducted regarding the reduction of the energy consumption in electrical circuits. Reversible circuit synthesis is considered to be one of the major efforts at reducing the amount of power consumption. The field of reversible circuit synthesis uses a large number of proposed algorithms to minimize the overall cost of circuits synthesis (represented in the line number and quantum cost), with minimal concern paid for synthesis time. However, because of the iterative nature of the synthesis optimization algorithms, synthesis time cannot be neglected as a parameter which needs to be tackled, especially for large-scale circuits which need to be realized by cascades of reversible gates. Reducing the synthesis cost can be achieved by Binary Decision Diagrams (BDDs), which are considered to be a step forward in this field. Nevertheless, the mapping of each BDD node into a cascade of reversible gates during the synthesis process is time-consuming. In this work, we implement the idea of the subtree-based mapping of BDD nodes to reversible gates instead of the classical nodal-based algorithm to effectively reduce the entire reversible circuit synthesis time. Considering Depth-First Search (DFS), we convert an entire BDD subtree in one step into a cascade of reversible gates. A look-up table for all possible combinations of subtrees and their corresponding reversible gates has been constructed, in which a hash key is used to directly access subtrees during the mapping process. This table is constructed as a result of a comprehensive study of all possible BDD subtrees and considered as a reference during the conversion process. The conducted experimental tests show a significant synthesis time reduction (around 95\% on average), preserving the correctness of the algorithm in generating a circuit realizing the required Boolean function.},
  issue = {12},
  langid = {english},
  keywords = {binary decision diagram (BDD),depth-first search (DFS),reversible circuits,subtrees,synthesis time reduction},
  file = {/home/krawczuk/Zotero/storage/TLJGLFCY/Hawash et al. - 2020 - Reversible Circuit Synthesis Time Reduction Based .pdf}
}

@article{hecherAdvancedToolsMethods2023,
  title = {Advanced Tools and Methods for Treewidth-Based Problem Solving},
  author = {Hecher, Markus},
  date = {2023-05-01},
  journaltitle = {it - Information Technology},
  volume = {65},
  number = {1-2},
  pages = {65--74},
  publisher = {{De Gruyter Oldenbourg}},
  issn = {2196-7032},
  doi = {10.1515/itit-2023-0004},
  url = {https://www.degruyter.com/document/doi/10.1515/itit-2023-0004/html},
  urldate = {2024-02-20},
  abstract = {Computer programs, so-called solvers, for solving the well-known Boolean satisfiability problem (S at ) have been improving for decades. Among the reasons, why these solvers are so fast, is the implicit usage of the formula’s structural properties during solving. One of such structural indicators is the so-called treewidth, which tries to measure how close a formula instance is to being easy (tree-like). This work focuses on logic-based problems and treewidth-based methods and tools for solving them. Many of these problems are also relevant for knowledge representation and reasoning (KR) as well as artificial intelligence (AI) in general. We present a new type of problem reduction, which is referred to by decomposition-guided ( DG ). This reduction type forms the basis to solve a problem for quantified Boolean formulas (QBFs) of bounded treewidth that has been open since 2004. The solution of this problem then gives rise to a new methodology for proving precise lower bounds for a range of further formalisms in logic, KR, and AI. Despite the established lower bounds, we implement an algorithm for solving extensions of S at efficiently, by directly using treewidth. Our implementation is based on finding abstractions of instances, which are then incrementally refined in the process. Thereby, our observations confirm that treewidth is an important measure that should be considered in the design of modern solvers.},
  langid = {english},
  keywords = {AI,ETH lower bounds,logic,parameterized complexity,quantitative reasoning,treewidth},
  file = {/home/krawczuk/Zotero/storage/5KBWR6BQ/Hecher - 2023 - Advanced tools and methods for treewidth-based pro.pdf}
}

@incollection{hicksBranchTreeDecomposition2005,
  title = {Branch and {{Tree Decomposition Techniques}} for {{Discrete Optimization}}},
  booktitle = {Emerging {{Theory}}, {{Methods}}, and {{Applications}}},
  author = {Hicks, Illya V. and Koster, Arie M. C. A. and Koloto?lu, Elif},
  date = {2005-09},
  series = {{{INFORMS TutORials}} in {{Operations Research}}},
  pages = {1--29},
  publisher = {{INFORMS}},
  doi = {10.1287/educ.1053.0017},
  url = {https://pubsonline.informs.org/doi/abs/10.1287/educ.1053.0017},
  urldate = {2024-02-20},
  isbn = {978-1-877640-21-6},
  keywords = {branchwidth,combinatorial optimization,graph algorithms,treewidth},
  file = {/home/krawczuk/Zotero/storage/8YXIPCY7/Hicks et al. - 2005 - Branch and Tree Decomposition Techniques for Discr.pdf}
}

@inproceedings{hoDenoisingDiffusionProbabilistic2020f,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2024-02-25},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/4VZV9L5I/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@inproceedings{hodgkinsonStochasticContinuousNormalizing2021,
  title = {Stochastic Continuous Normalizing Flows: Training {{SDEs}} as {{ODEs}}},
  shorttitle = {Stochastic Continuous Normalizing Flows},
  booktitle = {Proceedings of the {{Thirty-Seventh Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Hodgkinson, Liam and family=Heide, given=Chris, prefix=van der, useprefix=false and Roosta, Fred and Mahoney, Michael W.},
  date = {2021-12-01},
  pages = {1130--1140},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v161/hodgkinson21a.html},
  urldate = {2024-02-22},
  abstract = {We provide a general theoretical framework for stochastic continuous normalizing flows, an extension of continuous normalizing flows for density estimation of stochastic differential equations (SDEs). Using the theory of rough paths, the underlying Brownian motion is treated as a latent variable and approximated. Doing so enables the treatment of SDEs as random ordinary differential equations, which can be trained using existing techniques. For scalar loss functions, this approach naturally recovers the stochastic adjoint method of Li et al. [2020] for training neural SDEs, while supporting a more flexible class of approximations.},
  eventtitle = {Uncertainty in {{Artificial Intelligence}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CZW8X9GU/Hodgkinson et al. - 2021 - Stochastic continuous normalizing flows training .pdf}
}

@online{hodgkinsonStochasticNormalizingFlows2020,
  title = {Stochastic {{Normalizing Flows}}},
  author = {Hodgkinson, Liam and family=Heide, given=Chris, prefix=van der, useprefix=true and Roosta, Fred and Mahoney, Michael W.},
  date = {2020-02-25},
  eprint = {2002.09547},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2002.09547},
  url = {http://arxiv.org/abs/2002.09547},
  urldate = {2024-02-22},
  abstract = {We introduce stochastic normalizing flows, an extension of continuous normalizing flows for maximum likelihood estimation and variational inference (VI) using stochastic differential equations (SDEs). Using the theory of rough paths, the underlying Brownian motion is treated as a latent variable and approximated, enabling efficient training of neural SDEs as random neural ordinary differential equations. These SDEs can be used for constructing efficient Markov chains to sample from the underlying distribution of a given dataset. Furthermore, by considering families of targeted SDEs with prescribed stationary distribution, we can apply VI to the optimization of hyperparameters in stochastic MCMC.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/2SQHKHFX/Hodgkinson et al. - 2020 - Stochastic Normalizing Flows.pdf;/home/krawczuk/Zotero/storage/CWJGEQ4G/2002.html}
}

@inproceedings{hoogeboomEquivariantDiffusionMolecule2022d,
  title = {Equivariant {{Diffusion}} for {{Molecule Generation}} in {{3D}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Hoogeboom, Emiel and Satorras, Vı́ctor Garcia and Vignac, Clément and Welling, Max},
  date = {2022-06-28},
  pages = {8867--8887},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/hoogeboom22a.html},
  urldate = {2024-02-22},
  abstract = {This work introduces a diffusion model for molecule generation in 3D that is equivariant to Euclidean transformations. Our E(3) Equivariant Diffusion Model (EDM) learns to denoise a diffusion process with an equivariant network that jointly operates on both continuous (atom coordinates) and categorical features (atom types). In addition, we provide a probabilistic analysis which admits likelihood computation of molecules using our model. Experimentally, the proposed method significantly outperforms previous 3D molecular generative methods regarding the quality of generated samples and the efficiency at training time.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/58APPAE8/Hoogeboom et al. - 2022 - Equivariant Diffusion for Molecule Generation in 3.pdf}
}

@online{hornTopologicalGraphNeural2021,
  title = {Topological {{Graph Neural Networks}}},
  author = {Horn, Max and De Brouwer, Edward and Moor, Michael and Moreau, Yves and Rieck, Bastian and Borgwardt, Karsten},
  date = {2021-02-15},
  url = {https://arxiv.org/abs/2102.07835v4},
  urldate = {2024-02-20},
  abstract = {Graph neural networks (GNNs) are a powerful architecture for tackling graph learning tasks, yet have been shown to be oblivious to eminent substructures such as cycles. We present TOGL, a novel layer that incorporates global topological information of a graph using persistent homology. TOGL can be easily integrated into any type of GNN and is strictly more expressive (in terms the Weisfeiler--Lehman graph isomorphism test) than message-passing GNNs. Augmenting GNNs with TOGL leads to improved predictive performance for graph and node classification tasks, both on synthetic data sets, which can be classified by humans using their topology but not by ordinary GNNs, and on real-world data.},
  langid = {english},
  organization = {{arXiv.org}},
  file = {/home/krawczuk/Zotero/storage/M3Q6YB6N/Horn et al. - 2021 - Topological Graph Neural Networks.pdf}
}

@patent{hoyerNeuralReparameterizationOptimization2023,
  type = {patentus},
  title = {Neural Reparameterization for Optimization of Physical Designs},
  author = {Hoyer, Stephan Owen Steele and Sohl-Dickstein, Jascha Narain and Greydanus, Samuel James},
  holder = {{Google LLC}},
  date = {2023-02-07},
  number = {11574093B2},
  url = {https://patents.google.com/patent/US11574093B2/en},
  urldate = {2024-02-22},
  langid = {english},
  keywords = {design,learned model,machine,physical,solution},
  file = {/home/krawczuk/Zotero/storage/TC7WLA4R/Hoyer et al. - 2023 - Neural reparameterization for optimization of phys.pdf}
}

@article{huangBayesianOptimizationApproach2022,
  title = {Bayesian {{Optimization Approach}} for {{RF Circuit Synthesis}} via {{Multitask Neural Network Enhanced Gaussian Process}}},
  author = {Huang, Jiangli and Tao, Cong and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-11},
  journaltitle = {IEEE Transactions on Microwave Theory and Techniques},
  shortjournal = {IEEE Trans. Microwave Theory Techn.},
  volume = {70},
  number = {11},
  pages = {4787--4795},
  issn = {0018-9480, 1557-9670},
  doi = {10.1109/TMTT.2022.3194980},
  url = {https://ieeexplore.ieee.org/document/9852008/},
  urldate = {2024-02-20},
  abstract = {An RF integrated circuit design heavily relies upon experienced experts to iteratively tune the circuit parameters. A Bayesian optimization (BO) method is explored in existing works for automated analog and RF circuit synthesis. The overall performance can be further improved by constructing a model to exploit the correlation among different circuit specifications. In this article, we propose a BO approach for RF circuit synthesis via a multitask neural network enhanced Gaussian process (MTNN-GP). We present a novel multioutput GP model, in which the kernel functions of multiple outputs are constructed from a multitask neural network with shared hidden layers and taskspecific layers. Therefore, the correlation between the outputs can be captured by the shared hidden layers. Our proposed MTNN-GP-based BO method is compared with several stateof-the-art BO methods on three real word RF circuits and achieves best performance. The experimental results demonstrate the effectiveness and efficiency of our proposed method.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/G4EQSFQD/Huang et al. - 2022 - Bayesian Optimization Approach for RF Circuit Synt.pdf}
}

@article{huangMachineLearningElectronic2021g,
  title = {Machine {{Learning}} for {{Electronic Design Automation}}: {{A Survey}}},
  shorttitle = {Machine {{Learning}} for {{Electronic Design Automation}}},
  author = {Huang, Guyue and Hu, Jingbo and He, Yifan and Liu, Jialong and Ma, Mingyuan and Shen, Zhaoyang and Wu, Juejian and Xu, Yuanfan and Zhang, Hengrui and Zhong, Kai and Ning, Xuefei and Ma, Yuzhe and Yang, Haoyu and Yu, Bei and Yang, Huazhong and Wang, Yu},
  date = {2021-06-05},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {26},
  number = {5},
  pages = {40:1--40:46},
  issn = {1084-4309},
  doi = {10.1145/3451179},
  url = {https://dl.acm.org/doi/10.1145/3451179},
  urldate = {2024-02-24},
  abstract = {With the down-scaling of CMOS technology, the design complexity of very large-scale integrated is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 1990s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interest in incorporating ML to solve EDA tasks. In this article, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.},
  keywords = {Electronic design automation,machine learning,neural networks},
  file = {/home/krawczuk/Zotero/storage/SYNNP5JH/Huang et al. - 2021 - Machine Learning for Electronic Design Automation.pdf}
}

@article{huttonAutomaticGenerationSynthetic2002,
  title = {Automatic Generation of Synthetic Sequential Benchmark Circuits},
  author = {Hutton, M.D. and Rose, J.S. and Corneil, D.G.},
  date = {2002-08},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {21},
  number = {8},
  pages = {928--940},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2002.800456},
  url = {https://ieeexplore.ieee.org/abstract/document/1020350?casa_token=_xZfijPDv2kAAAAA:AzZcTFlVzzkSVUCb-P77q6Jg_VA3leMyffgFkCSWXmLugdY5bzZ1soIq4-9C-AGvLCwBl2XVajPl},
  urldate = {2024-02-19},
  abstract = {The design of programmable logic architectures and supporting computer-aided design tools fundamentally requires both a good understanding of the combinatorial nature of netlist graphs and sufficient quantities of realistic examples to evaluate or benchmark the results. In this paper, the authors investigate these two issues. They introduce an abstract model for describing sequential circuits and a collection of statistical parameters for better understanding the nature of circuits. Based upon this model they introduce and formally define the signature of a circuit netlist and the signature equivalence of netlists. They give an algorithm (GEN) for generating sequential benchmark netlists, significantly expanding previous work (Hutton et al, 1998) which generated purely combinational circuits. By comparing synthetic circuits to existing benchmarks and random graphs they show that GEN circuits are significantly more realistic than random graphs. The authors further illustrate the viabilty of the methodology by applying GEN to a case study comparing two partitioning algorithms.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Benchmark testing,Character generation,Circuit testing,Design automation,Logic circuits,Logic design,Partitioning algorithms,Programmable logic arrays,Programmable logic devices,Routing},
  file = {/home/krawczuk/Zotero/storage/DETIVN87/Hutton et al. - 2002 - Automatic generation of synthetic sequential bench.pdf;/home/krawczuk/Zotero/storage/QBVJA95Z/1020350.html}
}

@inproceedings{ishihataBagBasedSearchMetaAlgorithm2024,
  title = {The {{Bag-Based Search}}: {{A Meta-Algorithm}} to~{{Construct Tractable Logical Circuits}} for~{{Graphs Based}} on~{{Tree Decomposition}}},
  shorttitle = {The {{Bag-Based Search}}},
  booktitle = {Combinatorial {{Optimization}} and {{Applications}}},
  author = {Ishihata, Masakazu},
  editor = {Wu, Weili and Guo, Jianxiong},
  date = {2024},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {337--350},
  publisher = {{Springer Nature Switzerland}},
  location = {{Cham}},
  doi = {10.1007/978-3-031-49614-1_25},
  abstract = {Tractable logical circuits (TLCs) have attracted more attention in the AI field as bases of knowledge representation and tractable probabilistic modeling. We propose the bag-based search (BBS), a new meta-algorithm for constructing a TLC that accepts all subgraphs of a given input graph that satisfies a target graph property. We implemented BBS examples for various graph properties, including independent set, k-edgeset, dominating set, k-matchings, and spanning trees, and applied them to artificial and real-world graphs. The experimental results showed that BBS generated significantly smaller circuits than ZDDs obtained by the frontier-based search (FBS).},
  isbn = {978-3-031-49614-1},
  langid = {english}
}

@inproceedings{ivanovaReinforcementLearningDesign2022,
  title = {Reinforcement {{Learning}} at {{Design}} of {{Electronic Circuits}}: {{Review}} and {{Analysis}}},
  shorttitle = {Reinforcement {{Learning}} at {{Design}} of {{Electronic Circuits}}},
  booktitle = {Proceedings of the 2022 5th {{Artificial Intelligence}} and {{Cloud Computing Conference}}},
  author = {Ivanova, Malinka and Rozeva, Anna and Ninov, Angel and Stosovic, Miona Andrejevic},
  date = {2022-12-17},
  pages = {275--284},
  publisher = {{ACM}},
  location = {{Osaka Japan}},
  doi = {10.1145/3582099.3582140},
  url = {https://dl.acm.org/doi/10.1145/3582099.3582140},
  urldate = {2024-02-19},
  eventtitle = {{{AICCC}} 2022: 2022 5th {{Artificial Intelligence}} and {{Cloud Computing Conference}}},
  isbn = {978-1-4503-9874-9},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CM5RLSQJ/Ivanova et al. - 2022 - Reinforcement Learning at Design of Electronic Cir.pdf}
}

@inproceedings{jaegerSimpleLatentVariable2023,
  title = {A {{Simple Latent Variable Model}} for {{Graph Learning}} and {{Inference}}},
  author = {Jaeger, Manfred and Longa, Antonio and Azzolin, Steve and Schulte, Oliver and Passerini, Andrea},
  date = {2023-11-25},
  url = {https://openreview.net/forum?id=S9jem2KZVr},
  urldate = {2024-02-20},
  abstract = {We introduce a probabilistic latent variable model for graphs that generalizes both the established graphon and stochastic block models. This naive histogram AHK model is simple and versatile, and we demonstrate its use for disparate tasks including complex predictive inference usually not supported by other approaches, and graph generation. We analyze the tradeoffs entailed by the simplicity of the model, which imposes certain limitations on expressivity on the one hand, but on the other hand leads to robust generalization capabilities to graph sizes different from what was seen in the training data.},
  eventtitle = {The {{Second Learning}} on {{Graphs Conference}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/2UKK7NJ4/Jaeger et al. - 2023 - A Simple Latent Variable Model for Graph Learning .pdf}
}

@inproceedings{jangSimpleScalableRepresentation2023a,
  title = {A {{Simple}} and {{Scalable Representation}} for {{Graph Generation}}},
  author = {Jang, Yunhui and Lee, Seul and Ahn, Sungsoo},
  date = {2023-12-21},
  url = {https://openreview.net/forum?id=wjVKHEPoU2},
  urldate = {2024-02-20},
  abstract = {Recently, there has been a surge of interest in employing neural networks for graph generation, a fundamental statistical learning problem with critical applications like molecule design and community analysis. However, most approaches encounter significant limitations when generating large-scale graphs. This is due to their requirement to output the full adjacency matrices whose size grows quadratically with the number of nodes. In response to this challenge, we introduce a new, simple, and scalable graph representation named gap encoded edge list (GEEL) that has a small representation size that aligns with the number of edges. In addition, GEEL significantly reduces the vocabulary size by incorporating the gap encoding and bandwidth restriction schemes. GEEL can be autoregressively generated with the incorporation of node positional encoding, and we further extend GEEL to deal with attributed graphs by designing a new grammar. Our findings reveal that the adoption of this compact representation not only enhances scalability but also bolsters performance by simplifying the graph generation process. We conduct a comprehensive evaluation across ten non-attributed and two molecular graph generation tasks, demonstrating the effectiveness of GEEL.},
  eventtitle = {{{NeurIPS}} 2023 {{Workshop}}: {{New Frontiers}} in {{Graph Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/Y6V429HW/Jang et al. - 2023 - A Simple and Scalable Representation for Graph Gen.pdf}
}

@inproceedings{jinEMGraphFastLearningBased2021a,
  title = {{{EMGraph}}: {{Fast Learning-Based Electromigration Analysis}} for {{Multi-Segment Interconnect Using Graph Convolution Networks}}},
  shorttitle = {{{EMGraph}}},
  booktitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  author = {Jin, Wentian and Chen, Liang and Sadiqbatcha, Sheriff and Peng, Shaoyi and Tan, Sheldon X.-D.},
  date = {2021-12},
  pages = {919--924},
  issn = {0738-100X},
  doi = {10.1109/DAC18074.2021.9586239},
  url = {https://ieeexplore.ieee.org/abstract/document/9586239?casa_token=aPtfgBTKzREAAAAA:dWCqyGFU9oJLp2WomDQDeX_SSivmmB0zTFY9nPzO2I7PjEGCKXEwvSh89lwPi4KBHbZauYItMAoi},
  urldate = {2024-02-22},
  abstract = {Electromigration (EM) becomes a major concern for VLSI circuits as the technology advances in the nanometer regime. With Korhonen equations, EM assessment for VLSI circuits remains challenged due to the increasing integrated density. VLSI multisegment interconnect trees can be naturally viewed as graphs. Based on this observation, we propose a new graph convolution network (GCN) model, which is called EMGraph considering both node and edge embedding features, to estimate the transient EM stress of interconnect trees. Compared with recently proposed generative adversarial network (GAN) based stress image-generation method, EMGraph model can learn more transferable knowledge to predict stress distributions on new graphs without retraining via inductive learning. Trained on the large dataset, the model shows less than 1.5\% averaged error compared to the ground truth results and is orders of magnitude faster than both COMSOL and state-of-the-art method. It also achieves smaller model size, 4\textbackslash times accuracy and 14\textbackslash times speedup over the GAN-based method.},
  eventtitle = {2021 58th {{ACM}}/{{IEEE Design Automation Conference}} ({{DAC}})},
  keywords = {Convolution,Electromigration,Electromigration (EM),Generative adversarial networks,graph convolution network (GCN),hydrostatic stress assessment,Image edge detection,Integrated circuit interconnections,multisegment interconnect,Predictive models,Very large scale integration},
  file = {/home/krawczuk/Zotero/storage/HCNGD9JY/Jin et al. - 2021 - EMGraph Fast Learning-Based Electromigration Anal.pdf;/home/krawczuk/Zotero/storage/T5GR7I2E/9586239.html}
}

@inproceedings{jingHDMIHighorderDeep2021,
  title = {{{HDMI}}: {{High-order Deep Multiplex Infomax}}},
  shorttitle = {{{HDMI}}},
  booktitle = {Proceedings of the {{Web Conference}} 2021},
  author = {Jing, Baoyu and Park, Chanyoung and Tong, Hanghang},
  date = {2021-06-03},
  series = {{{WWW}} '21},
  pages = {2414--2424},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3442381.3449971},
  url = {https://dl.acm.org/doi/10.1145/3442381.3449971},
  urldate = {2024-02-22},
  abstract = {Networks have been widely used to represent the relations between objects such as academic networks and social networks, and learning embedding for networks has thus garnered plenty of research attention. Self-supervised network representation learning aims at extracting node embedding without external supervision. Recently, maximizing the mutual information between the local node embedding and the global summary (e.g. Deep Graph Infomax, or DGI for short) has shown promising results on many downstream tasks such as node classification. However, there are two major limitations of DGI. Firstly, DGI merely considers the extrinsic supervision signal (i.e., the mutual information between node embedding and global summary) while ignores the intrinsic signal (i.e., the mutual dependence between node embedding and node attributes). Secondly, nodes in a real-world network are usually connected by multiple edges with different relations, while DGI does not fully explore the various relations among nodes. To address the above-mentioned problems, we propose a novel framework, called High-order Deep Multiplex Infomax (HDMI), for learning node embedding on multiplex networks in a self-supervised way. To be more specific, we first design a joint supervision signal containing both extrinsic and intrinsic mutual information by high-order mutual information, and we propose a High-order Deep Infomax (HDI) to optimize the proposed supervision signal. Then we propose an attention based fusion module to combine node embedding from different layers of the multiplex network. Finally, we evaluate the proposed HDMI on various downstream tasks such as unsupervised clustering and supervised classification. The experimental results show that HDMI achieves state-of-the-art performance on these tasks.},
  isbn = {978-1-4503-8312-7},
  keywords = {High-order Mutual Information,Multiplex Networks,Network Representation Learning},
  file = {/home/krawczuk/Zotero/storage/N624ARPD/Jing et al. - 2021 - HDMI High-order Deep Multiplex Infomax.pdf}
}

@inproceedings{joScorebasedGenerativeModeling2022g,
  title = {Score-Based {{Generative Modeling}} of {{Graphs}} via the {{System}} of {{Stochastic Differential Equations}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Jo, Jaehyeong and Lee, Seul and Hwang, Sung Ju},
  date = {2022-06-28},
  pages = {10362--10383},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/jo22a.html},
  urldate = {2024-02-25},
  abstract = {Generating graph-structured data requires learning the underlying distribution of graphs. Yet, this is a challenging problem, and the previous graph generative methods either fail to capture the permutation-invariance property of graphs or cannot sufficiently model the complex dependency between nodes and edges, which is crucial for generating real-world graphs such as molecules. To overcome such limitations, we propose a novel score-based generative model for graphs with a continuous-time framework. Specifically, we propose a new graph diffusion process that models the joint distribution of the nodes and edges through a system of stochastic differential equations (SDEs). Then, we derive novel score matching objectives tailored for the proposed diffusion process to estimate the gradient of the joint log-density with respect to each component, and introduce a new solver for the system of SDEs to efficiently sample from the reverse diffusion process. We validate our graph generation method on diverse datasets, on which it either achieves significantly superior or competitive performance to the baselines. Further analysis shows that our method is able to generate molecules that lie close to the training distribution yet do not violate the chemical valency rule, demonstrating the effectiveness of the system of SDEs in modeling the node-edge relationships.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/I2RVC8UK/Jo et al. - 2022 - Score-based Generative Modeling of Graphs via the .pdf}
}

@inproceedings{kabaEquivarianceLearnedCanonicalization2023b,
  title = {Equivariance with {{Learned Canonicalization Functions}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Kaba, Sékou-Oumar and Mondal, Arnab Kumar and Zhang, Yan and Bengio, Yoshua and Ravanbakhsh, Siamak},
  date = {2023-07-03},
  pages = {15546--15566},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/kaba23a.html},
  urldate = {2024-02-20},
  abstract = {Symmetry-based neural networks often constrain the architecture in order to achieve invariance or equivariance to a group of transformations. In this paper, we propose an alternative that avoids this architectural constraint by learning to produce canonical representations of the data. These canonicalization functions can readily be plugged into non-equivariant backbone architectures. We offer explicit ways to implement them for some groups of interest. We show that this approach enjoys universality while providing interpretable insights. Our main hypothesis, supported by our empirical results, is that learning a small neural network to perform canonicalization is better than using predefined heuristics. Our experiments show that learning the canonicalization function is competitive with existing techniques for learning equivariant functions across many tasks, including image classification, NNN-body dynamics prediction, point cloud classification and part segmentation, while being faster across the board.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/9AWGFSE5/Kaba et al. - 2023 - Equivariance with Learned Canonicalization Functio.pdf}
}

@online{kabaSymmetryBreakingEquivariant2023,
  title = {Symmetry {{Breaking}} and {{Equivariant Neural Networks}}},
  author = {Kaba, Sékou-Oumar and Ravanbakhsh, Siamak},
  date = {2023-12-14},
  eprint = {2312.09016},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2312.09016},
  url = {http://arxiv.org/abs/2312.09016},
  urldate = {2024-02-20},
  abstract = {Using symmetry as an inductive bias in deep learning has been proven to be a principled approach for sample-efficient model design. However, the relationship between symmetry and the imperative for equivariance in neural networks is not always obvious. Here, we analyze a key limitation that arises in equivariant functions: their incapacity to break symmetry at the level of individual data samples. In response, we introduce a novel notion of 'relaxed equivariance' that circumvents this limitation. We further demonstrate how to incorporate this relaxation into equivariant multilayer perceptrons (E-MLPs), offering an alternative to the noise-injection method. The relevance of symmetry breaking is then discussed in various application domains: physics, graph representation learning, combinatorial optimization and equivariant decoding.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/RNVHCACP/Kaba and Ravanbakhsh - 2023 - Symmetry Breaking and Equivariant Neural Networks.pdf;/home/krawczuk/Zotero/storage/89E3Q7W5/2312.html}
}

@inproceedings{kenarangiPySynRapidSynthesis2021,
  title = {{{PySyn}}: {{A Rapid Synthesis}} for {{Mixed-Signal Machine Learning Classification}}},
  shorttitle = {{{PySyn}}},
  booktitle = {2021 {{IEEE International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  author = {Kenarangi, Farid and Partin-Vaisband, Inna},
  date = {2021-08},
  pages = {712--717},
  issn = {1558-3899},
  doi = {10.1109/MWSCAS47672.2021.9531745},
  url = {https://ieeexplore.ieee.org/abstract/document/9531745?casa_token=haHEH4bc_YQAAAAA:EziHYGmuknS-P2eqVUGSo7yum2miETOxw1IAkiTB8ATNWw9LXSZ-9xYg4xzqyJ6FoS-IEDzb2u7u},
  urldate = {2024-02-20},
  abstract = {Mixed-signal integrated circuits (ICs) for machine learning (ML) have been demonstrated as a powerful tool for efficient and accurate classification of large volumes of complex data. Despite the growing interest in ML ICs, the design process of mixed-signal ML classifiers is dominated by ad hoc approaches. In this paper, a rapid synthesizer is developed in Python (PySyn) for designing compact power-efficient high-performance ML classifiers. Circuit-level ML library is designed and leveraged within the flow. System-level tradeoffs are generated with PySyn and utilized to iteratively adjust the ML performance. PySyn is demonstrated with a state-of-the-art classifier, generating optimized netlists under input constraints.},
  eventtitle = {2021 {{IEEE International Midwest Symposium}} on {{Circuits}} and {{Systems}} ({{MWSCAS}})},
  keywords = {Circuit topology,Circuits and systems,classification,Integrated circuits,machine learning,Machine learning,mixed-signal,synthesis,Synthesizers,System performance,Tools},
  file = {/home/krawczuk/Zotero/storage/NCICRTH8/Kenarangi and Partin-Vaisband - 2021 - PySyn A Rapid Synthesis for Mixed-Signal Machine .pdf}
}

@online{kianiHardnessLearningSymmetries2024b,
  title = {On the Hardness of Learning under Symmetries},
  author = {Kiani, Bobak T. and Le, Thien and Lawrence, Hannah and Jegelka, Stefanie and Weber, Melanie},
  date = {2024-01-03},
  eprint = {2401.01869},
  eprinttype = {arxiv},
  eprintclass = {cs, math, stat},
  doi = {10.48550/arXiv.2401.01869},
  url = {http://arxiv.org/abs/2401.01869},
  urldate = {2024-02-24},
  abstract = {We study the problem of learning equivariant neural networks via gradient descent. The incorporation of known symmetries ("equivariance") into neural nets has empirically improved the performance of learning pipelines, in domains ranging from biology to computer vision. However, a rich yet separate line of learning theoretic research has demonstrated that actually learning shallow, fully-connected (i.e. non-symmetric) networks has exponential complexity in the correlational statistical query (CSQ) model, a framework encompassing gradient descent. In this work, we ask: are known problem symmetries sufficient to alleviate the fundamental hardness of learning neural nets with gradient descent? We answer this question in the negative. In particular, we give lower bounds for shallow graph neural networks, convolutional networks, invariant polynomials, and frame-averaged networks for permutation subgroups, which all scale either superpolynomially or exponentially in the relevant input dimension. Therefore, in spite of the significant inductive bias imparted via symmetry, actually learning the complete classes of functions represented by equivariant neural networks via gradient descent remains hard.},
  pubstate = {preprint},
  keywords = {Computer Science - Data Structures and Algorithms,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/VLEHCD5U/Kiani et al. - 2024 - On the hardness of learning under symmetries.pdf;/home/krawczuk/Zotero/storage/8Q27DSLZ/2401.html}
}

@inproceedings{kimMachineLearningFramework2021a,
  title = {Machine {{Learning Framework}} for {{Early Routability Prediction}} with {{Artificial Netlist Generator}}},
  booktitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Kim, Daeyeon and Kwon, Hyunjeong and Lee, Sung-Yun and Kim, Seungwon and Woo, Mingyu and Kang, Seokhyeong},
  date = {2021-02},
  pages = {1809--1814},
  issn = {1558-1101},
  doi = {10.23919/DATE51398.2021.9473966},
  url = {https://ieeexplore.ieee.org/abstract/document/9473966?casa_token=t0B7SSVXd0IAAAAA:1EhnWWeR6DNGZj4w3PZz2Ft2nnlfnW083G8QYqSq3uXiEVNRj0mqWOHkE1aUtKTQ5-6prWfdTrz4},
  urldate = {2024-02-19},
  abstract = {Recent routability research has exploited a machine learning (ML)-based modeling methodologies to consider various routability factors that are derived from placement solution. These factors are very related to the circuit characteristics (e.g., pin density, routing congestion, demand of routing resources, etc), and lack of circuit benchmarks in training can lead to poor predictability for ‘unseen’ circuit designs. In this paper, we propose a machine learning (ML) framework for early routability prediction modeling. The method includes a new artificial netlist generator (ANG) that generates an artificial gate-level netlist from the user-specified topology characteristics of synthetic circuit, even with real world circuit-like. In this framework, we exploit that ANG that supports obtaining ground truths for use in training ML-based model, the training dataset that have a wide range of topological characteristics provides strong ability to inference noisy, previous-unseen data. Compared to a design-specific training dataset [4] that is used for routability prediction modeling, we increase the test accuracy of binary classification (‘pass' or ‘fail’) on timing, DRC and routability by 6.3\%, 8.6\% and 6.6\%, and reduce the generalization error [12] by as much as 87\% compared to design-specific training dataset [4].},
  eventtitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {Benchmark testing,Generators,Machine learning,Predictive models,Routing,Topology,Training},
  file = {/home/krawczuk/Zotero/storage/9FCLBQTI/Kim et al. - 2021 - Machine Learning Framework for Early Routability P.pdf;/home/krawczuk/Zotero/storage/7JJNE8G4/9473966.html}
}

@online{kipfSemiSupervisedClassificationGraph2017b,
  title = {Semi-{{Supervised Classification}} with {{Graph Convolutional Networks}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017-02-22},
  eprint = {1609.02907},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1609.02907},
  url = {http://arxiv.org/abs/1609.02907},
  urldate = {2024-02-24},
  abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/TNIH769F/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf;/home/krawczuk/Zotero/storage/QTLHGVRV/1609.html}
}

@online{kipfVariationalGraphAutoEncoders2016b,
  title = {Variational {{Graph Auto-Encoders}}},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2016-11-21},
  eprint = {1611.07308},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1611.07308},
  url = {http://arxiv.org/abs/1611.07308},
  urldate = {2024-02-24},
  abstract = {We introduce the variational graph auto-encoder (VGAE), a framework for unsupervised learning on graph-structured data based on the variational auto-encoder (VAE). This model makes use of latent variables and is capable of learning interpretable latent representations for undirected graphs. We demonstrate this model using a graph convolutional network (GCN) encoder and a simple inner product decoder. Our model achieves competitive results on a link prediction task in citation networks. In contrast to most existing models for unsupervised learning on graph-structured data and link prediction, our model can naturally incorporate node features, which significantly improves predictive performance on a number of benchmark datasets.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/RV4ZX2MH/Kipf and Welling - 2016 - Variational Graph Auto-Encoders.pdf;/home/krawczuk/Zotero/storage/JCTFT3HQ/1611.html}
}

@article{koblahSurveyPerspectiveArtificial2023,
  title = {A {{Survey}} and {{Perspective}} on {{Artificial Intelligence}} for {{Security-Aware Electronic Design Automation}}},
  author = {Koblah, David and Acharya, Rabin and Capecci, Daniel and Dizon-Paradis, Olivia and Tajik, Shahin and Ganji, Fatemeh and Woodard, Damon and Forte, Domenic},
  date = {2023-03-06},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {28},
  number = {2},
  pages = {16:1--16:57},
  issn = {1084-4309},
  doi = {10.1145/3563391},
  url = {https://doi.org/10.1145/3563391},
  urldate = {2024-02-19},
  abstract = {Artificial intelligence (AI) and machine learning (ML) techniques have been increasingly used in several fields to improve performance and the level of automation. In recent years, this use has exponentially increased due to the advancement of high-performance computing and the ever increasing size of data. One of such fields is that of hardware design—specifically the design of digital and analog integrated circuits, where AI/ ML techniques have been extensively used to address ever-increasing design complexity, aggressive time to market, and the growing number of ubiquitous interconnected devices. However, the security concerns and issues related to integrated circuit design have been highly overlooked. In this article, we summarize the state-of-the-art in AI/ML for circuit design/optimization, security and engineering challenges, research in security-aware computer-aided design/electronic design automation, and future research directions and needs for using AI/ML for security-aware circuit design.},
  keywords = {deep learning,Integrated circuit,reinforcement learning,security primitive},
  file = {/home/krawczuk/Zotero/storage/MP4XKSSE/Koblah et al. - 2023 - A Survey and Perspective on Artificial Intelligenc.pdf}
}

@article{kofinasLatentFieldDiscovery2024,
  title = {Latent {{Field Discovery}} in {{Interacting Dynamical Systems}} with {{Neural Fields}}},
  author = {Kofinas, Miltiadis (Miltos) and Bekkers, Erik and Nagaraja, Naveen and Gavves, Efstratios},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/6521bd47ebaa28228cd6c74cb85afb65-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/GD4FZSWM/Kofinas et al. - 2024 - Latent Field Discovery in Interacting Dynamical Sy.pdf}
}

@inproceedings{kohlerSmoothNormalizingFlows2021,
  title = {Smooth {{Normalizing Flows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Köhler, Jonas and Krämer, Andreas and Noe, Frank},
  date = {2021},
  volume = {34},
  pages = {2796--2809},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/167434fa6219316417cd4160c0c5e7d2-Abstract.html},
  urldate = {2024-02-22},
  abstract = {Normalizing flows are a promising tool for modeling probability distributions in physical systems. While state-of-the-art flows accurately approximate distributions and energies, applications in physics additionally require smooth energies to compute forces and higher-order derivatives. Furthermore, such densities are often defined on non-trivial topologies. A recent example are Boltzmann Generators for generating 3D-structures of peptides and small proteins. These generative models leverage the space of internal coordinates (dihedrals, angles, and bonds), which is a product of hypertori and compact intervals. In this work, we introduce a class of smooth mixture transformations working on both compact intervals and hypertori.Mixture transformations employ root-finding methods to invert them in practice, which has so far prevented bi-directional flow training. To this end, we show that parameter gradients and forces of such inverses can be computed from forward evaluations via the inverse function theorem.We demonstrate two advantages of such smooth flows: they allow training by force matching to simulation data and can be used as potentials in molecular dynamics simulations.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YDAA8FTN/Köhler et al. - 2021 - Smooth Normalizing Flows.pdf}
}

@article{korhonenSingleExponentialTime2Approximation2023,
  title = {A {{Single-Exponential Time}} 2-{{Approximation Algorithm}} for {{Treewidth}}},
  author = {Korhonen, Tuukka},
  date = {2023-11-14},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  pages = {FOCS21-174},
  publisher = {{Society for Industrial and Applied Mathematics}},
  issn = {0097-5397},
  doi = {10.1137/22M147551X},
  url = {https://epubs.siam.org/doi/full/10.1137/22M147551X},
  urldate = {2024-02-20},
  abstract = {We give an algorithm that for an input \$n\$-vertex graph \$G\$ and integer \$k{$>$}0\$, in time \$2\^{}\{O(k)\} n\$, either outputs that the treewidth of \$G\$ is larger than \$k\$, or gives a tree decomposition of \$G\$ of width at most \$5k+4\$.  This is the first algorithm providing a constant factor approximation for treewidth which runs in time single exponential in \$k\$ and linear in \$n\$. Treewidth-based computations are subroutines of numerous algorithms.  Our algorithm can be used to speed up many such algorithms to work in time which is single exponential in the treewidth and linear in the input size.},
  pagetotal = {FOCS21-194},
  file = {/home/krawczuk/Zotero/storage/ZUNNL6SA/Korhonen - 2023 - A Single-Exponential Time 2-Approximation Algorith.pdf}
}

@inproceedings{krawczukGGGANGeometricGraph2020,
  title = {{{GG-GAN}}: {{A}} Geometric Graph Generative Adversarial Network, 2021},
  shorttitle = {{{GG-GAN}}},
  booktitle = {{{URL}} {{https://openreview.}} Net/Forum},
  author = {Krawczuk, Igor and Abranches, Pedro and Loukas, Andreas and Cevher, Volkan},
  date = {2020},
  keywords = {⛔ No DOI found}
}

@inproceedings{krinkeConstraintsTapeOutContinuous2019,
  title = {From {{Constraints}} to {{Tape-Out}}: {{Towards}} a {{Continuous AMS Design Flow}}},
  shorttitle = {From {{Constraints}} to {{Tape-Out}}},
  booktitle = {2019 {{IEEE}} 22nd {{International Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} \& {{Systems}} ({{DDECS}})},
  author = {Krinke, Andreas and Horst, Tilman and Gläser, Georg and Grabmann, Martin and Markus, Tobias and Prautsch, Benjamin and Hatnik, Uwe and Lienig, Jens},
  date = {2019-04},
  pages = {1--10},
  issn = {2473-2117},
  doi = {10.1109/DDECS.2019.8724669},
  url = {https://ieeexplore.ieee.org/document/8724669},
  urldate = {2024-02-19},
  abstract = {The effort in designing analog/mixed-signal (AMS) integrated circuits is characterized by the largely manual work involved in the design of analog cells and their integration into the overall circuit. This inequality in effort between analog and digital cells increases with the use of modern, more complex technology nodes. To mitigate this problem, this paper presents four methods to improve existing mixed-signal design flows: (1) automatic schematic generation from a system-level model, (2) flexible automatic analog layout generation, (3) constraint propagation and budget calculation for dependency resolution, and (4) verification of nonfunctional effects. The implementation of these steps results in a novel AMS design flow with a significantly higher degree of automation.},
  eventtitle = {2019 {{IEEE}} 22nd {{International Symposium}} on {{Design}} and {{Diagnostics}} of {{Electronic Circuits}} \& {{Systems}} ({{DDECS}})},
  keywords = {Automation,Computer architecture,Hardware design languages,Layout,Libraries,Phase locked loops,Tools},
  file = {/home/krawczuk/Zotero/storage/GJJCN9GP/Krinke et al. - 2019 - From Constraints to Tape-Out Towards a Continuous.pdf;/home/krawczuk/Zotero/storage/2UTNK387/8724669.html}
}

@inproceedings{kunalGANAGraphConvolutional2020e,
  title = {{{GANA}}: {{Graph Convolutional Network Based Automated Netlist Annotation}} for {{Analog Circuits}}},
  shorttitle = {{{GANA}}},
  booktitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Kunal, Kishor and Dhar, Tonmoy and Madhusudan, Meghna and Poojary, Jitesh and Sharma, Arvind and Xu, Wenbin and Burns, Steven M. and Hu, Jiang and Harjani, Ramesh and Sapatnekar, Sachin S.},
  date = {2020-03},
  pages = {55--60},
  issn = {1558-1101},
  doi = {10.23919/DATE48585.2020.9116329},
  url = {https://ieeexplore.ieee.org/abstract/document/9116329?casa_token=pmDFJicmP08AAAAA:1rCal6CTB7-Eamcy3AvSogNocH1GGEo4qlxNM7-egouIg952ujAHMwdEnWLA-IPDRraV4uaxiUeh},
  urldate = {2024-02-19},
  abstract = {Automated subcircuit identification and annotation enables the creation of hierarchical representations of analog netlists, and can facilitate a variety of design automation tasks such as circuit layout and optimization. Subcircuit identification must navigate the numerous alternative structures that can implement any analog function, but traditional graph-based methods cannot easily identify the large number of such structural variants. The novel approach in this paper is based on the use of a trained graph convolutional neural network (GCN) that identifies netlist elements for circuit blocks at upper levels of the design hierarchy. Structures at lower levels of hierarchy are identified using graph-based algorithms. The proposed recognition scheme organically detects layout constraints, such as symmetry and matching, whose identification is essential for high-quality hierarchical layout. The subcircuit identification method demonstrates a high degree of accuracy over a wide range of analog designs, successfully identifies larger circuits that contain subblocks such as OTAs, LNAs, mixers, oscillators, and band-pass filters, and provides hierarchical decompositions of such circuits.},
  eventtitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {Annotations,Bipartite graph,Convolution,Current mirrors,Layout,Topology,Transistors},
  file = {/home/krawczuk/Zotero/storage/QVGYP4WI/Kunal et al. - 2020 - GANA Graph Convolutional Network Based Automated .pdf;/home/krawczuk/Zotero/storage/5SV8PYA2/9116329.html}
}

@article{kuznetsovMolGrowGraphNormalizing2021a,
  title = {{{MolGrow}}: {{A Graph Normalizing Flow}} for {{Hierarchical Molecular Generation}}},
  shorttitle = {{{MolGrow}}},
  author = {Kuznetsov, Maksim and Polykovskiy, Daniil},
  date = {2021-05-18},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  number = {9},
  pages = {8226--8234},
  issn = {2374-3468},
  doi = {10.1609/aaai.v35i9.17001},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/17001},
  urldate = {2024-02-22},
  abstract = {We propose a hierarchical normalizing flow model for generating molecular graphs. The model produces new molecular structures from a single-node graph by recursively splitting every node into two. All operations are invertible and can be used as plug-and-play modules. The hierarchical nature of the latent codes allows for precise changes in the resulting graph: perturbations in the first layer cause global structural changes, while perturbations in the consequent layers change the resulting molecule only marginally. Proposed model outperforms existing generative graph models on the distribution learning task. We also show successful experiments on global and constrained optimization of chemical properties using latent codes of the model.},
  issue = {9},
  langid = {english},
  keywords = {Graph-based Machine Learning},
  file = {/home/krawczuk/Zotero/storage/VPR36Y2S/Kuznetsov and Polykovskiy - 2021 - MolGrow A Graph Normalizing Flow for Hierarchical.pdf}
}

@article{lafonDiffusionMapsCoarsegraining2006,
  title = {Diffusion Maps and Coarse-Graining: A Unified Framework for Dimensionality Reduction, Graph Partitioning, and Data Set Parameterization},
  shorttitle = {Diffusion Maps and Coarse-Graining},
  author = {Lafon, S. and Lee, A.B.},
  date = {2006-09},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {28},
  number = {9},
  pages = {1393--1403},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2006.184},
  url = {https://ieeexplore.ieee.org/abstract/document/1661543?casa_token=r7SGqxty3uAAAAAA:v_U04YujQFSbPp3lYzprOgaQAX7V-oRuEKCD8jW3udjNolVinnLfat0k5oq8RL-JqSmnowKBpM18},
  urldate = {2024-02-22},
  abstract = {We provide evidence that nonlinear dimensionality reduction, clustering, and data set parameterization can be solved within one and the same framework. The main idea is to define a system of coordinates with an explicit metric that reflects the connectivity of a given data set and that is robust to noise. Our construction, which is based on a Markov random walk on the data, offers a general scheme of simultaneously reorganizing and subsampling graphs and arbitrarily shaped data sets in high dimensions using intrinsic geometry. We show that clustering in embedding spaces is equivalent to compressing operators. The objective of data partitioning and clustering is to coarse-grain the random walk on the data while at the same time preserving a diffusion operator for the intrinsic geometry or connectivity of the data set up to some accuracy. We show that the quantization distortion in diffusion space bounds the error of compression of the operator, thus giving a rigorous justification for k-means clustering in diffusion space and a precise measure of the performance of general clustering algorithms.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {clustering,Clustering algorithms,clustering similarity measures,compression (coding),Distortion measurement,Eigenvalues and eigenfunctions,Extraterrestrial measurements,Geometry,graph algorithms.,graph-theoretic methods,information visualization,knowledge retrieval,Machine learning,Markov processes,Noise robustness,Noise shaping,Nonlinear distortion,quantization,Quantization,text analysis,Text analysis},
  file = {/home/krawczuk/Zotero/storage/2SA869Z3/Lafon and Lee - 2006 - Diffusion maps and coarse-graining a unified fram.pdf;/home/krawczuk/Zotero/storage/NFX3P5LK/1661543.html}
}

@online{LateBreakingResults,
  title = {Late {{Breaking Results}}: {{Attention}} in {{Graph2Seq Neural Networks}} towards {{Push-Button Analog IC Placement}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9586177?casa_token=YkwAi19gql0AAAAA:jq63XhXbWzGuT8FYjFwMbchwC2Fh-5PE30Hte_oACun-3RT6F2J4wWreex2NvwvfFYzHpyazN82m},
  urldate = {2024-02-20}
}

@online{LearningGraphTopological,
  title = {Learning {{Graph Topological Features}} via {{GAN}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/8638941},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/WKZKG7DS/8638941.html}
}

@incollection{leeSemiconductorManufacturingAutomation2023,
  title = {Semiconductor {{Manufacturing Automation}}},
  booktitle = {Springer {{Handbook}} of {{Automation}}},
  author = {Lee, Tae-Eog and Kim, Hyun-Jung and Yu, Tae-Sun},
  editor = {Nof, Shimon Y.},
  date = {2023},
  series = {Springer {{Handbooks}}},
  pages = {841--863},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-96729-1_38},
  url = {https://doi.org/10.1007/978-3-030-96729-1_38},
  urldate = {2024-02-19},
  abstract = {We review automation requirements and technologies for semiconductor manufacturing. We first discuss equipment integration architectures and control to meet automation requirements for modern fabs. We explain tool architectures and operational issues for modern integrated tools such as cluster tools, which combine several processing modules with wafer-handling robots. We then review recent progress in tool science for scheduling and control of integrated tools and discuss control software architecture, design, and development for integrated tools. Next, we discuss requirements and technologies in fab integration architectures and operation such as modern fab architectures and automated material-handling systems, communication architecture and networking, fab control application integration, and fab control and management. We finally discuss recent trends on smart fabs with machine learning.},
  isbn = {978-3-030-96729-1},
  langid = {english},
  keywords = {Assembly process,Cyclic schedule,Fab architecture,Semiconductor equipment,Semiconductor manufacturing system,Smart fab,Wafer fabrication process}
}

@online{lezcano-casadoAdaptiveMomentumMethods2020,
  title = {Adaptive and {{Momentum Methods}} on {{Manifolds Through Trivializations}}},
  author = {Lezcano-Casado, Mario},
  date = {2020-10-09},
  eprint = {2010.04617},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2010.04617},
  url = {http://arxiv.org/abs/2010.04617},
  urldate = {2024-02-23},
  abstract = {Adaptive methods do not have a direct generalization to manifolds as the adaptive term is not invariant. Momentum methods on manifolds suffer from efficiency problems stemming from the curvature of the manifold. We introduce a framework to generalize adaptive and momentum methods to arbitrary manifolds by noting that for every differentiable manifold, there exists a radially convex open set that covers almost all the manifold. Being radially convex, this set is diffeomorphic to \$\textbackslash mathbb\{R\}\^{}n\$. This gives a natural generalization of any adaptive and momentum-based algorithm to a set that covers almost all the manifold in an arbitrary manifolds. We also show how to extend these methods to the context of gradient descent methods with a retraction. For its implementation, we bring an approximation to the exponential of matrices that needs just of 5 matrix multiplications, making it particularly efficient on GPUs. In practice, we see that this family of algorithms closes the numerical gap created by an incorrect use of momentum and adaptive methods on manifolds. At the same time, we see that the most efficient algorithm of this family is given by simply pulling back the problem to the tangent space at the initial point via the exponential map.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis,Mathematics - Optimization and Control},
  file = {/home/krawczuk/Zotero/storage/M3VBANV3/Lezcano-Casado - 2020 - Adaptive and Momentum Methods on Manifolds Through.pdf;/home/krawczuk/Zotero/storage/EM73HRA7/2010.html}
}

@online{lezcano-casadoGeometricOptimisationManifolds2022a,
  title = {Geometric {{Optimisation}} on {{Manifolds}} with {{Applications}} to {{Deep Learning}}},
  author = {Lezcano-Casado, Mario},
  date = {2022-03-09},
  eprint = {2203.04794},
  eprinttype = {arxiv},
  eprintclass = {cs, math},
  doi = {10.48550/arXiv.2203.04794},
  url = {http://arxiv.org/abs/2203.04794},
  urldate = {2024-02-23},
  abstract = {We design and implement a Python library to help the non-expert using all these powerful tools in a way that is efficient, extensible, and simple to incorporate into the workflow of the data scientist, practitioner, and applied researcher. The algorithms implemented in this library have been designed with usability and GPU efficiency in mind, and they can be added to any PyTorch model with just one extra line of code. We showcase the effectiveness of these tools on an application of optimisation on manifolds in the setting of time series analysis. In this setting, orthogonal and unitary optimisation is used to constraint and regularise recurrent models and avoid vanishing and exploding gradient problems. The algorithms designed for GeoTorch allow us to achieve state of the art results in the standard tests for this family of models. We use tools from comparison geometry to give bounds on quantities that are of interest in optimisation problems. In particular, we build on the work of (Kaul 1976) to give explicit bounds on the norm of the second derivative of the Riemannian exponential.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/krawczuk/Zotero/storage/F9GNQP5J/Lezcano-Casado - 2022 - Geometric Optimisation on Manifolds with Applicati.pdf;/home/krawczuk/Zotero/storage/WUWWENHG/2203.html}
}

@online{lHowDesignProcess2019,
  title = {How Is the {{Design Process}} of {{Microchips}}: {{Analog IC Design Flow}} to {{Tapeout}}},
  shorttitle = {How Is the {{Design Process}} of {{Microchips}}},
  author = {L, Alberto},
  date = {2019-09-01T11:09:56+00:00},
  url = {https://miscircuitos.com/design-process-of-chips-asics-flow-from-design-to-tapeout/},
  urldate = {2024-02-19},
  abstract = {Flowchart with the Analog IC Design Flow from the architecture to tape out. An overview of how ASIC integrated circuits chips are designed and fabricated.},
  langid = {american},
  organization = {{MisCircuitos.com}},
  file = {/home/krawczuk/Zotero/storage/DUHCBXHV/design-process-of-chips-asics-flow-from-design-to-tapeout.html}
}

@article{liAdaptiveLayoutDecomposition2022a,
  title = {Adaptive {{Layout Decomposition With Graph Embedding Neural Networks}}},
  author = {Li, Wei and Ma, Yuzhe and Lin, Yibo and Yu, Bei},
  date = {2022-11},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {11},
  pages = {5030--5042},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2022.3140729},
  url = {https://ieeexplore.ieee.org/abstract/document/9672139?casa_token=H7aXYoXqoRsAAAAA:FV_lv-zfdfH0hasodGZR4fYBmowWv7vRcyeYF5q8oJCYfI_5_HAH-UaEFLsFAqxVzSHRw3xTiTuQ},
  urldate = {2024-02-22},
  abstract = {Multiple patterning layout decomposition (MPLD) has been widely investigated, but so far there is no decomposer that dominates others in terms of both result quality and efficiency. This observation motivates us to explore how to adaptively select the most suitable MPLD strategy for a given layout graph, which is nontrivial and still an open problem. In this article, we propose a layout decomposition framework based on graph convolutional networks to obtain the graph embeddings of the layout. The graph embeddings are used for graph library construction, decomposer selection, graph matching, stitch removal prediction, and graph coloring. In addition, we design a fast nonstitch layout decomposition algorithm that purely depends on the message passing graph neural network. The experimental results show that our graph embedding-based framework can achieve optimal decompositions in the widely used benchmark with a significant runtime drop even compared with fast but nonoptimal heuristics.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Adaptation models,Color,Costs,Design methodology,Layout,layout decomposition,Libraries,Lithography,Runtime,VLSI design},
  file = {/home/krawczuk/Zotero/storage/P9FNYHG8/Li et al. - 2022 - Adaptive Layout Decomposition With Graph Embedding.pdf;/home/krawczuk/Zotero/storage/EWLQL9E8/9672139.html}
}

@article{liangMILEMultiLevelFramework2021,
  title = {{{MILE}}: {{A Multi-Level Framework}} for {{Scalable Graph Embedding}}},
  shorttitle = {{{MILE}}},
  author = {Liang, Jiongqian and Gurukar, Saket and Parthasarathy, Srinivasan},
  date = {2021-05-22},
  journaltitle = {Proceedings of the International AAAI Conference on Web and Social Media},
  volume = {15},
  pages = {361--372},
  issn = {2334-0770},
  doi = {10.1609/icwsm.v15i1.18067},
  url = {https://ojs.aaai.org/index.php/ICWSM/article/view/18067},
  urldate = {2024-02-22},
  abstract = {Recently there has been a surge of interest in designing graph embedding methods. Few, if any, can scale to a large-sized graph with millions of nodes due to both computational complexity and memory requirements. In this paper, we relax this limitation by introducing the MultI-Level Embedding (MILE) framework – a generic methodology allowing contemporary graph embedding methods to scale to large graphs. MILE repeatedly coarsens the graph into smaller ones using a hybrid matching technique to maintain the backbone structure of the graph. It then applies existing embedding methods on the coarsest graph and refines the embeddings to the original graph through a graph convolution neural network that it learns. The proposed MILE framework is agnostic to the underlying graph embedding techniques and can be applied to many existing graph embedding methods without modifying them. We employ our framework on several popular graph embedding techniques and conduct embedding for real-world graphs. Experimental results on five large-scale datasets demonstrate that MILE significantly boosts the speed (order of magnitude) of graph embedding while generating embeddings of better quality, for the task of node classification. MILE can comfortably scale to a graph with 9 million nodes and 40 million edges, on which existing methods run out of memory or take too long to compute on a modern workstation. Our code and data are publicly available with detailed instructions for adding new base embedding methods: https://github.com/jiongqian/MILE.},
  langid = {english},
  keywords = {communities identification,expertise and authority discovery,Social network analysis},
  file = {/home/krawczuk/Zotero/storage/3LTASK4A/Liang et al. - 2021 - MILE A Multi-Level Framework for Scalable Graph E.pdf}
}

@online{LibraryfreeStructureRecognition,
  title = {Library-Free {{Structure Recognition}} for {{Analog Circuits}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9474102?casa_token=g6W7e4QDJ-8AAAAA:AWFq8MmUWRffGreMfbsiF2pS-zHiO3Jgr9WB5sTsQmwyIntwp7CcyA62Tc7A4QvKAnDdUeQZFh8P},
  urldate = {2024-02-20}
}

@incollection{lienigAddressingReliabilityPhysical2020,
  title = {Addressing {{Reliability}} in {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {257--302},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_7},
  url = {https://doi.org/10.1007/978-3-030-39284-0_7},
  urldate = {2024-02-19},
  abstract = {Reliability of electronic circuits is becoming an increasing concern due to the ongoing downscaling of the structural dimensions and the continuous increase in performance requirements. This final chapter addresses the many options available to a layout designer, given the enormous influence of physical design on circuit reliability. Hence, the goal of this chapter is to summarize the state of the art in reliability-driven physical design and related mitigating measures. We start by presenting reliability issues that can lead to temporary circuit malfunctions. We discuss in this context parasitic effects in the bulk of silicon (Sect. 7.1), at its surface (Sect. 7.2), and in the interconnect layers (Sect. 7.3). Our main goal is to show how these effects can be suppressed through appropriate layout measures. After having presented temporarily-induced malfunctions and their mitigation options, we discuss the growing challenges of preventing ICs from irreversible damage. This requires the investigation of overvoltage events (Sect. 7.4) and migration processes, such as electromigration, thermal and stress migration (Sect. 7.5). Again, not only do we discuss the physical background to this damage, we also present appropriate mitigation measures.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigBridgesTechnologyInterfaces2020,
  title = {Bridges to {{Technology}}: {{Interfaces}}, {{Design Rules}}, and {{Libraries}}},
  shorttitle = {Bridges to {{Technology}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {83--126},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_3},
  url = {https://doi.org/10.1007/978-3-030-39284-0_3},
  urldate = {2024-02-19},
  abstract = {Having presented fabrication technology for IC chips in Chap. 2, we now investigate in detail an important aspect of the physical design process: data interfaces. We introduce circuit, layout and mask data structures, that is, the main input and output data in the design steps, in this chapter. First, we explain the input to physical design—circuit data—while focusing on schematics and netlists (Sect. 3.1); we then discuss the output of the physical design step: layout data such as layers and polygons (Sect. 3.2). Mask data, which are the data required by the foundry and generated at the end of the design process, are described in Sect. 3.3. Here, we introduce “layout post processing”, where amendments and additions to the chip layout data are performed in order to convert a physical layout into data for mask production. Technology data, provided by the chip manufacturing foundry, are crucial for producing the physical design. An important portion of these data are technological constraints which are modeled in the geometrical design rules used in physical design. Essentially, geometrical design rules are constraints for physical design, whose compliance ensures the manufacturability of the layout results. Geometrical design rules are presented in detail in Sect. 3.4. Technology data are organized in libraries. These libraries, which are extensively used in IC and PCB design, are covered in our final Sect. 3.5.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@book{lienigFundamentalsLayoutDesign2020,
  title = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0},
  url = {http://link.springer.com/10.1007/978-3-030-39284-0},
  urldate = {2024-02-19},
  isbn = {978-3-030-39283-3 978-3-030-39284-0},
  langid = {english},
  keywords = {Analog layout,Design rules,Failure mechanisms,Layout constraints,Layout design,Layout generators,Matching,Parasitic effects,Physical design automation,Semiconductor fabrication},
  file = {/home/krawczuk/Zotero/storage/86TKUXRJ/Lienig and Scheible - 2020 - Fundamentals of Layout Design for Electronic Circu.pdf}
}

@incollection{lienigIntroduction2020,
  title = {Introduction},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {1--29},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_1},
  url = {https://doi.org/10.1007/978-3-030-39284-0_1},
  urldate = {2024-02-19},
  abstract = {This chapter gives a sound introduction to the technologies, tasks and methodologies used to design the layout of an electronic circuit. With this basic design knowledge as a foundation, the subsequent chapters then delve deeper into specific constraints and aspects of physical design, such as semiconductor technologies (Chap. 2 ), interfaces, design rules and libraries (Chap. 3 ), design flows and models (Chap. 4 ), design steps (Chap. 5 ), analog design specifics (Chap. 6 ), and finally reliability measures (Chap. 7 ). In Sect. 1.1, we introduce several of the most common fabrication technologies for electronic systems. The central topic of this book is the physical design of integrated circuits (aka chips, ICs) but hybrid technologies and printed circuit boards (PCBs) are also considered. In Sect. 1.2 of our introduction, we examine in more detail the significance and peculiarities of this related branch of modern electronics—also known as microelectronics. In Sect. 1.3, we then consider the physical design of both integrated circuits and printed circuits boards with a specific emphasis on their primary design steps. After these opening sections, we close the introductory chapter in Sect. 1.4 by presenting our motivation for this book and describing the organization of the chapters that follow.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigMethodologiesPhysicalDesign2020,
  title = {Methodologies for {{Physical Design}}: {{Models}}, {{Styles}}, {{Tasks}}, and {{Flows}}},
  shorttitle = {Methodologies for {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {127--164},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_4},
  url = {https://doi.org/10.1007/978-3-030-39284-0_4},
  urldate = {2024-02-19},
  abstract = {In Chap. 2 we covered technologies and in Chap. 3 we saw how these technologies interface with physical design. Here in Chap. 4 we now provide an end-to-end overview of the physical design process, namely how to physically construct the layout of an electronic circuit. In this chapter we present the fundamental knowledge an engineer must possess to carry out this task. In Chap. 5 we then discuss each of the specific physical design steps in further detail. We begin the chapter by introducing the design flow (Sect. 4.1), design models (Sect. 4.2) and design styles (Sect. 4.3). Next, we investigate various design tasks and related tools (Sect. 4.4), before discussing optimization goals and design constraints (Sect. 4.5). Up to this point our treatise has focused mainly on the digital design flow. In Sect. 4.6 we introduce the characteristics of, and differences between, analog, digital, and mixed-signal design flows. Looking toward the future, we conclude the chapter by presenting two different yet complementary visions for analog-design automation to overcome the analog-digital design gap (Sect. 4.7).},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigSpecialLayoutTechniques2020,
  title = {Special {{Layout Techniques}} for {{Analog IC Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {213--255},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_6},
  url = {https://doi.org/10.1007/978-3-030-39284-0_6},
  urldate = {2024-02-19},
  abstract = {While the physical design steps introduced in Chaps. 4 and 5 are universal, analog integrated circuits present further challenges that require additional layout techniques. There are many differences between analog and digital, and as such, the design flows and tools vary in both cases. Analog circuits are generally less complex in terms of transistor count and are designed in a manual fashion. The distinct lack of design automation means that manual design is still in widespread use today, requiring specialist knowledge that is unique to analog design. This specialist knowledge is covered in this chapter. We discussed analog design flows in Chap. 4 (Sects. 4.6 and 4.7), previously. Now we present layout techniques that accompany these analog flows, which an analog layout designer must be fully aware of. We start with an introduction to sheet resistances and wells (Sects. 6.1 and 6.2) as this knowledge is needed for the sizing and understanding of analog devices, which we then cover in Sect. 6.3. The methodology for cell generators, which produce such analog devices, is presented in Sect. 6.4. An explanation of the fundamental importance of symmetry and a treatise of resulting matching concepts (Sects. 6.5 and 6.6) conclude this chapter.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigStepsPhysicalDesign2020,
  title = {Steps in {{Physical Design}}: {{From Netlist Generation}} to {{Layout Post Processing}}},
  shorttitle = {Steps in {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {165--211},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_5},
  url = {https://doi.org/10.1007/978-3-030-39284-0_5},
  urldate = {2024-02-19},
  abstract = {Due to its complexity, the physical design process is divided into several primary steps. Having introduced in Chap. 4 the flow, constraints and methodologies of today’s physical design process, we now investigate the various steps required to generate its output: a layout. These steps, which transform a netlist into optimized mask data, are dealt with one by one in this chapter. A layout is generated from a netlist. We first describe how a netlist is created, that is, either by using hardware description languages (HDLs) in digital design (Sect. 5.1), or by deriving it from a schematic, as is common in analog design (Sect. 5.2). Then the physical design steps, comprising partitioning, floorplanning, placement, and routing, are presented in detail (Sect. 5.3). All of these steps are supported by highly sophisticated EDA tools in the case of digital designs, which is our focus here. We also discuss in this section the key aspects of symbolic compaction, standard-cell design and PCB design. When the physical design phase is completed, the resulting layout must be verified. This verification step confirms both functional correctness and design manufacturability. Methodologies and tools for comprehensive design verification, with a focus on physical verification, are covered in Sect. 5.4. Finally, we briefly touch on layout post-processing methodologies, such as resolution enhancement techniques (RET), that might impact physical design (Sect. 5.5).},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigStepsPhysicalDesign2020a,
  title = {Steps in {{Physical Design}}: {{From Netlist Generation}} to {{Layout Post Processing}}},
  shorttitle = {Steps in {{Physical Design}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {165--211},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_5},
  url = {https://doi.org/10.1007/978-3-030-39284-0_5},
  urldate = {2024-02-19},
  abstract = {Due to its complexity, the physical design process is divided into several primary steps. Having introduced in Chap. 4 the flow, constraints and methodologies of today’s physical design process, we now investigate the various steps required to generate its output: a layout. These steps, which transform a netlist into optimized mask data, are dealt with one by one in this chapter. A layout is generated from a netlist. We first describe how a netlist is created, that is, either by using hardware description languages (HDLs) in digital design (Sect. 5.1), or by deriving it from a schematic, as is common in analog design (Sect. 5.2). Then the physical design steps, comprising partitioning, floorplanning, placement, and routing, are presented in detail (Sect. 5.3). All of these steps are supported by highly sophisticated EDA tools in the case of digital designs, which is our focus here. We also discuss in this section the key aspects of symbolic compaction, standard-cell design and PCB design. When the physical design phase is completed, the resulting layout must be verified. This verification step confirms both functional correctness and design manufacturability. Methodologies and tools for comprehensive design verification, with a focus on physical verification, are covered in Sect. 5.4. Finally, we briefly touch on layout post-processing methodologies, such as resolution enhancement techniques (RET), that might impact physical design (Sect. 5.5).},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@incollection{lienigTechnologyKnowHowSilicon2020,
  title = {Technology {{Know-How}}: {{From Silicon}} to {{Devices}}},
  shorttitle = {Technology {{Know-How}}},
  booktitle = {Fundamentals of {{Layout Design}} for {{Electronic Circuits}}},
  author = {Lienig, Jens and Scheible, Juergen},
  editor = {Lienig, Jens and Scheible, Juergen},
  date = {2020},
  pages = {31--82},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-39284-0_2},
  url = {https://doi.org/10.1007/978-3-030-39284-0_2},
  urldate = {2024-02-19},
  abstract = {We discuss the fabrication technologies for IC chips in this chapter. We will focus on the main process steps and especially on those aspects that are of particular importance for understanding how they affect, and in some cases drive, the layout of ICs. All our analyses in this chapter will be for silicon as the base material; the principles and understanding gained can be applied to other substrates as well. Following a brief introduction to the fundamentals of IC fabrication (Sect. 2.1) and the base material used in it, namely silicon (Sect. 2.2), we discuss the photolithography process deployed for all structuring work in Sect. 2.3. We will then present in Sect. 2.4 some theoretical opening remarks on typical phenomena encountered in IC fabrication. Knowledge of these phenomena is very useful for understanding the process steps we cover in Sects. 2.5–2.8. We examine a simple exemplar process in Sect. 2.9 and observe how a field-effect transistor (FET) – the most important device in modern integrated circuits—is created. To drive the key points home, we provide a review of each topic at the end of every section from the point of view of layout design by discussing relevant physical design aspects.},
  isbn = {978-3-030-39284-0},
  langid = {english}
}

@article{limExpressiveSignEquivariant2024,
  title = {Expressive {{Sign Equivariant Networks}} for {{Spectral Geometric Learning}}},
  author = {Lim, Derek and Robinson, Joshua and Jegelka, Stefanie and Maron, Haggai},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/3516aa3393f0279e04c099f724664f99-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/YTKNFA26/Lim et al. - 2024 - Expressive Sign Equivariant Networks for Spectral .pdf}
}

@incollection{liMultiLayeredNetworkEmbedding2018,
  title = {Multi-{{Layered Network Embedding}}},
  booktitle = {Proceedings of the 2018 {{SIAM International Conference}} on {{Data Mining}} ({{SDM}})},
  author = {Li, Jundong and Chen, Chen and Tong, Hanghang and Liu, Huan},
  date = {2018-05-07},
  series = {Proceedings},
  pages = {684--692},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9781611975321.77},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611975321.77},
  urldate = {2024-02-22},
  abstract = {Network embedding has gained more attentions in recent years. It has been shown that the learned low-dimensional node vector representations could advance a myriad of graph mining tasks such as node classification, community detection, and link prediction. A vast majority of the existing efforts are overwhelmingly devoted to single-layered networks or homogeneous networks with a single type of nodes and node interactions. However, in many real-world applications, a variety of networks could be abstracted and presented in a multilayered fashion. Typical multi-layered networks include critical infrastructure systems, collaboration platforms, social recommender systems, to name a few. Despite the widespread use of multi-layered networks, it remains a daunting task to learn vector representations of different types of nodes due to the bewildering combination of both within-layer connections and cross-layer network dependencies. In this paper, we study a novel problem of multi-layered network embedding. In particular, we propose a principled framework – MANE to model both within-layer connections and cross-layer network dependencies simultaneously in a unified optimization framework for embedding representation learning. Experiments on real-world multi-layered networks corroborate the effectiveness of the proposed framework.},
  file = {/home/krawczuk/Zotero/storage/ZT4Z6MQ3/Li et al. - 2018 - Multi-Layered Network Embedding.pdf}
}

@online{lippeCategoricalNormalizingFlows2021a,
  title = {Categorical {{Normalizing Flows}} via {{Continuous Transformations}}},
  author = {Lippe, Phillip and Gavves, Efstratios},
  date = {2021-01-21},
  eprint = {2006.09790},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2006.09790},
  url = {http://arxiv.org/abs/2006.09790},
  urldate = {2024-02-24},
  abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate \textbackslash emph\{Categorical Normalizing Flows\}, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. Using a factorized decoder, we introduce an inductive bias to model any interactions in the normalizing flow. As a consequence, we do not only simplify the optimization compared to having a joint decoder, but also make it possible to scale up to a large number of categories that is currently impossible with discrete normalizing flows. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs. GraphCNF implements a three step approach modeling the nodes, edges and adjacency matrix stepwise to increase efficiency. On molecule generation, GraphCNF outperforms both one-shot and autoregressive flow-based state-of-the-art.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/Z9W6FBUY/Lippe and Gavves - 2021 - Categorical Normalizing Flows via Continuous Trans.pdf;/home/krawczuk/Zotero/storage/49CZS9LC/2006.html}
}

@inproceedings{liuDETDetectingSystem2020a,
  title = {S {\textsuperscript{3}} {{DET}}: {{Detecting System Symmetry Constraints}} for {{Analog Circuits}} with {{Graph Similarity}}},
  shorttitle = {S {\textsuperscript{3}} {{DET}}},
  booktitle = {2020 25th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Liu, Mingjie and Li, Wuxi and Zhu, Keren and Xu, Biying and Lin, Yibo and Shen, Linxiao and Tang, Xiyuan and Sun, Nan and Pan, David Z.},
  date = {2020-01},
  pages = {193--198},
  publisher = {{IEEE}},
  location = {{Beijing, China}},
  doi = {10.1109/ASP-DAC47756.2020.9045109},
  url = {https://ieeexplore.ieee.org/document/9045109/},
  urldate = {2024-02-20},
  abstract = {Symmetry and matching between critical building blocks have a significant impact on analog system performance. However, there is limited research on generating system level symmetry constraints. In this paper, we propose a novel method of detecting system symmetry constraints for analog circuits with graph similarity. Leveraging spectral graph analysis and graph centrality, the proposed algorithm can be applied to circuits and systems of large scale and different architectures. To the best of our knowledge, this is the first work in detecting system level symmetry constraints for analog and mixed-signal (AMS) circuits. Experimental results show that the proposed method can achieve high accuracy of 88.3\% with low false alarm rate of less than 1.1\% in largescale AMS designs.},
  eventtitle = {2020 25th {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  isbn = {978-1-72814-123-7},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/8YFFVMHY/Liu et al. - 2020 - S 3 DET Detecting System Symmetry Cons.pdf}
}

@article{liuGNNCapChipScaleInterconnect2024,
  title = {{{GNN-Cap}}: {{Chip-Scale Interconnect Capacitance Extraction Using Graph Neural Network}}},
  shorttitle = {{{GNN-Cap}}},
  author = {Liu, Lihao and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2024},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  pages = {1--1},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2023.3331942},
  url = {https://ieeexplore.ieee.org/document/10314730/},
  urldate = {2024-02-22},
  abstract = {Interconnect capacitive parasitics are becoming increasingly dominant at finer technology nodes. Chip-scale interconnect capacitance extraction is a critical but challenging task. The structure patterns of nanometer-scale on-chip interconnects are complex. The accuracy of widely used patternmatching-based capacitance extraction methods is limited by labor-intensive pattern library construction. This work presents GNN-Cap, a graph neural network-based method for chip-scale interconnect capacitance extraction. GNN-Cap uses graph presentation learning to model the complex interconnect structural patterns, which enables accurate and efficient prediction of wiring capacitances. Compared with StarRC, the de facto commercial capacitance extraction tool, GNN-Cap achieves a speed up of 11 X to 13 X, and reduces the average relative errors of total and coupling capacitances by 81\% and 59\%, respectively.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/7C3A2BXP/Liu et al. - 2024 - GNN-Cap Chip-Scale Interconnect Capacitance Extra.pdf}
}

@article{liuGONEndtoendOptimization2022,
  title = {{{GON}}: {{End-to-end}} Optimization Framework for Constraint Graph Optimization Problems},
  shorttitle = {{{GON}}},
  author = {Liu, Chuan and Wang, Jingwei and Cao, Yunkang and Liu, Min and Shen, Weiming},
  date = {2022-10-27},
  journaltitle = {Knowledge-Based Systems},
  shortjournal = {Knowledge-Based Systems},
  volume = {254},
  pages = {109697},
  issn = {0950-7051},
  doi = {10.1016/j.knosys.2022.109697},
  url = {https://www.sciencedirect.com/science/article/pii/S0950705122008590},
  urldate = {2024-02-22},
  abstract = {Real-world computational applications often require solving combinatorial optimization problems on graphs, i.e., graph optimization problems (GOPs). An emerging trend is using graph neural networks (GNNs) to tackle GOPs. However, for GOPs with constraints, a great challenge faced by GNNs-based methods is to produce optimal solutions that satisfy the constraints. Existing methods relying on supervised learning require a large amount of labeled data, while unsupervised learning methods often require designing elaborate network architectures. To address these limitations, we propose an end-to-end optimization framework for constraint GOPs, coined Graph Optimization Network (GON). GON is a simple and effective method aiming to address general GOPs without designing specific network architectures for different problems. The proposed framework is evaluated on two typical constraint GOPs, one with only soft constraints (the balanced graph partitioning problem) and the other with hard constraints (the maximum independent set problem). We model the two problems as node assignment problems and design the corresponding constraint-aware loss function for each so that the model can be directly optimized to generate optimal solutions that satisfy the constraints. Experiments on various benchmarks show that the proposed GON equipped with a vanilla GNN can achieve excellent performance on both GOPs, superior to state-of-the-art methods. The results suggest that GON is a promising solver to address NP-hard GOPs.},
  keywords = {Balanced graph partitioning,Graph neural networks,Graph optimization,Maximum independent set},
  file = {/home/krawczuk/Zotero/storage/PQT9ETG3/S0950705122008590.html}
}

@inproceedings{liuGraphNormalizingFlows2019a,
  title = {Graph {{Normalizing Flows}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Jenny and Kumar, Aviral and Ba, Jimmy and Kiros, Jamie and Swersky, Kevin},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/1e44fdf9c44d7328fecc02d677ed704d-Abstract.html},
  urldate = {2024-02-24},
  abstract = {We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/F9Y9EHVC/Liu et al. - 2019 - Graph Normalizing Flows.pdf}
}

@inproceedings{liuOpenSAROpenSource2021,
  title = {{{OpenSAR}}: {{An Open Source Automated End-to-end SAR ADC Compiler}}},
  shorttitle = {{{OpenSAR}}},
  booktitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  author = {Liu, Mingjie and Tang, Xiyuan and Zhu, Keren and Chen, Hao and Sun, Nan and Pan, David Z.},
  date = {2021-11-01},
  pages = {1--9},
  publisher = {{IEEE}},
  location = {{Munich, Germany}},
  doi = {10.1109/ICCAD51958.2021.9643494},
  url = {https://ieeexplore.ieee.org/document/9643494/},
  urldate = {2024-02-20},
  abstract = {Despite recent developments in automated analog sizing and analog layout generation, there is doubt whether analog design automation techniques could scale to system-level designs. On the other hand, analog designs are considered major roadblocks for open source hardware with limited available design automation tools. In this work, we present OpenSAR, the first open source automated end-to-end successive approximation register (SAR) analog-to-digital converter (ADC) compiler. OpenSAR only requires system performance specifications as the minimal input and outputs DRC and LVS clean layouts. Compared with prior work, we leverage automated placement and routing to generate analog building blocks, removing the need to design layout templates or libraries. We optimize the redundant non-binary capacitor digital-toanalog converter (CDAC) array design for yield considerations with a template-based layout generator that interleaves capacitor rows and columns to reduce process gradient mismatch. Post layout simulations demonstrate that the generated prototype designs achieve state-of-theart resolution, speed, and energy efficiency.},
  eventtitle = {2021 {{IEEE}}/{{ACM International Conference On Computer Aided Design}} ({{ICCAD}})},
  isbn = {978-1-66544-507-8},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/9XET6G6T/Liu et al. - 2021 - OpenSAR An Open Source Automated End-to-end SAR A.pdf}
}

@article{liVarianceReducedGradientEstimation2023,
  title = {Variance-{{Reduced Gradient Estimation}} via {{Noise-Reuse}} in {{Online Evolution Strategies}}},
  author = {Li, Oscar and Harrison, James and Sohl-Dickstein, Jascha and Smith, Virginia and Metz, Luke},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/8e69a97cbdd91ac0808603fa589d6c17-Abstract-Conference.html},
  urldate = {2024-02-22},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/2GYBGSEP/Li et al. - 2023 - Variance-Reduced Gradient Estimation via Noise-Reu.pdf}
}

@article{liWireSizingNontree2007,
  title = {Wire Sizing for Non-Tree Topology},
  author = {Li, Zhuo and Zhou, Ying and Shi, Weiping},
  date = {2007-05},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {26},
  number = {5},
  pages = {872--880},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2007.8361581},
  url = {https://ieeexplore.ieee.org/abstract/document/8361581?casa_token=hD2tSo05Z9EAAAAA:1S9tJPpBOkHyMFEglZG4oO27Ghp51MRw8xphvZ7HITcMR1_Ybg7ooneh-TEV_Zv6CHBYmWGtbpiB},
  urldate = {2024-02-20},
  abstract = {Most existing methods for interconnect wire sizing are designed for RC trees. With the increasing popularity of the non-tree topology in clock networks and multiple link networks, wire sizing for non-tree networks becomes an important problem. In this paper, we propose the first systematic method to size the wires of general non-tree RC networks. Our method consists of three steps: 1) decompose a non-tree RC network into a tree RC network such that the Elmore delay at every sink remains unchanged; 2) size wires of the tree; and 3) merge the wires back to the original non-tree network. All three steps can be implemented in low-order polynomial time. Using this method, previous wiresizing techniques for tree topology for various objectives, such as minimizing the maximum delay, minimizing the total area or power, and reducing skew variability under process variations, can be applied to non-tree topologies. For certain types of networks, such as the tree+link network, our method gives the optimal solution, provided the tree wire sizing is optimal. Compared with the previous best wire-sizing method for non-tree circuits we can achieve 2\% to 17\% Elmore delay reduction with 14\% to 30\% total wire area reduction. Compared with unsized minimum width networks, our delay is 25\% less and the skew is 34\% less, under SPICE simulation. For the tree+link network, we can achieve significant delay reduction and zero skew in nominal case, while get up to 66\% skew variation reduction.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Capacitance,Clocks,Delays,Interconnect synthesis,Network topology,optimization,physicaldesign,postlayout resynthesis,Resistance,routing,timing optimization,Topology,Wires},
  file = {/home/krawczuk/Zotero/storage/6SGLMDGC/Li et al. - 2007 - Wire sizing for non-tree topology.pdf}
}

@inproceedings{loperaSurveyGraphNeural2021d,
  ids = {loperaSurveyGraphNeural2021e},
  title = {A {{Survey}} of {{Graph Neural Networks}} for {{Electronic Design Automation}}},
  booktitle = {2021 {{ACM}}/{{IEEE}} 3rd {{Workshop}} on {{Machine Learning}} for {{CAD}} ({{MLCAD}})},
  author = {Lopera, Daniela Sánchez and Servadei, Lorenzo and Kiprit, Gamze Naz and Hazra, Souvik and Wille, Robert and Ecker, Wolfgang},
  date = {2021-08},
  pages = {1--6},
  doi = {10.1109/MLCAD52597.2021.9531070},
  url = {https://ieeexplore.ieee.org/abstract/document/9531070?casa_token=vqNtApKK8gkAAAAA:bK9rjq4aTs56Oc_6fi7jRjHZgBJzSPLK1zxTl4Fgf2-dz6EdFnpmao8sw-tbVIVoPQcDzl4nMjE},
  urldate = {2024-02-24},
  abstract = {Driven by Moore’s law, the chip design complexity is steadily increasing. Electronic Design Automation (EDA) has been able to cope with the challenging very large-scale integration process, assuring scalability, reliability, and proper time-to-market. However, EDA approaches are time and resource-demanding, and they often do not guarantee optimal solutions. To alleviate these, Machine Learning (ML) has been incorporated into many stages of the design flow, such as in placement and routing. Many solutions employ Euclidean data and ML techniques without considering that many EDA objects are represented naturally as graphs. The trending Graph Neural Networks are an opportunity to solve EDA problems directly using graph structures for circuits, intermediate RTLs, and netlists. In this paper, we present a comprehensive review of the existing works linking the EDA flow for chip design and Graph Neural Networks.},
  eventtitle = {2021 {{ACM}}/{{IEEE}} 3rd {{Workshop}} on {{Machine Learning}} for {{CAD}} ({{MLCAD}})},
  keywords = {Design automation,Electronic Design Automation,Graph neural networks,Graph Neural Networks,Machine learning,Machine Learning,Measurement,Register-Transfer Level,Reliability engineering,Routing,Scalability,Very Large-scale Integration},
  file = {/home/krawczuk/Zotero/storage/CWUDS85W/Lopera et al. - 2021 - A Survey of Graph Neural Networks for Electronic D.pdf;/home/krawczuk/Zotero/storage/FZ3LVAXW/9531070.html}
}

@article{luAutomaticOpAmpGeneration2023,
  title = {Automatic {{Op-Amp Generation}} from {{Specification}} to {{Layout}}},
  author = {Lu, Jialin and Lei, Liangbo and Huang, Jiangli and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2023},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  publisher = {{IEEE}},
  doi = {10.1109/TCAD.2023.3296374},
  url = {https://ieeexplore.ieee.org/abstract/document/10185586/?casa_token=YacwtZ2cDAMAAAAA:2Q22uayJlS-1TX591B3jALrZ4pbZ-URyWS_Hf-jAb3NmJeDLMN_z_i1NtF_2vdXLZcoYEZRYsIcD},
  urldate = {2024-02-19}
}

@article{ludkeAddThinDiffusion2024,
  title = {Add and {{Thin}}: {{Diffusion}} for {{Temporal Point Processes}}},
  shorttitle = {Add and {{Thin}}},
  author = {Lüdke, David and Biloš, Marin and Shchur, Oleksandr and Lienen, Marten and Günnemann, Stephan},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/b1d9c7e7bd265d81aae8d74a7a6bd7f1-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/RA2M5LWM/Lüdke et al. - 2024 - Add and Thin Diffusion for Temporal Point Process.pdf}
}

@inproceedings{luoGraphDFDiscreteFlow2021d,
  title = {{{GraphDF}}: {{A Discrete Flow Model}} for {{Molecular Graph Generation}}},
  shorttitle = {{{GraphDF}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Luo, Youzhi and Yan, Keqiang and Ji, Shuiwang},
  date = {2021-07-01},
  pages = {7192--7203},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/luo21a.html},
  urldate = {2024-02-20},
  abstract = {We consider the problem of molecular graph generation using deep models. While graphs are discrete, most existing methods use continuous latent variables, resulting in inaccurate modeling of discrete graph structures. In this work, we propose GraphDF, a novel discrete latent variable model for molecular graph generation based on normalizing flow methods. GraphDF uses invertible modulo shift transforms to map discrete latent variables to graph nodes and edges. We show that the use of discrete latent variables reduces computational costs and eliminates the negative effect of dequantization. Comprehensive experimental results show that GraphDF outperforms prior methods on random generation, property optimization, and constrained optimization tasks.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/AIBUSQZL/Luo et al. - 2021 - GraphDF A Discrete Flow Model for Molecular Graph.pdf;/home/krawczuk/Zotero/storage/GBUG92KT/Luo et al. - 2021 - GraphDF A Discrete Flow Model for Molecular Graph.pdf}
}

@inproceedings{luTopologyOptimizationOperational2022,
  title = {Topology {{Optimization}} of {{Operational Amplifier}} in {{Continuous Space}} via {{Graph Embedding}}},
  booktitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Lu, Jialin and Lei, Liangbo and Yang, Fan and Shang, Li and Zeng, Xuan},
  date = {2022-03},
  pages = {142--147},
  issn = {1558-1101},
  doi = {10.23919/DATE54114.2022.9774676},
  url = {https://ieeexplore.ieee.org/abstract/document/9774676?casa_token=SM8jM4GK_qMAAAAA:gPM11JjhdtGnORbikJKlZiaUsja3S33V-N-070gPHDeJyJDRR-U4LbhlSOy-yqm5DCBnFU6H3FL2},
  urldate = {2024-02-20},
  abstract = {Operational amplifier is a key building block in analog circuits. However, the design process of the operational amplifier is complex and time-consuming, as there are no practical automation tools available in the industry. This paper presents a new topology optimization method for operational amplifiers. The behavioral description of the operational amplifier is described using a directed acyclic graph (DAG), which is then transformed into a low-dimensional embedding in continuous space using a variational graph autoencoder. Topology search is performed in the continuous embedding space using stochastic optimization methods, such as Bayesian Optimization. The yield search results are then transformed back to operational amplifier topologies using a graph decoder. The proposed method is also equipped with a surrogate model for performance prediction. Experimental results show that the proposed approach can achieve significant speedup over the genetic searching algorithms. The produced three-stage operational amplifiers offer competitive performance compared to manual designs.},
  eventtitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {Directed acyclic graph,Operational amplifiers,Optimization methods,Performance evaluation,Predictive models,Representation learning,Stochastic processes},
  file = {/home/krawczuk/Zotero/storage/C7WJRM8M/Lu et al. - 2022 - Topology Optimization of Operational Amplifier in .pdf;/home/krawczuk/Zotero/storage/PVVSSYM5/9774676.html}
}

@online{madhawaGraphNVPInvertibleFlow2019a,
  title = {{{GraphNVP}}: {{An Invertible Flow Model}} for {{Generating Molecular Graphs}}},
  shorttitle = {{{GraphNVP}}},
  author = {Madhawa, Kaushalya and Ishiguro, Katushiko and Nakago, Kosuke and Abe, Motoki},
  date = {2019-05-28},
  eprint = {1905.11600},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1905.11600},
  url = {http://arxiv.org/abs/1905.11600},
  urldate = {2024-02-24},
  abstract = {We propose GraphNVP, the first invertible, normalizing flow-based molecular graph generation model. We decompose the generation of a graph into two steps: generation of (i) an adjacency tensor and (ii) node attributes. This decomposition yields the exact likelihood maximization on graph-structured data, combined with two novel reversible flows. We empirically demonstrate that our model efficiently generates valid molecular graphs with almost no duplicated molecules. In addition, we observe that the learned latent space can be used to generate molecules with desired chemical properties.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/WK79C9MS/Madhawa et al. - 2019 - GraphNVP An Invertible Flow Model for Generating .pdf;/home/krawczuk/Zotero/storage/NXTB9LPV/1905.html}
}

@article{maLaplacianCanonizationMinimalist2024,
  title = {Laplacian {{Canonization}}: {{A Minimalist Approach}} to {{Sign}} and {{Basis Invariant Spectral Embedding}}},
  shorttitle = {Laplacian {{Canonization}}},
  author = {Ma, George and Wang, Yifei and Wang, Yisen},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/257b3a7438b1f3709e91a86adf2fdc0a-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/PL5JSHRM/Ma et al. - 2024 - Laplacian Canonization A Minimalist Approach to S.pdf}
}

@article{malliarosCoreDecompositionNetworks2020,
  title = {The Core Decomposition of Networks: Theory, Algorithms and Applications},
  shorttitle = {The Core Decomposition of Networks},
  author = {Malliaros, Fragkiskos D. and Giatsidis, Christos and Papadopoulos, Apostolos N. and Vazirgiannis, Michalis},
  date = {2020-01-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {29},
  number = {1},
  pages = {61--92},
  issn = {0949-877X},
  doi = {10.1007/s00778-019-00587-4},
  url = {https://doi.org/10.1007/s00778-019-00587-4},
  urldate = {2024-02-20},
  abstract = {The core decomposition of networks has attracted significant attention due to its numerous applications in real-life problems. Simply stated, the core decomposition of a network (graph) assigns to each graph node v, an integer number c(v) (the core number), capturing how well v is connected with respect to its neighbors. This concept is strongly related to the concept of graph degeneracy, which has a long history in graph theory. Although the core decomposition concept is extremely simple, there is an enormous interest in the topic from diverse application domains, mainly because it can be used to analyze a network in a simple and concise manner by quantifying the significance of graph nodes. Therefore, there exists a respectable number of research works that either propose efficient algorithmic techniques under different settings and graph types or apply the concept to another problem or scientific area. Based on this large interest in the topic, in this survey, we perform an in-depth discussion of core decomposition, focusing mainly on: (i) the basic theory and fundamental concepts, (ii) the algorithmic techniques proposed for computing it efficiently under different settings, and (iii) the applications that can benefit significantly from it.},
  langid = {english},
  keywords = {Algorithms,Core decomposition,Graph degeneracy,Graph mining,Graph theory},
  file = {/home/krawczuk/Zotero/storage/HL6XIPN8/Malliaros et al. - 2020 - The core decomposition of networks theory, algori.pdf}
}

@inproceedings{mangalagiriAnalogLayoutSynthesis2019,
  title = {Analog {{Layout Synthesis}}: {{Are We There Yet}}?},
  shorttitle = {Analog {{Layout Synthesis}}},
  booktitle = {Proceedings of the 2019 {{International Symposium}} on {{Physical Design}}},
  author = {Mangalagiri, Prasanth},
  date = {2019-04-04},
  series = {{{ISPD}} '19},
  pages = {127},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3299902.3311065},
  url = {https://doi.org/10.1145/3299902.3311065},
  urldate = {2024-02-19},
  abstract = {Over the past decade, spurred by advances in mobile computing, there has been a fundamental shift in computing needs of consumer applications. There has been an industry-wide transition from highly CPU-centric to a peripheral-centric, connectivity and data-driven computing. This has paved way to the resurgence of Analog Mixed Signal Designs in both system-on-chip, and core computing architectures. However, the design automation capabilities used in production analog design flows have remained primarily manual with assisted-automation. Analog layout design and layout parasitic dependent circuit convergence remain a key bottleneck in industrial analog IP design. In this talk, we analyze the current state of analog design automation. We present a continuum of design scenarios, ranging from leading-edge design, to design migration across incremental process derivatives, and define the context of analog layout synthesis in each of these scenarios. We present an overview of recent advances in EDA research specific to analog layout automation [1] [2] and discuss their strengths and weaknesses when adapted to industrial analog IP design flows. Motivated by the confluence of emerging trends in EDA [3], and machine learning research [4], we discuss opportunities to bridge the "last-mile" gaps in automation, by combining constraint-driven, generator-based automation approaches, with statistical data-driven predictive methods.},
  isbn = {978-1-4503-6253-5},
  keywords = {analog layout synthesis,design convergence,eda.,layout migration,machine learning,physical design automation}
}

@inproceedings{maronProvablyPowerfulGraph2019b,
  title = {Provably {{Powerful Graph Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maron, Haggai and Ben-Hamu, Heli and Serviansky, Hadar and Lipman, Yaron},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/bb04af0f7ecaee4aae62035497da1387-Abstract.html},
  urldate = {2024-02-24},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/N6B52E85/Maron et al. - 2019 - Provably Powerful Graph Networks.pdf}
}

@article{martinkusAbDiffuserFullatomGeneration2024,
  title = {{{AbDiffuser}}: Full-Atom Generation of in-Vitro Functioning Antibodies},
  shorttitle = {{{AbDiffuser}}},
  author = {Martinkus, Karolis and Ludwiczak, Jan and Liang, Wei-Ching and Lafrance-Vanasse, Julien and Hotzel, Isidro and Rajpal, Arvind and Wu, Yan and Cho, Kyunghyun and Bonneau, Richard and Gligorijevic, Vladimir and Loukas, Andreas},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/801ec05b0aae9fcd2ef35c168bd538e0-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/CVCVTSC8/Martinkus et al. - 2024 - AbDiffuser full-atom generation of in-vitro funct.pdf}
}

@inproceedings{martinkusSPECTRESpectralConditioning2022b,
  title = {{{SPECTRE}}: {{Spectral Conditioning Helps}} to {{Overcome}} the {{Expressivity Limits}} of {{One-shot Graph Generators}}},
  shorttitle = {{{SPECTRE}}},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Machine Learning}}},
  author = {Martinkus, Karolis and Loukas, Andreas and Perraudin, Nathanaël and Wattenhofer, Roger},
  date = {2022-06-28},
  pages = {15159--15179},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v162/martinkus22a.html},
  urldate = {2024-02-20},
  abstract = {We approach the graph generation problem from a spectral perspective by first generating the dominant parts of the graph Laplacian spectrum and then building a graph matching these eigenvalues and eigenvectors. Spectral conditioning allows for direct modeling of the global and local graph structure and helps to overcome the expressivity and mode collapse issues of one-shot graph generators. Our novel GAN, called SPECTRE, enables the one-shot generation of much larger graphs than previously possible with one-shot models. SPECTRE outperforms state-of-the-art deep autoregressive generators in terms of modeling fidelity, while also avoiding expensive sequential generation and dependence on node ordering. A case in point, in sizable synthetic and real-world graphs SPECTRE achieves a 4-to-170 fold improvement over the best competitor that does not overfit and is 23-to-30 times faster than autoregressive generators.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/E3J8ZP25/Martinkus et al. - 2022 - SPECTRE Spectral Conditioning Helps to Overcome t.pdf}
}

@inproceedings{maziarzLearningExtendMolecular2021,
  title = {Learning to {{Extend Molecular Scaffolds}} with {{Structural Motifs}}},
  author = {Maziarz, Krzysztof and Jackson-Flux, Henry Richard and Cameron, Pashmina and Sirockin, Finton and Schneider, Nadine and Stiefl, Nikolaus and Segler, Marwin and Brockschmidt, Marc},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=ZTsoE8G3GG},
  urldate = {2024-02-20},
  abstract = {Recent advancements in deep learning-based modeling of molecules promise to accelerate in silico drug discovery. A plethora of generative models is available, building molecules either atom-by-atom and bond-by-bond or fragment-by-fragment. However, many drug discovery projects require a fixed scaffold to be present in the generated molecule, and incorporating that constraint has only recently been explored. Here, we propose MoLeR, a graph-based model that naturally supports scaffolds as initial seed of the generative procedure, which is possible because it is not conditioned on the generation history. Our experiments show that MoLeR performs comparably to state-of-the-art methods on unconstrained molecular optimization tasks, and outperforms them on scaffold-based tasks, while being an order of magnitude faster to train and sample from than existing approaches. Furthermore, we show the influence of a number of seemingly minor design choices on the overall performance.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/IYPVPWMU/Maziarz et al. - 2021 - Learning to Extend Molecular Scaffolds with Struct.pdf}
}

@inproceedings{meadeGateLevelNetlistReverse2016,
  title = {Gate-{{Level Netlist Reverse Engineering Tool Set}} for {{Functionality Recovery}} and {{Malicious Logic Detection}}},
  author = {Meade, Travis and Zhang, Shaojie and Jin, Yier and Zhao, Zheng and Pan, David},
  date = {2016-11-01},
  pages = {342--346},
  publisher = {{ASM International}},
  doi = {10.31399/asm.cp.istfa2016p0342},
  url = {https://dx.doi.org/10.31399/asm.cp.istfa2016p0342},
  urldate = {2024-02-19},
  abstract = {Abstract. Reliance on third-party resources, including thirdparty IP cores and fabrication foundries, as well as wide usage of commercial-off-the-shelf (COTS) components has raised concerns that backdoors and/or hardware Trojans may be inserted into fabricated chips. Defending against hardware backdoors and/or Trojans has primarily focused on detection at various stages in the supply chain. Netlist reverse engineering tools have been investigated as an alternative to existing chip-level reverse engineering methods which can help recover functional netlists from fabricated chips, but fall short of detecting malicious logic or recovering high-level functionality. In this work, we develop a netlist reverse engineering tool-set which recovers high-level functionality from the netlist, thereby aiding malicious logic detection. The tool-set performs state register identification, control logic recovery and datapath tracking, which facilitates validation of encrypted/obfuscated hardware IP cores. Relying on 3-SAT algorithms and topology-based computational methods, we demonstrate that the developed tool-set can handle netlists of various complexities.},
  eventtitle = {{{ISTFA}} 2016},
  langid = {english}
}

@inproceedings{meadeGateLevelNetlistReverse2016a,
  title = {Gate-{{Level Netlist Reverse Engineering Tool Set}} for {{Functionality Recovery}} and {{Malicious Logic Detection}}},
  author = {Meade, Travis and Zhang, Shaojie and Jin, Yier and Zhao, Zheng and Pan, David},
  date = {2016-11-01},
  pages = {342--346},
  publisher = {{ASM International}},
  doi = {10.31399/asm.cp.istfa2016p0342},
  url = {https://dx.doi.org/10.31399/asm.cp.istfa2016p0342},
  urldate = {2024-02-19},
  abstract = {Abstract. Reliance on third-party resources, including thirdparty IP cores and fabrication foundries, as well as wide usage of commercial-off-the-shelf (COTS) components has raised concerns that backdoors and/or hardware Trojans may be inserted into fabricated chips. Defending against hardware backdoors and/or Trojans has primarily focused on detection at various stages in the supply chain. Netlist reverse engineering tools have been investigated as an alternative to existing chip-level reverse engineering methods which can help recover functional netlists from fabricated chips, but fall short of detecting malicious logic or recovering high-level functionality. In this work, we develop a netlist reverse engineering tool-set which recovers high-level functionality from the netlist, thereby aiding malicious logic detection. The tool-set performs state register identification, control logic recovery and datapath tracking, which facilitates validation of encrypted/obfuscated hardware IP cores. Relying on 3-SAT algorithms and topology-based computational methods, we demonstrate that the developed tool-set can handle netlists of various complexities.},
  eventtitle = {{{ISTFA}} 2016},
  langid = {english}
}

@inproceedings{meadeNetlistReverseEngineering2016,
  title = {Netlist Reverse Engineering for High-Level Functionality Reconstruction},
  booktitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  author = {Meade, Travis and Zhang, Shaojie and Jin, Yier},
  date = {2016-01},
  pages = {655--660},
  issn = {2153-697X},
  doi = {10.1109/ASPDAC.2016.7428086},
  url = {https://ieeexplore.ieee.org/abstract/document/7428086?casa_token=QaB4X0R0lY0AAAAA:LY9xjkYObFJFeANY4kbmFr3X122ry0Q1xoQX7zWMryI99VeJS2zqbxlHsldNfiDpPcv5JWb2I7Dl},
  urldate = {2024-02-19},
  abstract = {In a modern IC design flow, from specification development to chip fabrication, various security threats are emergent. Of particular concern are modifications made to third-party IP cores and commercial off-the-shelf (COTS) chips where no golden models are available for comparisons. Toward this direction, we develop a tool, named Reverse Engineering Finite State Machine (REFSM), that helps end-users reconstruct a high-level description of the control logic from a flattened netlist. We demonstrate that REFSM effectively recovers circuit control logic from netlists with varying degrees of complexity. Experimental results also showed that the developed tool can easily identify malicious logic from a flattened (or even obfuscated) netlist. If combined with chip level reverse engineering techniques, the developed REFSM tool can help detect the insertion of hardware Trojans in fabricated circuits.},
  eventtitle = {2016 21st {{Asia}} and {{South Pacific Design Automation Conference}} ({{ASP-DAC}})},
  keywords = {Hardware,IP networks,Logic gates,Registers,Reverse engineering,Trojan horses},
  file = {/home/krawczuk/Zotero/storage/VI6VGNCB/Meade et al. - 2016 - Netlist reverse engineering for high-level functio.pdf;/home/krawczuk/Zotero/storage/GVGJUHTK/7428086.html}
}

@article{meissnerFEATSFrameworkExplorative2015d,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}}},
  shorttitle = {{{FEATS}}},
  author = {Meissner, Markus and Hedrich, Lars},
  date = {2015-02},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {34},
  number = {2},
  pages = {213--226},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2014.2376987},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=7W00nHRy4joAAAAA:-NpwTCtaXxFtwCYJSnhNC1ZDlMRv_6NxpQFZLRE9OlqvgScAq5z60SejgPlV3fs_nRW4pm7RSSuT},
  urldate = {2024-02-20},
  abstract = {This paper proposes a new methodology for automated analog circuit synthesis, aiming to address the challenges known from other analog synthesis approaches: unsatisfactory time predictability due to stochastic-driven circuit generation methods, the dereliction of the creative part during the design process, and the inflexibility leading to synthesis tools, which mostly only handle just one circuit class. This contribution presents the underlying concepts and ideas to provide the predictability, flexibility, and creative freedom in order to elevate analog circuit design to the next step. A circuit generation algorithm is presented, which allows a full design-space exploration. Furthermore, an isomorphism algorithm is developed, which reduces a given set of circuits to its unique being one of the first methodologies addressing this issue. Thus, the algorithm handles vast amounts of circuits in a very efficient manner. The results demonstrate the claimed feasibility and applicability of the synthesis framework in general and in the context of system design.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Abstracts,Algorithm design and analysis,Analog circuits,design for space exploration,Design for space exploration,Engines,Libraries,performance optimization,Ports (Computers),symbolic techniques,symmetry detection,Symmetry detection,synthesis,Synthesis,Topology},
  file = {/home/krawczuk/Zotero/storage/8NNKF6ZF/Meissner and Hedrich - 2015 - FEATS Framework for Explorative Analog Topology S.pdf}
}

@article{meissnerFEATSFrameworkExplorative2015e,
  title = {{{FEATS}}: {{Framework}} for {{Explorative Analog Topology Synthesis}}},
  shorttitle = {{{FEATS}}},
  author = {Meissner, Markus and Hedrich, Lars},
  date = {2015-02},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {34},
  number = {2},
  pages = {213--226},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2014.2376987},
  url = {https://ieeexplore.ieee.org/abstract/document/6980087?casa_token=7W00nHRy4joAAAAA:-NpwTCtaXxFtwCYJSnhNC1ZDlMRv_6NxpQFZLRE9OlqvgScAq5z60SejgPlV3fs_nRW4pm7RSSuT},
  urldate = {2024-02-20},
  abstract = {This paper proposes a new methodology for automated analog circuit synthesis, aiming to address the challenges known from other analog synthesis approaches: unsatisfactory time predictability due to stochastic-driven circuit generation methods, the dereliction of the creative part during the design process, and the inflexibility leading to synthesis tools, which mostly only handle just one circuit class. This contribution presents the underlying concepts and ideas to provide the predictability, flexibility, and creative freedom in order to elevate analog circuit design to the next step. A circuit generation algorithm is presented, which allows a full design-space exploration. Furthermore, an isomorphism algorithm is developed, which reduces a given set of circuits to its unique being one of the first methodologies addressing this issue. Thus, the algorithm handles vast amounts of circuits in a very efficient manner. The results demonstrate the claimed feasibility and applicability of the synthesis framework in general and in the context of system design.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Abstracts,Algorithm design and analysis,Analog circuits,design for space exploration,Design for space exploration,Engines,Libraries,performance optimization,Ports (Computers),symbolic techniques,symmetry detection,Symmetry detection,synthesis,Synthesis,Topology},
  file = {/home/krawczuk/Zotero/storage/VT3SSR4J/Meissner and Hedrich - 2015 - FEATS Framework for Explorative Analog Topology S.pdf}
}

@online{metzUnrolledGenerativeAdversarial2017a,
  title = {Unrolled {{Generative Adversarial Networks}}},
  author = {Metz, Luke and Poole, Ben and Pfau, David and Sohl-Dickstein, Jascha},
  date = {2017-05-12},
  eprint = {1611.02163},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1611.02163},
  url = {http://arxiv.org/abs/1611.02163},
  urldate = {2024-02-22},
  abstract = {We introduce a method to stabilize Generative Adversarial Networks (GANs) by defining the generator objective with respect to an unrolled optimization of the discriminator. This allows training to be adjusted between using the optimal discriminator in the generator's objective, which is ideal but infeasible in practice, and using the current value of the discriminator, which is often unstable and leads to poor solutions. We show how this technique solves the common problem of mode collapse, stabilizes training of GANs with complex recurrent generators, and increases diversity and coverage of the data distribution by the generator.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/2BTV6DU9/Metz et al. - 2017 - Unrolled Generative Adversarial Networks.pdf;/home/krawczuk/Zotero/storage/D3YJYM8J/1611.html}
}

@article{meurerPythonArrayAPI2023,
  title = {Python {{Array API Standard}}: {{Toward Array Interoperability}} in the {{Scientific Python Ecosystem}}},
  author = {Meurer, Aaron and Reines, Athan and Gommers, Ralf and Fang, Yao-Lung L and Kirkham, John and Barber, Matthew and Müller, Andreas and Zha, Sheng and Shanabrook, Saul and Gacha, Stephannie Jiménez and Lezcano-Casado, Mario and Fan, Thomas J and Reddy, Tyler and Passos, Alexandre and Kwon, Hyukjin and Oliphant, Travis},
  date = {2023},
  abstract = {The Python array API standard specifies standardized application programming interfaces (APIs) and behaviors for array and tensor objects and operations as commonly found in libraries such as NumPy [1], CuPy [2], PyTorch [3], JAX [4], TensorFlow [5], Dask [6], and MXNet [7]. The establishment and subsequent adoption of the standard aims to reduce ecosystem fragmentation and facilitate array library interoperability in user code and among array-consuming libraries, such as scikit-learn [8] and SciPy [9]. A key benefit of array interoperability for downstream consumers of the standard is device agnosticism, whereby previously CPU-bound implementations can more readily leverage hardware acceleration via graphics processing units (GPUs), tensor processing units (TPUs), and other accelerator devices.},
  langid = {english},
  keywords = {❓ Multiple DOI},
  file = {/home/krawczuk/Zotero/storage/4WF7ZAY7/Meurer et al. - 2023 - Python Array API Standard Toward Array Interopera.pdf}
}

@online{Mg2vecLearningRelationshipPreserving,
  title = {Mg2vec: {{Learning Relationship-Preserving Heterogeneous Graph Representations}} via {{Metagraph Embedding}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9089251?casa_token=4RGA3jCG5zsAAAAA:oEZwjXhBIJreFavjipIsBtilcbT4v_yGF7oi9rVPWRP40c4s3R9KfQs_2jyNrP4Nln_k7ztEjnkg},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/ULI74JT3/9089251.html}
}

@inproceedings{michelPathNeuralNetworks2023,
  title = {Path {{Neural Networks}}: {{Expressive}} and {{Accurate Graph Neural Networks}}},
  shorttitle = {Path {{Neural Networks}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Michel, Gaspard and Nikolentzos, Giannis and Lutzeyer, Johannes F. and Vazirgiannis, Michalis},
  date = {2023-07-03},
  pages = {24737--24755},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v202/michel23a.html},
  urldate = {2024-02-24},
  abstract = {Graph neural networks (GNNs) have recently become the standard approach for learning with graph-structured data. Prior work has shed light into their potential, but also their limitations. Unfortunately, it was shown that standard GNNs are limited in their expressive power. These models are no more powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of distinguishing non-isomorphic graphs. In this paper, we propose Path Neural Networks (PathNNs), a model that updates node representations by aggregating paths emanating from nodes. We derive three different variants of the PathNN model that aggregate single shortest paths, all shortest paths and all simple paths of length up to K. We prove that two of these variants are strictly more powerful than the 1-WL algorithm, and we experimentally validate our theoretical results. We find that PathNNs can distinguish pairs of non-isomorphic graphs that are indistinguishable by 1-WL, while our most expressive PathNN variant can even distinguish between 3-WL indistinguishable graphs. The different PathNN variants are also evaluated on graph classification and graph regression datasets, where in most cases, they outperform the baseline methods.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/ZR53J8K5/Michel et al. - 2023 - Path Neural Networks Expressive and Accurate Grap.pdf}
}

@article{minaReviewMachineLearning2022e,
  title = {A {{Review}} of {{Machine Learning Techniques}} in {{Analog Integrated Circuit Design Automation}}},
  author = {Mina, Rayan and Jabbour, Chadi and Sakr, George E.},
  date = {2022-01},
  journaltitle = {Electronics},
  volume = {11},
  number = {3},
  pages = {435},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics11030435},
  url = {https://www.mdpi.com/2079-9292/11/3/435},
  urldate = {2024-02-20},
  abstract = {Analog integrated circuit design is widely considered a time-consuming task due to the acute dependence of analog performance on the transistors’ and passives’ dimensions. An important research effort has been conducted in the past decade to reduce the front-end design cycles of analog circuits by means of various automation approaches. On the other hand, the significant progress in high-performance computing hardware has made machine learning an attractive and accessible solution for everyone. The objectives of this paper were: (1) to provide a comprehensive overview of the existing state-of-the-art machine learning techniques used in analog circuit sizing and analyze their effectiveness in achieving the desired goals; (2) to point out the remaining open challenges, as well as the most relevant research directions to be explored. Finally, the different analog circuits on which machine learning techniques were applied are also presented and their results discussed from a circuit designer perspective.},
  issue = {3},
  langid = {english},
  keywords = {analog IC design,automated circuit sizing,deep neural networks,machine learning},
  file = {/home/krawczuk/Zotero/storage/SEF5SAU4/Mina et al. - 2022 - A Review of Machine Learning Techniques in Analog .pdf}
}

@inproceedings{miteaAutomatedConstraintdrivenTopology2011a,
  title = {Automated Constraint-Driven Topology Synthesis for Analog Circuits},
  booktitle = {2011 {{Design}}, {{Automation}} \& {{Test}} in {{Europe}}},
  author = {Mitea, Oliver and Meissner, Markus and Hedrich, Lars and Jores, Peter},
  date = {2011-03},
  pages = {1--4},
  issn = {1558-1101},
  doi = {10.1109/DATE.2011.5763264},
  url = {https://ieeexplore.ieee.org/abstract/document/5763264?casa_token=FCUFCCq0IToAAAAA:jXu9dEY_XxgfJymLAYjN0_-vgyvirF4Yu0xNS5d9uNVxqQf9PV_ccPxoLD5-q0WuM-6vt39MvWTp},
  urldate = {2024-02-19},
  abstract = {This contribution will present a fully automated approach for explorative topology synthesis of small analog circuit blocks. Circuits are composed from a library of basic building blocks. Therefore, various algorithms are used to explore the entire design space, even allowing to generate unusual circuits. Correct combination of the basic blocks is accomplished through generic electrical rules, which ensure the fundamental electrical functionality of the generated circuit. Additionally, symmetry constraints are introduced to narrow the design space, which leads to more reasonable circuits. Further a replaceable bias-voltage generator is included into the circuit to replicate real world circumstances. For the first evaluation and selection of best candidate circuits, fast symbolic analysis techniques are used. The final sizing is done through a parallelized industrial based sizing method. Experimental results show the feasibility of this synthesis approach.},
  eventtitle = {2011 {{Design}}, {{Automation}} \& {{Test}} in {{Europe}}},
  keywords = {Algorithm design and analysis,Analog circuits,Circuit topology,Design automation,Generators,Libraries,Topology},
  file = {/home/krawczuk/Zotero/storage/R2JA3ILU/Mitea et al. - 2011 - Automated constraint-driven topology synthesis for.pdf;/home/krawczuk/Zotero/storage/WRFNQTBG/5763264.html}
}

@incollection{moDevelopmentWorldIC2024,
  title = {Development of {{World IC Industry}}},
  booktitle = {Handbook of {{Integrated Circuit Industry}}},
  author = {Mo, Da-Kang and Li, Ke},
  editor = {Wang, Yangyuan and Chi, Min-Hwa and Lou, Jesse Jen-Chung and Chen, Chun-Zhang},
  date = {2024},
  pages = {45--79},
  publisher = {{Springer Nature}},
  location = {{Singapore}},
  doi = {10.1007/978-981-99-2836-1_4},
  url = {https://doi.org/10.1007/978-981-99-2836-1_4},
  urldate = {2024-02-23},
  abstract = {The major IC enterprises facing to the market directly are fabless design enterprises (no production line), integrated device manufacturers (IDM), and intellectual property (IP) providers. EDA enterprises primarily provide design methodologies and tool suites but not for providing chip fabrication services, while wafer foundry (or Fab) can provide IC fabrication services. In the IC industry chain, IPs are commonly provided as process-proven and can be embedded in chips or known good dies (KGD); packaging and testing companies mainly provide services for Fabs, and IDM companies, materials companies, and special equipment companies mainly provide the required materials and equipment, respectively, for chip manufacturers. The industry is subject to the influences by several semiconductor organizations, such as World Semiconductor Council (WSC), Semiconductor Equipment and Materials International (SEMI), and Global Semiconductor Association (GSA). Additionally, there are also several IC market research and consulting companies, World Semiconductor Trade Statistics (WSTS), IC Insights, Gartner, Trend Force, and Yole, whose roles as if are IC liaisons in between IC designs and research, chip manufacturing, and product marketing.},
  isbn = {978-981-9928-36-1},
  langid = {english},
  keywords = {Fabless,Foundry,GSA,Integrated device manufacturers (IDM),Intellectual property (IP),SEMI}
}

@article{mondalEquivariantAdaptationLarge2024,
  title = {Equivariant {{Adaptation}} of {{Large Pretrained Models}}},
  author = {Mondal, Arnab Kumar and Panigrahi, Siba Smarak and Kaba, Oumar and Mudumba, Sai Rajeswar and Ravanbakhsh, Siamak},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/9d5856318032ef3630cb580f4e24f823-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/X7CNH72X/Mondal et al. - 2024 - Equivariant Adaptation of Large Pretrained Models.pdf}
}

@incollection{mutzel19GraphDrawing2004,
  title = {19. {{Graph Drawing}}: {{Exact Optimization Helps}}!},
  shorttitle = {19. {{Graph Drawing}}},
  booktitle = {The {{Sharpest Cut}}},
  author = {Mutzel, Petra and Jünger, Michael},
  date = {2004-01},
  series = {{{MOS-SIAM Series}} on {{Optimization}}},
  pages = {327--352},
  publisher = {{Society for Industrial and Applied Mathematics}},
  doi = {10.1137/1.9780898718805.ch19},
  url = {https://epubs.siam.org/doi/abs/10.1137/1.9780898718805.ch19},
  urldate = {2024-02-22},
  abstract = {19.1 Introduction Graph drawing deals with the design and implementation of algorithms for generating automatic layouts of graphs that can be read and understood easily. A good drawing should reveal the structure of the given graph. With applications in business process modeling, software (re-)engineering, and database design, the field of graph drawing is becoming increasingly important. Figure 19.1(a) shows a diagram of the dependencies of the electric power industry as it appeared in a German newspaper [51], while Figure 19.1(b) shows an automatically generated layout of the same diagram. Figure 19.2(a) shows the original drawing of a unified modeling language (UML) diagram taken from [24], while Figure 19.2(b) shows an automatically generated layout of the same diagram. It is difficult to model the niceness of a layout, since this often depends on the particular application. However, there exist some criteria that are commonly accepted as important. The vertices should be evenly distributed over the space, overlaps between nodes and other objects should be avoided, and the lengths of the edges and the drawing area should be small. Among the most important criteria is a small number of edge crossings.},
  isbn = {978-0-89871-552-1},
  file = {/home/krawczuk/Zotero/storage/M3DG2ZNA/Mutzel and Jünger - 2004 - 19. Graph Drawing Exact Optimization Helps!.pdf}
}

@online{NetworkRoutingOptimization,
  title = {Network {{Routing Optimization Based}} on {{Machine Learning Using Graph Networks Robust}} against {{Topology Change}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/document/9016573},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/JA8XF8X3/9016573.html}
}

@online{NeulftNovelApproach,
  title = {Neulft: {{A Novel Approach}} to {{Nonlinear Canonical Polyadic Decomposition}} on {{High-Dimensional Incomplete Tensors}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9779437?casa_token=gNy0YjrACWcAAAAA:ojNOGgD1Fsk-4OH8BuhC0xpmsoSQx3spKqNhhVa15_tdxL9BFgH-iEmFwHWSFQade9IaMg8sI6gg},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/TSELBYDV/9779437.html}
}

@inproceedings{neunerLibraryfreeStructureRecognition2021,
  title = {Library-Free {{Structure Recognition}} for {{Analog Circuits}}},
  booktitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Neuner, Maximilian and Abel, Inga and Graeb, Helmut},
  date = {2021-02-01},
  pages = {1366--1371},
  publisher = {{IEEE}},
  location = {{Grenoble, France}},
  doi = {10.23919/DATE51398.2021.9474102},
  url = {https://ieeexplore.ieee.org/document/9474102/},
  urldate = {2024-02-20},
  abstract = {Extracting structural information of a design is one crucial aspect of many circuit verification and synthesis methods. State-of-the-art structure recognition methods use a predefined building block library to identify the basic building blocks in a circuit. However, the capability of these algorithms is limited by the scope, correctness and completeness of the provided library. This paper presents a new method to automatically generate the recognition rules required to identify a given circuit topology in a large design. Device pairs are grouped into building blocks by analyzing their characteristics, e.g., their connectivity, to enable a structure recognition as unambiguous as possible. The resulting blocks are consecutively assembled to larger blocks until the full building block description of the given topology has been established. Building block libraries dedicated to one specific topology type, e.g., operational amplifiers, can be obtained by applying the method to its basic version, subsequently extending the generated library by the additional elements required to identify its topology variants using the presented method. Experimental results for six folded cascode amplifier and five level shifter topologies are given.},
  eventtitle = {2021 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  isbn = {978-3-9819263-5-4},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/H5ELFFFM/Neuner et al. - 2021 - Library-free Structure Recognition for Analog Circ.pdf}
}

@inproceedings{niuPermutationInvariantGraph2020b,
  title = {Permutation {{Invariant Graph Generation}} via {{Score-Based Generative Modeling}}},
  booktitle = {Proceedings of the {{Twenty Third International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Niu, Chenhao and Song, Yang and Song, Jiaming and Zhao, Shengjia and Grover, Aditya and Ermon, Stefano},
  date = {2020-06-03},
  pages = {4474--4484},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v108/niu20a.html},
  urldate = {2024-02-24},
  abstract = {Learning generative models for graph-structured data is challenging because graphs are discrete, combinatorial, and the underlying data distribution is invariant to the ordering of nodes. However, most of the existing generative models for graphs are not invariant to the chosen ordering, which might lead to an undesirable bias in the learned distribution. To address this difficulty, we propose a permutation invariant approach to modeling graphs, using the recent framework of score-based generative modeling. In particular, we design a permutation equivariant, multi-channel graph neural network to model the gradient of the data distribution at the input graph (a.k.a., the score function). This permutation equivariant model of gradients implicitly defines a permutation invariant distribution for graphs. We train this graph neural network with score matching and sample from it with annealed Langevin dynamics. In our experiments, we first demonstrate the capacity of this new architecture in learning discrete graph algorithms. For graph generation, we find that our learning approach achieves better or comparable results to existing models on benchmark datasets.},
  eventtitle = {International {{Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/BU6Q5TN3/Niu et al. - 2020 - Permutation Invariant Graph Generation via Score-B.pdf;/home/krawczuk/Zotero/storage/QRJAS4HU/Niu et al. - 2020 - Permutation Invariant Graph Generation via Score-B.pdf}
}

@online{NovelCircuitTopology,
  title = {Novel Circuit Topology Synthesis Method Using Circuit Feature Mining and Symbolic Comparison | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/6800231?casa_token=akoAiJ_81HQAAAAA:waBB34WJe20EgDuESVSR8xnrvHaP718KEb7m87Vv3S9Zbpq9dn18PVRP_yE_ckfngQA3lLhyGf_L},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/GF8XXTFD/6800231.html}
}

@book{ochottaPracticalSynthesisHighPerformance2012,
  title = {Practical {{Synthesis}} of {{High-Performance Analog Circuits}}},
  author = {Ochotta, Emil S. and Mukherjee, Tamal and Rutenbar, Rob A. and Carley, L. Richard},
  date = {2012-12-06},
  eprint = {WAfTBwAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{Springer Science \& Business Media}},
  abstract = {Practical Synthesis of High-Performance Analog Circuits presents a technique for automating the design of analog circuits.  Market competition and the astounding pace of technological innovation exert tremendous pressure on circuit design engineers to turn ideas into products quickly and get them to market. In digital Application Specific Integrated Circuit (ASIC) design, computer aided design (CAD) tools have substantially eased this pressure by automating many of the laborious steps in the design process, thereby allowing the designer to maximise his design expertise.  But the world is not solely digital.  Cellular telephones, magnetic disk drives, neural networks and speech recognition systems are a few of the recent technological innovations that rely on a core of analog circuitry and exploit the density and performance of mixed analog/digital ASICs. To maximize profit, these mixed-signal ASICs must also make it to market as quickly as possible. However, although the engineer working on the digital portion of the ASIC can rely on sophisticated CAD tools to automate much of the design process, there is little help for the engineer working on the analog portion of the chip. With the exception of simulators to verify the circuit design when it is complete, there are almost no general purpose CAD tools that an analog design engineer can take advantage of to automate the analog design flow and reduce his time to market.  Practical Synthesis of High-Performance Analog Circuits presents a new variation-tolerant analog synthesis strategy that is a significant step towards ending the wait for a practical analog synthesis tool. A new synthesis strategy is presented that can fully automate the path from a circuit topology and performance specifications to a sized variation-tolerant circuit schematic. This strategy relies on asymptotic waveform evaluation to predict circuit performance and simulated annealing to solve a novel non-linear infinite programming optimization formulation of the circuit synthesis problem via a sequence of smaller optimization problems.  Practical Synthesis of High-Performance Analog Circuits will be of interest to analog circuit designers, CAD/EDA industry professionals, academics and students.},
  isbn = {978-1-4615-5565-0},
  langid = {english},
  pagetotal = {308},
  keywords = {{Computers / Design, Graphics \& Media / CAD-CAM},Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Circuits / General}
}

@online{OpenSAROpenSource,
  title = {{{OpenSAR}}: {{An Open Source Automated End-to-end SAR ADC Compiler}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643494?casa_token=vdB_Xn_qm0UAAAAA:BSytjW9dx6dpMX8eJP5851lyqjRebOMuql_c0nx_j8rM-oCnbAtSSpK110B5yHJevetfYWeMSYQZ},
  urldate = {2024-02-20}
}

@inproceedings{papezSumProductSetNetworks2023,
  title = {Sum-{{Product-Set Networks}}},
  author = {Papez, Milan and Rektoris, Martin and Pevný, Tomáš and Smidl, Vaclav},
  date = {2023-07-13},
  url = {https://openreview.net/forum?id=8hqxY5fUwg},
  urldate = {2024-02-20},
  abstract = {Daily internet communication relies heavily on tree-structured graphs, embodied by popular data formats such as XML and JSON. However, many recent generative (probabilistic) models utilize neural networks to learn a probability distribution over undirected cyclic graphs. This assumption of a generic graph structure brings various computational challenges, and, more importantly, the presence of non-linearities in neural networks does not permit tractable probabilistic inference. We address these problems by proposing sum-product-set networks, an extension of probabilistic circuits from unstructured tensor data to tree-structured graph data. To this end, we use random finite sets to reflect a variable number of nodes and edges in the graph and to allow for exact and efficient inference. We demonstrate that our tractable model performs comparably to various intractable models based on neural networks.},
  eventtitle = {The 6th {{Workshop}} on {{Tractable Probabilistic Modeling}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/32AFN8B4/Papez et al. - 2023 - Sum-Product-Set Networks.pdf}
}

@article{parkSemiconductorsIntersectionGeoeconomics2023,
  title = {Semiconductors at the {{Intersection}} of {{Geoeconomics}}, {{Technonationalism}}, and {{Global Value Chains}}},
  author = {Park, Seohee},
  date = {2023-08},
  journaltitle = {Social Sciences},
  volume = {12},
  number = {8},
  pages = {466},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2076-0760},
  doi = {10.3390/socsci12080466},
  url = {https://www.mdpi.com/2076-0760/12/8/466},
  urldate = {2024-02-23},
  abstract = {This study provides a historical and contemporary analysis of the United States’ strategies in the global semiconductor industry, framed within Joseph Nye’s three-dimensional chessboard analysis. This study examines the strategic responses of the United States from the 1980s to the present, connecting these shifts to changes in international politics and geoeconomic alliances. It scrutinizes how the U.S. utilized its unipolar power to respond to Japan’s growing semiconductor industry influence in the 1980s and its adoption of free-market principles during the globalization era of the 1990s and 2000s. It further discusses how these multilateral shifts have led to a resurgence of technonationalism in the late 2010s, responding to asymmetric interdependence in the global value chain of the semiconductor industry. This research contributes to the comprehension of the dynamics of the industry within international politics and suggests insights into the ongoing Sino–American competition and strategic realignment in the sector.},
  issue = {8},
  langid = {english},
  keywords = {geoeconomics,global value chain (GVC),globalization,semiconductor industry,technonationalism},
  file = {/home/krawczuk/Zotero/storage/X4HWCMHE/Park - 2023 - Semiconductors at the Intersection of Geoeconomics.pdf}
}

@inproceedings{piechDeepKnowledgeTracing2015,
  title = {Deep {{Knowledge Tracing}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Piech, Chris and Bassen, Jonathan and Huang, Jonathan and Ganguli, Surya and Sahami, Mehran and Guibas, Leonidas J and Sohl-Dickstein, Jascha},
  date = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/bac9162b47c56fc8a4d2a519803d51b3-Abstract.html},
  urldate = {2024-02-22},
  abstract = {Knowledge tracing, where a machine models the knowledge of a student as they interact with coursework, is an established and significantly unsolved problem in computer supported education.In this paper we explore the benefit of using recurrent neural networks to model student learning.This family of models have important advantages over current state of the art methods in that they do not require the explicit encoding of human domain knowledge,and have a far more flexible functional form which can capture substantially more complex student interactions.We show that these neural networks outperform the current state of the art in prediction on real student data,while allowing straightforward interpretation and discovery of structure in the curriculum.These results suggest a promising new line of research for knowledge tracing.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/TYMUETP9/Piech et al. - 2015 - Deep Knowledge Tracing.pdf}
}

@incollection{pilipczukComputingTreeDecompositions2020,
  title = {Computing {{Tree Decompositions}}},
  booktitle = {Treewidth, {{Kernels}}, and {{Algorithms}}: {{Essays Dedicated}} to {{Hans L}}. {{Bodlaender}} on the {{Occasion}} of {{His}} 60th {{Birthday}}},
  author = {Pilipczuk, Michał},
  editor = {Fomin, Fedor V. and Kratsch, Stefan and family=Leeuwen, given=Erik Jan, prefix=van, useprefix=true},
  date = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {189--213},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-42071-0_14},
  url = {https://doi.org/10.1007/978-3-030-42071-0_14},
  urldate = {2024-02-20},
  abstract = {In this chapter we review the most important algorithmic approaches to the following problem: given a graph G, compute a tree decomposition of G of (nearly) optimum width. We present the 4-approximation algorithm running in time \$\$\textbackslash mathcal \{O\}(27\^{}k\textbackslash cdot k\^{}2\textbackslash cdot n\^{}2)\$\$O(27k·k2·n2), which was first proposed by Robertson and Seymour in the Graph Minors series, and we discuss the main ideas behind the exact algorithm of Bodlaender that runs in linear fixed-parameter time~[2].},
  isbn = {978-3-030-42071-0},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/4I9E6FXP/Pilipczuk - 2020 - Computing Tree Decompositions.pdf}
}

@article{pratherTreeCircuits1965,
  title = {On {{Tree Circuits}}},
  author = {Prather, Ronald E.},
  date = {1965-12},
  journaltitle = {IEEE Transactions on Electronic Computers},
  volume = {EC-14},
  number = {6},
  pages = {841--851},
  issn = {0367-7508},
  doi = {10.1109/PGEC.1965.264078},
  url = {https://ieeexplore.ieee.org/abstract/document/4038603?casa_token=T7OMi2npWwcAAAAA:815vqzdNgMR9_rp6Zx4rJ0KfWSqjC7CEzTlv9po0jZLCPu8gVCmdvjmdf5foIFWNizQBdTuOMq5l},
  urldate = {2024-02-20},
  abstract = {This article is primarily concerned with means for finding economical tree circuit realizations-iterative applications of decompositions f(xn, xn-1,..., x1) = Fi(Gi(xn, xn-1,..., xi,..., x1), Hi,(xn, xn-1,..., xi,..., x1), xi) and their associated circuitry-for Boolean functions f(xn, xn-1,..., x1). A uniform estimate or inequality shows that when synthesis is effected with two-input-per-gate circuitry, tree circuits are preferable to those which result from conventional sum of products techniques. The practical significance of the estimate is illustrated by its application to the tree circuit synthesis problem. The relationship of tree circuit theory to decomposition theory is established and the extension of the present theory to the corresponding multiple-output and incompletely-specified problems is indicated.},
  eventtitle = {{{IEEE Transactions}} on {{Electronic Computers}}},
  keywords = {Application software,Boolean functions,Circuit synthesis,Circuit theory,Cost function,Diodes}
}

@article{prautschGeneratingGeneratorUserDriven2023,
  title = {Generating the {{Generator}}: {{A User-Driven}} and {{Template-Based Approach}} towards {{Analog Layout Automation}}},
  shorttitle = {Generating the {{Generator}}},
  author = {Prautsch, Benjamin and Eichler, Uwe and Hatnik, Uwe},
  date = {2023-01},
  journaltitle = {Electronics},
  volume = {12},
  number = {4},
  pages = {1047},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics12041047},
  url = {https://www.mdpi.com/2079-9292/12/4/1047},
  urldate = {2024-02-19},
  abstract = {Various analog design automation attempts have addressed the shortcomings of the still largely manual and, thus, inefficient and risky analog design approach. These methods can roughly be divided into synthesis and procedural generation. An important key aspect has, however, rarely been considered: usability. While synthesis requires sophisticated constraints, procedural generators require expert programmers. Both prevent users from adopting the respective method. Thus, we propose a new approach to automatically create procedural generators in a user-driven way. First, analog generators, which also create symbols and layouts, are utilized during schematic entry to encapsulate common analog building blocks. Second, automatic code creation builds a hierarchical generator for all views with the schematic as input. Third, the approach links the building block generators with the layout through an object-oriented template library that is accessible through generator parameters, allowing the user to control the arrangement. No programming is required to reach this state. We believe that our approach will significantly ease the transition of analog designers to procedural generation. At the same time, the templates allow for a “bridge” to open frameworks and synthesis approaches so that the methodologies can be both better spread and combined. This way, comprehensive frameworks of both synthesis-based and procedural-based analog automation methods can be built in a user-driven way, and designers are enabled to gain early layout insight and ease IP reusability.},
  issue = {4},
  langid = {english},
  keywords = {analog layout,code generation,design automation,EDA,generators,IC design,reuse,templates,usability},
  file = {/home/krawczuk/Zotero/storage/FDLRB8W5/Prautsch et al. - 2023 - Generating the Generator A User-Driven and Templa.pdf}
}

@inreference{ProgramSynthesis2023,
  title = {Program Synthesis},
  booktitle = {Wikipedia},
  date = {2023-11-14T22:02:39Z},
  url = {https://en.wikipedia.org/w/index.php?title=Program_synthesis&oldid=1185150452},
  urldate = {2024-02-20},
  abstract = {In computer science, program synthesis is the task to construct a program that provably satisfies a given high-level formal specification. In contrast to program verification, the program is to be constructed rather than given; however, both fields make use of formal proof techniques, and both comprise approaches of different degrees of automation. In contrast to automatic programming techniques, specifications in program synthesis are usually non-algorithmic statements in an appropriate logical calculus.The primary application of program synthesis is to relieve the programmer of the burden of writing correct, efficient code that satisfies a specification. However, program synthesis also has applications to superoptimization and inference of loop invariants.},
  langid = {english},
  annotation = {Page Version ID: 1185150452},
  file = {/home/krawczuk/Zotero/storage/NU5V4JLC/Program_synthesis.html}
}

@thesis{puertasDirectTreeDecomposition2014,
  type = {Doctoral thesis},
  title = {Direct Tree Decomposition of Geometric Constraint Graphs},
  author = {Puertas, Tarres and Isabel, Marta},
  date = {2014-12-18},
  journaltitle = {TDX (Tesis Doctorals en Xarxa)},
  institution = {{Universitat Politècnica de Catalunya}},
  doi = {10.5821/dissertation-2117-95581},
  url = {https://upcommons.upc.edu/handle/2117/95581},
  urldate = {2024-02-20},
  abstract = {DOI: 10.5821/dissertation-2117-95581},
  langid = {english},
  keywords = {Algorismes computacionals,Àrees temàtiques de la UPC::Informàtica,Disseny assistit per ordinador,Geometria computacional,Grafs,Sistemes CAD-CAM,Teoria de},
  annotation = {Accepted: 2015-01-14T12:39:14Z},
  file = {/home/krawczuk/Zotero/storage/LGC7URIF/Puertas and Isabel - 2014 - Direct tree decomposition of geometric constraint .pdf}
}

@inproceedings{puLearningLearnGraph2021,
  title = {Learning to {{Learn Graph Topologies}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Pu, Xingyue and Cao, Tianyue and Zhang, Xiaoyun and Dong, Xiaowen and Chen, Siheng},
  date = {2021},
  volume = {34},
  pages = {4249--4262},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/21e4ef94f2a6b23597efabaec584b504-Abstract.html},
  urldate = {2024-02-20},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/HR7KWEZT/Pu et al. - 2021 - Learning to Learn Graph Topologies.pdf}
}

@online{qinSparseTrainingDiscrete2023e,
  title = {Sparse {{Training}} of {{Discrete Diffusion Models}} for {{Graph Generation}}},
  author = {Qin, Yiming and Vignac, Clement and Frossard, Pascal},
  date = {2023-11-03},
  eprint = {2311.02142},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.02142},
  url = {http://arxiv.org/abs/2311.02142},
  urldate = {2024-02-20},
  abstract = {Generative models for graphs often encounter scalability challenges due to the inherent need to predict interactions for every node pair. Despite the sparsity often exhibited by real-world graphs, the unpredictable sparsity patterns of their adjacency matrices, stemming from their unordered nature, leads to quadratic computational complexity. In this work, we introduce SparseDiff, a denoising diffusion model for graph generation that is able to exploit sparsity during its training phase. At the core of SparseDiff is a message-passing neural network tailored to predict only a subset of edges during each forward pass. When combined with a sparsity-preserving noise model, this model can efficiently work with edge lists representations of graphs, paving the way for scalability to much larger structures. During the sampling phase, SparseDiff iteratively populates the adjacency matrix from its prior state, ensuring prediction of the full graph while controlling memory utilization. Experimental results show that SparseDiff simultaneously matches state-of-the-art in generation performance on both small and large graphs, highlighting the versatility of our method.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/DZTTPU7D/Qin et al. - 2023 - Sparse Training of Discrete Diffusion Models for G.pdf;/home/krawczuk/Zotero/storage/CF5ZF7XN/2311.html}
}

@article{rabinovichChurchSynthesisProblem2007,
  title = {The {{Church Synthesis Problem}} with {{Parameters}}},
  author = {Rabinovich, Alexander},
  date = {2007-11-14},
  journaltitle = {Logical Methods in Computer Science},
  volume = {Volume 3, Issue 4},
  eprint = {0708.3477},
  eprinttype = {arxiv},
  eprintclass = {cs},
  pages = {1233},
  issn = {1860-5974},
  doi = {10.2168/LMCS-3(4:9)2007},
  url = {http://arxiv.org/abs/0708.3477},
  urldate = {2024-02-20},
  abstract = {For a two-variable formula \&psi;(X,Y) of Monadic Logic of Order (MLO) the Church Synthesis Problem concerns the existence and construction of an operator Y=F(X) such that \&psi;(X,F(X)) is universally valid over Nat. B\textbackslash "\{u\}chi and Landweber proved that the Church synthesis problem is decidable; moreover, they showed that if there is an operator F that solves the Church Synthesis Problem, then it can also be solved by an operator defined by a finite state automaton or equivalently by an MLO formula. We investigate a parameterized version of the Church synthesis problem. In this version \&psi; might contain as a parameter a unary predicate P. We show that the Church synthesis problem for P is computable if and only if the monadic theory of},
  keywords = {Computer Science - Logic in Computer Science,F.4.1,F.4.3},
  file = {/home/krawczuk/Zotero/storage/IECGWSHX/Rabinovich - 2007 - The Church Synthesis Problem with Parameters.pdf;/home/krawczuk/Zotero/storage/4XQW8MDA/0708.html}
}

@inproceedings{renWhyAreGraph2022b,
  title = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}? ({{Invited Paper}})},
  shorttitle = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}?},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Ren, Haoxing and Nath, Siddhartha and Zhang, Yanqing and Chen, Hao and Liu, Mingjie},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3561093},
  url = {https://dl.acm.org/doi/10.1145/3508352.3561093},
  urldate = {2024-02-20},
  abstract = {In this paper, we discuss the source of effectiveness of Graph Neural Networks (GNNs) in EDA, particularly in the VLSI design automation domain. We argue that the effectiveness comes from the fact that GNNs implicitly embed the prior knowledge and inductive biases associated with given VLSI tasks, which is one of the three approaches to make a learning algorithm physics-informed. These inductive biases are different to those common used in GNNs designed for other structured data, such as social networks and citation networks. We will illustrate this principle with several recent GNN examples in the VLSI domain, including predictive tasks such as switching activity prediction, timing prediction, parasitics prediction, layout symmetry prediction, as well as optimization tasks such as gate sizing and macro and cell transistor placement. We will also discuss the challenges of applications of GNN and the opportunity of applying self-supervised learning techniques with GNN for VLSI optimization.},
  isbn = {978-1-4503-9217-4},
  file = {/home/krawczuk/Zotero/storage/QW9J4549/Ren et al. - 2022 - Why are Graph Neural Networks Effective for EDA Pr.pdf}
}

@inproceedings{renWhyAreGraph2022c,
  title = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}? ({{Invited Paper}})},
  shorttitle = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}?},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Ren, Haoxing and Nath, Siddhartha and Zhang, Yanqing and Chen, Hao and Liu, Mingjie},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3561093},
  url = {https://dl.acm.org/doi/10.1145/3508352.3561093},
  urldate = {2024-02-24},
  abstract = {In this paper, we discuss the source of effectiveness of Graph Neural Networks (GNNs) in EDA, particularly in the VLSI design automation domain. We argue that the effectiveness comes from the fact that GNNs implicitly embed the prior knowledge and inductive biases associated with given VLSI tasks, which is one of the three approaches to make a learning algorithm physics-informed. These inductive biases are different to those common used in GNNs designed for other structured data, such as social networks and citation networks. We will illustrate this principle with several recent GNN examples in the VLSI domain, including predictive tasks such as switching activity prediction, timing prediction, parasitics prediction, layout symmetry prediction, as well as optimization tasks such as gate sizing and macro and cell transistor placement. We will also discuss the challenges of applications of GNN and the opportunity of applying self-supervised learning techniques with GNN for VLSI optimization.},
  isbn = {978-1-4503-9217-4}
}

@inproceedings{renWhyAreGraph2022d,
  title = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}? ({{Invited Paper}})},
  shorttitle = {Why Are {{Graph Neural Networks Effective}} for {{EDA Problems}}?},
  booktitle = {Proceedings of the 41st {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  author = {Ren, Haoxing and Nath, Siddhartha and Zhang, Yanqing and Chen, Hao and Liu, Mingjie},
  date = {2022-12-22},
  series = {{{ICCAD}} '22},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3508352.3561093},
  url = {https://dl.acm.org/doi/10.1145/3508352.3561093},
  urldate = {2024-02-24},
  abstract = {In this paper, we discuss the source of effectiveness of Graph Neural Networks (GNNs) in EDA, particularly in the VLSI design automation domain. We argue that the effectiveness comes from the fact that GNNs implicitly embed the prior knowledge and inductive biases associated with given VLSI tasks, which is one of the three approaches to make a learning algorithm physics-informed. These inductive biases are different to those common used in GNNs designed for other structured data, such as social networks and citation networks. We will illustrate this principle with several recent GNN examples in the VLSI domain, including predictive tasks such as switching activity prediction, timing prediction, parasitics prediction, layout symmetry prediction, as well as optimization tasks such as gate sizing and macro and cell transistor placement. We will also discuss the challenges of applications of GNN and the opportunity of applying self-supervised learning techniques with GNN for VLSI optimization.},
  isbn = {978-1-4503-9217-4}
}

@article{robertsonGraphMinorsObstructions1991,
  title = {Graph Minors. {{X}}. {{Obstructions}} to Tree-Decomposition},
  author = {Robertson, Neil and Seymour, P. D},
  date = {1991-07-01},
  journaltitle = {Journal of Combinatorial Theory, Series B},
  shortjournal = {Journal of Combinatorial Theory, Series B},
  volume = {52},
  number = {2},
  pages = {153--190},
  issn = {0095-8956},
  doi = {10.1016/0095-8956(91)90061-N},
  url = {https://www.sciencedirect.com/science/article/pii/009589569190061N},
  urldate = {2024-02-20},
  abstract = {Roughly, a graph has small “tree-width” if it can be constructed by piecing small graphs together in a tree structure. Here we study the obstructions to the existence of such a tree structure. We find, for instance: 1.(i) a minimax formula relating tree-width with the largest such obstructions2.(ii) an association between such obstructions and large grid minors of the graph3.(iii) a “tree-decomposition” of the graph into pieces corresponding with the obstructions. These results will be of use in later papers.}
}

@article{rojecAnalogCircuitTopology2018,
  title = {Analog {{Circuit Topology Representation}} for {{Automated Synthesis}} and {{Optimization}}},
  author = {Rojec, Žiga and Olenšek, Jernej and Fajfar, Iztok},
  date = {2018-06-28},
  journaltitle = {Informacije MIDEM},
  volume = {48},
  number = {1},
  pages = {29--40},
  issn = {2232-6979},
  url = {https://ojs.midem-drustvo.si/index.php/InfMIDEM/article/view/465},
  urldate = {2024-02-19},
  abstract = {For several decades, computers have helped analog designers with circuit simulation and evaluation. To further simplify and speed-up designer’s work, novel methods are being introduced that help to fine-tune numerical parameters to meet the performance criteria. With a lack of capable engineers, a shortage of specific knowledge or time to design an analog building block, software for fully automated synthesis of both topology and parameters is becoming crucial. Most research in this field is based on circuit modifications according to evolutionary principles of surviving of the fittest. One of the challenges of the design of appropriate software is a representation of a circuit topology that will allow topology modifications with the smallest possible computational effort. Many existing solutions suffer either from the uncontrolled growth of the size of the circuit (so-called bloat) or from the limitation of the topology structure to a set of predefined blocks. In this paper, we discuss an analog circuit topology representation in a form of a binary upper-triangular matrix that is both bloat safe and offers a large solution space. We describe the basic structure of the matrix, the redundancy phenomena of logical elements, and the translation of the matrix representation to a regular SPICE netlist. We use an evolutionary algorithm to evolve the topology matrix and a classical parameter optimization algorithm to tune the circuit parameters. Based on a high-level circuit definition and a fixed building-block bank, our topology representation technique showed success in a fully automatic synthesis of passive circuits. We demonstrate the ability to automatically discover a passive high-pass filter topology.},
  issue = {1},
  langid = {english},
  keywords = {⛔ No DOI found,analog circuits,Automated synthesis,computer-aided design,evolutionary algorithms},
  file = {/home/krawczuk/Zotero/storage/UIG4WFV4/Rojec et al. - 2018 - Analog Circuit Topology Representation for Automat.pdf}
}

@inproceedings{rueggGeneralizationDirectedGraph2016,
  title = {A {{Generalization}} of the {{Directed Graph Layering Problem}}},
  booktitle = {Graph {{Drawing}} and {{Network Visualization}}},
  author = {Rüegg, Ulf and Ehlers, Thorsten and Spönemann, Miro and family=Hanxleden, given=Reinhard, prefix=von, useprefix=true},
  editor = {Hu, Yifan and Nöllenburg, Martin},
  date = {2016},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {196--208},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-50106-2_16},
  abstract = {The Directed Layering Problem (DLP) solves a step of the widely used layer-based approach to automatically draw directed acyclic graphs. To cater for cyclic graphs, usually a preprocessing step is used that solves the Feedback Arc Set Problem (FASP) to make the graph acyclic before a layering is determined.},
  isbn = {978-3-319-50106-2},
  langid = {english},
  keywords = {Feedback arc set,Integer programming,Layer assignment,Layer-based layout,Linear arrangement},
  file = {/home/krawczuk/Zotero/storage/CYM75N2Q/Rüegg et al. - 2016 - A Generalization of the Directed Graph Layering Pr.pdf}
}

@inproceedings{safariDWidthMoreNatural2005,
  title = {D-{{Width}}: {{A More Natural Measure}} for {{Directed Tree Width}}},
  shorttitle = {D-{{Width}}},
  booktitle = {Mathematical {{Foundations}} of {{Computer Science}} 2005},
  author = {Safari, Mohammad Ali},
  editor = {Jȩdrzejowicz, Joanna and Szepietowski, Andrzej},
  date = {2005},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {745--756},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11549345_64},
  abstract = {Due to extensive research on tree-width for undirected graphs and due to its many applications in various fields it has been a natural desire for many years to generalize the idea of tree decomposition to directed graphs, but since many parameters in tree-width behave very differently in the world of digraphs, the generalization is still in its preliminary steps.},
  isbn = {978-3-540-31867-5},
  langid = {english}
}

@article{sanchezComprehensiveSurveyElectronic2023,
  title = {A {{Comprehensive Survey}} on {{Electronic Design Automation}} and {{Graph Neural Networks}}: {{Theory}} and {{Applications}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Electronic Design Automation}} and {{Graph Neural Networks}}},
  author = {Sánchez, Daniela and Servadei, Lorenzo and Kiprit, Gamze Naz and Wille, Robert and Ecker, Wolfgang},
  date = {2023-03-31},
  journaltitle = {ACM Transactions on Design Automation of Electronic Systems},
  shortjournal = {ACM Trans. Des. Autom. Electron. Syst.},
  volume = {28},
  number = {2},
  pages = {1--27},
  issn = {1084-4309, 1557-7309},
  doi = {10.1145/3543853},
  url = {https://dl.acm.org/doi/10.1145/3543853},
  urldate = {2024-02-20},
  abstract = {Driven by Moore’s law, the chip design complexity is steadily increasing. Electronic Design Automation (EDA) has been able to cope with the challenging very large-scale integration process, assuring scalability, reliability, and proper time-to-market. However, EDA approaches are time and resource demanding, and they often do not guarantee optimal solutions. To alleviate these, Machine Learning (ML) has been incorporated into many stages of the design flow, such as in placement and routing. Many solutions employ Euclidean data and ML techniques without considering that many EDA objects are represented naturally as graphs. The trending Graph Neural Networks (GNNs) are an opportunity to solve EDA problems directly using graph structures for circuits, intermediate Register Transfer Levels, and netlists. In this article, we present a comprehensive review of the existing works linking the EDA flow for chip design and GNNs. We map those works to a design pipeline by defining graphs, tasks, and model types. Furthermore, we analyze their practical implications and outcomes. We conclude by summarizing challenges faced when applying GNNs within the EDA design flow.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/QF9XR76W/Sánchez et al. - 2023 - A Comprehensive Survey on Electronic Design Automa.pdf}
}

@article{sarkarSpectralCharacterizationHierarchical2013a,
  title = {Spectral {{Characterization}} of {{Hierarchical Network Modularity}} and {{Limits}} of {{Modularity Detection}}},
  author = {Sarkar, Somwrita and Henderson, James A. and Robinson, Peter A.},
  date = {2013-01-28},
  journaltitle = {PLoS ONE},
  shortjournal = {PLoS One},
  volume = {8},
  number = {1},
  eprint = {23382895},
  eprinttype = {pmid},
  pages = {e54383},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0054383},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3557301/},
  urldate = {2024-02-22},
  abstract = {Many real world networks are reported to have hierarchically modular organization. However, there exists no algorithm-independent metric to characterize hierarchical modularity in a complex system. The main results of the paper are a set of methods to address this problem. First, classical results from random matrix theory are used to derive the spectrum of a typical stochastic block model hierarchical modular network form. Second, it is shown that hierarchical modularity can be fingerprinted using the spectrum of its largest eigenvalues and gaps between clusters of closely spaced eigenvalues that are well separated from the bulk distribution of eigenvalues around the origin. Third, some well-known results on fingerprinting non-hierarchical modularity in networks automatically follow as special cases, threreby unifying these previously fragmented results. Finally, using these spectral results, it is found that the limits of detection of modularity can be empirically established by studying the mean values of the largest eigenvalues and the limits of the bulk distribution of eigenvalues for an ensemble of networks. It is shown that even when modularity and hierarchical modularity are present in a weak form in the network, they are impossible to detect, because some of the leading eigenvalues fall within the bulk distribution. This provides a threshold for the detection of modularity. Eigenvalue distributions of some technological, social, and biological networks are studied, and the implications of detecting hierarchical modularity in real world networks are discussed.},
  pmcid = {PMC3557301},
  file = {/home/krawczuk/Zotero/storage/TL5RJIRA/Sarkar et al. - 2013 - Spectral Characterization of Hierarchical Network .pdf}
}

@inproceedings{sawadaNetworkRoutingOptimization2020,
  title = {Network {{Routing Optimization Based}} on {{Machine Learning Using Graph Networks Robust}} against {{Topology Change}}},
  booktitle = {2020 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  author = {Sawada, Kaku and Kotani, Daisuke and Okabe, Yasuo},
  date = {2020-01},
  pages = {608--615},
  issn = {1976-7684},
  doi = {10.1109/ICOIN48656.2020.9016573},
  url = {https://ieeexplore.ieee.org/document/9016573},
  urldate = {2024-02-19},
  abstract = {There is an increasing demand of real-time routing optimazation using Sotware Defined Networking (SDN) for better Quality of Service. Since the problem of finding the optimum routing for any QoS metric is hard to solve for a medium or larger size network, quasi-optimization using metaheuristics, such as Genetic Algorithm (GA) and Simulated Annealing (SA), have been investigated, but it is still impossible to satisfy the requirement of real-time optimization. There have been some attempts to solve this with machine learning. By learning a model beforehand, it is possible to output a near-optimal solution in a short time during network operation. The open problem with this approach is that machine learning models cannot deal with topology change of the network. In this paper, we create a model which is robust for topology change by using Graph Networks. Applying the proposed model to maximum bandwidth utilization, we have gotten the accuracy of about 61.0\% for solution of GA, and the prediction time is 150 times faster than GA.},
  eventtitle = {2020 {{International Conference}} on {{Information Networking}} ({{ICOIN}})},
  keywords = {Computational modeling,Graph Networks,Machine learning,Machine Learning,Network topology,Neural networks,Quality of service,Routing,Routing Opti-mization,Software Defined Networking,Topology},
  file = {/home/krawczuk/Zotero/storage/UQWYC8WG/Sawada et al. - 2020 - Network Routing Optimization Based on Machine Lear.pdf;/home/krawczuk/Zotero/storage/PCEJTVFD/9016573.html}
}

@article{scarselliGraphNeuralNetwork2009,
  title = {The {{Graph Neural Network Model}}},
  author = {Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  date = {2009-01},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {20},
  number = {1},
  pages = {61--80},
  issn = {1941-0093},
  doi = {10.1109/TNN.2008.2005605},
  url = {https://ieeexplore.ieee.org/document/4700287},
  urldate = {2024-02-24},
  abstract = {Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function tau(G,n) isin IRm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Biological system modeling,Biology,Chemistry,Computer vision,Data engineering,Data mining,graph neural networks (GNNs),graph processing,Graphical domains,Neural networks,Parameter estimation,Pattern recognition,recursive neural networks,Supervised learning},
  file = {/home/krawczuk/Zotero/storage/CPF3TLQH/Scarselli et al. - 2009 - The Graph Neural Network Model.pdf;/home/krawczuk/Zotero/storage/2Z4EWFRJ/4700287.html}
}

@inproceedings{scheibleAutomationAnalogIC2015,
  title = {Automation of {{Analog IC Layout}}: {{Challenges}} and {{Solutions}}},
  shorttitle = {Automation of {{Analog IC Layout}}},
  booktitle = {Proceedings of the 2015 {{Symposium}} on {{International Symposium}} on {{Physical Design}}},
  author = {Scheible, Juergen and Lienig, Jens},
  date = {2015-03-29},
  series = {{{ISPD}} '15},
  pages = {33--40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2717764.2717781},
  url = {https://dl.acm.org/doi/10.1145/2717764.2717781},
  urldate = {2024-02-19},
  abstract = {Physical analog IC design has not been automated to the same degree as digital IC design. This shortfall is primarily rooted in the analog IC design problem itself, which is considerably more complex even for small problem sizes. Significant progress has been made in analog automation in several R\&D target areas in recent years. Constraint engineering and generator-based module approaches are among the innovations that have emerged. Our paper will first present a brief review of the state of the art of analog layout automation. We will then introduce active and open research areas and present two visions -- a "continuous layout design flow" and a "bottom-up meets top-down design flow" -- which could significantly push analog design automation towards its goal of analog synthesis.},
  isbn = {978-1-4503-3399-3},
  keywords = {analog design,analog layout automation,constraint engineering,design methodology,layout,physical design},
  file = {/home/krawczuk/Zotero/storage/YMDJ3H6B/Scheible and Lienig - 2015 - Automation of Analog IC Layout Challenges and Sol.pdf}
}

@inproceedings{scheibleAutomationAnalogIC2015a,
  title = {Automation of {{Analog IC Layout}}: {{Challenges}} and {{Solutions}}},
  shorttitle = {Automation of {{Analog IC Layout}}},
  booktitle = {Proceedings of the 2015 {{Symposium}} on {{International Symposium}} on {{Physical Design}}},
  author = {Scheible, Juergen and Lienig, Jens},
  date = {2015-03-29},
  series = {{{ISPD}} '15},
  pages = {33--40},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2717764.2717781},
  url = {https://dl.acm.org/doi/10.1145/2717764.2717781},
  urldate = {2024-02-19},
  abstract = {Physical analog IC design has not been automated to the same degree as digital IC design. This shortfall is primarily rooted in the analog IC design problem itself, which is considerably more complex even for small problem sizes. Significant progress has been made in analog automation in several R\&D target areas in recent years. Constraint engineering and generator-based module approaches are among the innovations that have emerged. Our paper will first present a brief review of the state of the art of analog layout automation. We will then introduce active and open research areas and present two visions -- a "continuous layout design flow" and a "bottom-up meets top-down design flow" -- which could significantly push analog design automation towards its goal of analog synthesis.},
  isbn = {978-1-4503-3399-3},
  keywords = {analog design,analog layout automation,constraint engineering,design methodology,layout,physical design},
  file = {/home/krawczuk/Zotero/storage/P45CZEHG/Scheible and Lienig - 2015 - Automation of Analog IC Layout Challenges and Sol.pdf}
}

@inproceedings{schmittNeuralCircuitSynthesis2021a,
  title = {Neural {{Circuit Synthesis}} from {{Specification Patterns}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Schmitt, Frederik and Hahn, Christopher and Rabe, Markus N and Finkbeiner, Bernd},
  date = {2021},
  volume = {34},
  pages = {15408--15420},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/8230bea7d54bcdf99cdfe85cb07313d5-Abstract.html},
  urldate = {2024-02-24},
  abstract = {We train hierarchical Transformers on the task of synthesizing hardware circuits directly out of high-level logical specifications in linear-time temporal logic (LTL). The LTL synthesis problem is a well-known algorithmic challenge with a long history and an annual competition is organized to track the improvement of algorithms and tooling over time. New approaches using machine learning might open a lot of possibilities in this area, but suffer from the lack of sufficient amounts of training data. In this paper, we consider a method to generate large amounts of additional training data, i.e., pairs of specifications and circuits implementing them. We ensure that this synthetic data is sufficiently close to human-written specifications by mining common patterns from the specifications used in the synthesis competitions. We show that hierarchical Transformers trained on this synthetic data solve a significant portion of problems from the synthesis competitions, and even out-of-distribution examples from a recent case study.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/MNXLFDLE/Schmitt et al. - 2021 - Neural Circuit Synthesis from Specification Patter.pdf}
}

@online{SchnyderGridEmbeddingAlgorithm,
  title = {Schnyder's {{Grid-Embedding Algorithm}}},
  url = {https://ics.uci.edu/~eppstein/gina/schnyder/},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/QKMBLTSL/schnyder.html}
}

@article{seljakSlicedNormalizingFlow,
  title = {Sliced {{Normalizing Flow}} Optimization and Sampling},
  author = {Seljak, Uroš},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/DZUVSSBG/Seljak - Sliced Normalizing Flow optimization and sampling.pdf}
}

@online{SemiconductorIndustryAssociation2024,
  title = {Semiconductor {{Industry Association}}},
  date = {2024-02-05},
  url = {https://www.semiconductors.org/policies/tax/market-data/?type=post},
  urldate = {2024-02-19},
  langid = {american},
  organization = {{Semiconductor Industry Association}},
  file = {/home/krawczuk/Zotero/storage/X545CNMK/market-data.html}
}

@inproceedings{settaluriAutoCktDeepReinforcement2020f,
  title = {{{AutoCkt}}: {{Deep Reinforcement Learning}} of {{Analog Circuit Designs}}},
  shorttitle = {{{AutoCkt}}},
  booktitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Settaluri, Keertana and Haj-Ali, Ameer and Huang, Qijing and Hakhamaneshi, Kourosh and Nikolic, Borivoje},
  date = {2020-03},
  pages = {490--495},
  publisher = {{IEEE}},
  location = {{Grenoble, France}},
  doi = {10.23919/DATE48585.2020.9116200},
  url = {https://ieeexplore.ieee.org/document/9116200/},
  urldate = {2024-02-20},
  abstract = {Domain specialization under energy constraints in deeply-scaled CMOS has been driving the need for agile development of Systems on a Chip (SoCs). While digital subsystems have design flows that are conducive to rapid iterations from specification to layout, analog and mixed-signal modules face the challenge of a long human-in-the-middle iteration loop that requires expert intuition to verify that post-layout circuit parameters meet the original design specification. Existing automated solutions that optimize circuit parameters for a given target design specification have limitations of being schematic-only, inaccurate, sample-inefficient or not generalizable. This work presents AutoCkt, a machine learning optimization framework trained using deep reinforcement learning that not only finds post-layout circuit parameters for a given target specification, but also gains knowledge about the entire design space through a sparse subsampling technique. Our results show that for multiple circuit topologies, AutoCkt is able to converge and meet all target specifications on at least 96.3\% of tested design goals in schematic simulation, on average 40× faster than a traditional genetic algorithm. Using the Berkeley Analog Generator, AutoCkt is able to design 40 LVS passed operational amplifiers in 68 hours, 9.6× faster than the state-of-the-art when considering layout parasitics.},
  eventtitle = {2020 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  isbn = {978-3-9819263-4-7},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/L2XLXA5J/Settaluri et al. - 2020 - AutoCkt Deep Reinforcement Learning of Analog Cir.pdf}
}

@article{settaluriAutomatedDesignAnalog2022a,
  title = {Automated {{Design}} of {{Analog Circuits Using Reinforcement Learning}}},
  author = {Settaluri, Keertana and Liu, Zhaokai and Khurana, Rishubh and Mirhaj, Arash and Jain, Rajeev and Nikolic, Borivoje},
  date = {2022-09},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  volume = {41},
  number = {9},
  pages = {2794--2807},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2021.3120547},
  url = {https://ieeexplore.ieee.org/document/9576505/},
  urldate = {2024-02-20},
  abstract = {Analog and mixed-signal (AMS) blocks are often a crucial and time-consuming part of System-on-Chip (SoC) design, primarily due to a manual circuit and layout iterations. Existing automated solutions for selecting circuit parameters for a given target specification are often not efficient, accurate, or reliable. In order for an automated sizing tool to be practical, we posit that it must: 1) return valid results for a large range of target specifications; 2) understand where and why it is unable to meet certain specifications; 3) consider true layout parasitic simulations for complete end-to-end design; and 4) be automated, allowing most of the design effort to fall on the tool. In this article, we address these critical points by establishing an automated reinforcement learning framework, AutoCkt, by 1) successfully deploying it on a complex two-stage transimpedance amplifier and two-stage folded cascode with biasing in the 16-nm FinFet technology; 2) implementing a new combined distribution deployment algorithm to improve efficiency; 3) analyzing in-depth the efficacy of the trained agent; and 4) demonstrating the functionality of this tool when considering a topology that is highly sensitive to layout parasitics. Our algorithm not only successfully reaches unique, valid, and practical performances, but also does so in state-of-the-art run time, up to 38X more efficient than prior work. In addition, our tool averages just four parasitic simulations obtained by using the Berkeley Analog Generator, to achieve a target specification post-layout for the folded cascode. AutoCkt successfully generates LVS-passed designs with validation in process corner variation results.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/6Z5FZJF9/Settaluri et al. - 2022 - Automated Design of Analog Circuits Using Reinforc.pdf}
}

@book{shaulyDesignRulesSemiconductor2022,
  title = {Design {{Rules}} in a {{Semiconductor Foundry}}},
  author = {Shauly, Eitan N.},
  date = {2022-11-30},
  eprint = {RdaTEAAAQBAJ},
  eprinttype = {googlebooks},
  publisher = {{CRC Press}},
  abstract = {Nowadays over 50\% of integrated circuits are fabricated at wafer foundries. This book presents a foundry-integrated perspective of the field and is a comprehensive and up-to-date manual designed to serve process, device, layout, and design engineers. It comprises chapters carefully selected to cover topics relevant for them to deal with their work. The book provides an insight into the different types of design rules (DRs) and considerations for setting new DRs. It discusses isolation, gate patterning, S/D contacts, metal lines, MOL, air gaps, and so on. It explains in detail the layout rules needed to support advanced planarization processes, different types of dummies, and related utilities as well as presents a large set of guidelines and layout-aware modeling for RF CMOS and analog modules. It also discusses the layout DRs for different mobility enhancement techniques and their related modeling, listing many of the dedicated rules for static random-access memory (SRAM), embedded polyfuse (ePF), and LogicNVM. The book also provides the setting and calibration of the process parameters set and describes the 28\textasciitilde 20 nm planar MOSFET process flow for low-power and high-performance mobile applications in a step-by-step manner. It includes FEOL and BEOL physical and environmental tests for qualifications together with automotive qualification and design for automotive (DfA). Written for the professionals, the book belongs to the bookshelf of microelectronic discipline experts.},
  isbn = {978-1-00-063135-7},
  langid = {english},
  pagetotal = {831},
  keywords = {Science / Life Sciences / General,Science / Physics / General,Science / Physics / Optics \& Light,Technology \& Engineering / Electrical,Technology \& Engineering / Electronics / Microelectronics,Technology \& Engineering / Electronics / Optoelectronics,Technology \& Engineering / Environmental / General,Technology \& Engineering / Materials Science / General,Technology \& Engineering / Microwaves}
}

@article{shiAutomaticGenerationMacromodels2023,
  title = {Automatic Generation of Macromodels and Design Equations for Application to {{Op Amp}} Design},
  author = {Shi, Guoyong},
  date = {2023-10},
  journaltitle = {International Journal of Circuit Theory and Applications},
  shortjournal = {Circuit Theory \& Apps},
  volume = {51},
  number = {10},
  pages = {4521--4549},
  issn = {0098-9886, 1097-007X},
  doi = {10.1002/cta.3673},
  url = {https://onlinelibrary.wiley.com/doi/10.1002/cta.3673},
  urldate = {2024-02-19},
  abstract = {Summary             Operational amplifier (Op Amp) is a special class of analog integrated circuits. Multiple‐stage Op Amp design is of particular interest recently. However, analysis of this class of circuits has to be done manually and requires advanced skills. Automatic analysis of this class of circuits is studied in this paper, which takes a novel approach by circuit recognition and symbolic generation. Circuit recognition is deterministic, implemented by deterministic circuit partitioning and functional block extraction methods. Symbolic analysis of extracted circuit blocks can subsequently be performed with greatly reduced complexity while generated models and equations bear more useful information for circuit design. Symbolic results so generated can be applied in design space exploration, including tasks like finding the limitation in circuit topology, conducting initial sizing, and determining performance tradeoffs. This paper proposes the key algorithms for realizing the recognition of a few well‐known functional blocks frequently used in CMOS Op Amp circuits and further explores the possibility of using the recognized circuit cells in combination with the gm/ID method for circuit sizing. Preliminary tests show that this approach is a potential candidate for locating the performance boundaries of multiple‐stage Op Amps.},
  langid = {english}
}

@article{shiGraphPairDecisionDiagram2013,
  title = {Graph-{{Pair Decision Diagram Construction}} for {{Topological Symbolic Circuit Analysis}}},
  author = {Shi, Guoyong},
  date = {2013-02},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {32},
  number = {2},
  pages = {275--288},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2012.2217963},
  url = {https://ieeexplore.ieee.org/abstract/document/6416104},
  urldate = {2024-02-20},
  abstract = {Symbolic circuit analysis is concerned with analytical construction of circuit response in the frequency (or time) domain, for which an efficient data structure is required. Recent research has justified that the binary decision diagram (BDD) is a superior data structure with the following distinguishing feature: a large number of product terms can be compactly represented by a BDD, on which numerical computations and analytical deductions can be performed directly. Using BDD for symbolic circuit analysis requires an efficient method for construction. In this paper, a graph-based construction method, called graph-pair decision diagram (GPDD), is developed. Given a small-signal circuit, a pair of graphs representing the circuit is created, from which a GPDD is constructed by successively reducing the graph pair. The GPDD algorithm, which generates cancellation-free symbolic terms, differs from the existing determinant decision diagram (DDD) algorithm. Detailed theory and implementable algorithms for the GPDD construction are developed, and a runtime performance comparison to DDD is made. It is demonstrated that the runtime performance using GPDD is comparable to that of DDD in terms of time and memory complexity for exact symbolic analysis, although the GPDD algorithm has to generate a much larger number of symbolic product terms.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Algorithm design and analysis,Analog design automation,binary decision diagram (BDD),Boolean functions,Circuit analysis,Data structures,determinant decision diagram (DDD),graph-pair decision diagram (GPDD),implicit enumeration,Integrated circuit modeling,SPICE,symbolic analysis,Voltage control},
  file = {/home/krawczuk/Zotero/storage/UNI5IUY9/6416104.html}
}

@article{shinLAYGO2CustomLayout2023,
  title = {{{LAYGO2}}: {{A Custom Layout Generation Engine Based}} on {{Dynamic Templates}} and {{Grids}} for {{Advanced CMOS Technologies}}},
  shorttitle = {{{LAYGO2}}},
  author = {Shin, Taeho and Lee, Dongjun and Kim, Dongwhee and Sung, Gaeryun and Shin, Wookjin and Jo, Yunseong and Park, Hyungjoo and Han, Jaeduk},
  date = {2023-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {12},
  pages = {4402--4412},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2023.3294462},
  url = {https://ieeexplore.ieee.org/document/10182288},
  urldate = {2024-02-19},
  abstract = {This article presents an automatic layout generation framework in advanced CMOS technologies. The framework extends the template-and-grid-based layout generation methodology to produce optimal layouts more efficiently. Layout templates and grids are dynamically created and adjusted during the generation phase to provide more reusability and flexibility. Virtual instances are used to encapsulate the dynamically generated layout structures. Internal node probes embedded in the dynamic templates capture parasitic effects precisely. The framework also implements various post-processing functions to handle process-specific tasks while maintaining the overall process portability of procedural layout generators. The post-processing functions include cut/dummy pattern generation and multiple-patterning adjustment. The generator description coverage is enhanced with circular grid indexing/slicing and conditional conversion operators. The layout generation framework is applied to generate various DRC/LVS clean layouts automatically in advanced CMOS technologies, achieving 0.66–249.35 transistors-per-line (the ratio of the generated transistor count to the source lines of code) generation efficiencies.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,CMOS technology,Codes,design rules,full-custom circuits,Generators,Layout,layout generation,Organizations,Routing,Task analysis},
  file = {/home/krawczuk/Zotero/storage/8DCBUVXC/10182288.html}
}

@online{signal-aardvark-4179MetaGPTGrosslyMisreported2024,
  type = {Reddit Post},
  title = {[{{D}}] {{MetaGPT}} Grossly Misreported Baseline Numbers and Got an {{ICLR Oral}}!},
  author = {Signal-Aardvark-4179},
  date = {2024-02-22T17:06:50},
  url = {www.reddit.com/r/MachineLearning/comments/1axbm0f/d_metagpt_grossly_misreported_baseline_numbers/},
  urldate = {2024-02-22},
  organization = {{r/MachineLearning}},
  file = {/home/krawczuk/Zotero/storage/9JCG4UYV/d_metagpt_grossly_misreported_baseline_numbers.html}
}

@online{SignalDivisionAwareAnalogCircuit,
  title = {Signal-{{Division-Aware Analog Circuit Topology Synthesis Aided}} by {{Transfer Learning}} | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/10045726?casa_token=gzH1HcQ4RzQAAAAA:aj34Uh6FCyG-oxCWsuMwBYYzkcx1LbJuA1UVG42YWfTN0Mw-7ck2ZV_whOfZBV4y2RtXUz-h40nC},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/TQ5ITJF5/10045726.html}
}

@inproceedings{singhalLearningTransformGeneralizable2023,
  title = {Learning to {{Transform}} for {{Generalizable Instance-wise Invariance}}},
  author = {Singhal, Utkarsh and Esteves, Carlos and Makadia, Ameesh and Yu, Stella X.},
  date = {2023},
  pages = {6211--6221},
  url = {https://openaccess.thecvf.com/content/ICCV2023/html/Singhal_Learning_to_Transform_for_Generalizable_Instance-wise_Invariance_ICCV_2023_paper.html},
  urldate = {2024-02-20},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/A5A6R52I/Singhal et al. - 2023 - Learning to Transform for Generalizable Instance-w.pdf}
}

@inproceedings{sohl-dicksteinDeepUnsupervisedLearning2015b,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-06-01},
  pages = {2256--2265},
  publisher = {{PMLR}},
  issn = {1938-7228},
  url = {https://proceedings.mlr.press/v37/sohl-dickstein15.html},
  urldate = {2024-02-22},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/CPI9QRW6/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf}
}

@online{songDenoisingDiffusionImplicit2022d,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  date = {2022-10-05},
  eprint = {2010.02502},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2010.02502},
  url = {http://arxiv.org/abs/2010.02502},
  urldate = {2024-02-25},
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/X69BMS36/Song et al. - 2022 - Denoising Diffusion Implicit Models.pdf;/home/krawczuk/Zotero/storage/LNZJ9BMA/2010.html}
}

@inproceedings{songGenerativeModelingEstimating2019b,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
  urldate = {2024-02-25},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples  comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/C4HJKZEV/Song and Ermon - 2019 - Generative Modeling by Estimating Gradients of the.pdf}
}

@online{songMaximumLikelihoodTraining2021a,
  title = {Maximum {{Likelihood Training}} of {{Score-Based Diffusion Models}}},
  author = {Song, Yang and Durkan, Conor and Murray, Iain and Ermon, Stefano},
  date = {2021-10-20},
  eprint = {2101.09258},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2101.09258},
  url = {http://arxiv.org/abs/2101.09258},
  urldate = {2024-02-22},
  abstract = {Score-based diffusion models synthesize samples by reversing a stochastic process that diffuses data to noise, and are trained by minimizing a weighted combination of score matching losses. The log-likelihood of score-based diffusion models can be tractably computed through a connection to continuous normalizing flows, but log-likelihood is not directly optimized by the weighted combination of score matching losses. We show that for a specific weighting scheme, the objective upper bounds the negative log-likelihood, thus enabling approximate maximum likelihood training of score-based diffusion models. We empirically observe that maximum likelihood training consistently improves the likelihood of score-based diffusion models across multiple datasets, stochastic processes, and model architectures. Our best models achieve negative log-likelihoods of 2.83 and 3.76 bits/dim on CIFAR-10 and ImageNet 32x32 without any data augmentation, on a par with state-of-the-art autoregressive models on these tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/234MB2S3/Song et al. - 2021 - Maximum Likelihood Training of Score-Based Diffusi.pdf;/home/krawczuk/Zotero/storage/LJNBHJXL/2101.html}
}

@article{sorkhabiAutomatedTopologySynthesis2017a,
  title = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits: {{A}} Survey},
  shorttitle = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits},
  author = {Sorkhabi, Samin Ebrahim and Zhang, Lihong},
  date = {2017-01-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {56},
  pages = {128--138},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2016.10.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926016300931},
  urldate = {2024-02-19},
  abstract = {The puzzle of automatically synthesizing analog and radio frequency (RF) circuit topology has not yet been offered with an industrially-acceptable solution although endeavors still continue to seek a conquest in this area. This survey provides a comprehensive study of the techniques utilized for this purpose. The existing methods are analyzed from four different viewpoints, namely, structural view, conceptual view, implementation view, and application view. Different schemes are perused with their advantages and drawbacks discussed in the context of balanced performance between configuration-space coverage and search efficiency. Some prospective trends are pointed out to shed light on the upcoming research activities.},
  keywords = {Analog circuit topology synthesis,Analog electronic design automation,Classification},
  file = {/home/krawczuk/Zotero/storage/GWPEG9NF/S0167926016300931.html}
}

@article{sorkhabiAutomatedTopologySynthesis2017b,
  title = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits: {{A}} Survey},
  shorttitle = {Automated Topology Synthesis of Analog and {{RF}} Integrated Circuits},
  author = {Sorkhabi, Samin Ebrahim and Zhang, Lihong},
  date = {2017-01-01},
  journaltitle = {Integration},
  shortjournal = {Integration},
  volume = {56},
  pages = {128--138},
  issn = {0167-9260},
  doi = {10.1016/j.vlsi.2016.10.017},
  url = {https://www.sciencedirect.com/science/article/pii/S0167926016300931},
  urldate = {2024-02-20},
  abstract = {The puzzle of automatically synthesizing analog and radio frequency (RF) circuit topology has not yet been offered with an industrially-acceptable solution although endeavors still continue to seek a conquest in this area. This survey provides a comprehensive study of the techniques utilized for this purpose. The existing methods are analyzed from four different viewpoints, namely, structural view, conceptual view, implementation view, and application view. Different schemes are perused with their advantages and drawbacks discussed in the context of balanced performance between configuration-space coverage and search efficiency. Some prospective trends are pointed out to shed light on the upcoming research activities.},
  keywords = {Analog circuit topology synthesis,Analog electronic design automation,Classification}
}

@inreference{SpatialNetwork2023,
  title = {Spatial Network},
  booktitle = {Wikipedia},
  date = {2023-10-12T03:52:20Z},
  url = {https://en.wikipedia.org/w/index.php?title=Spatial_network&oldid=1179742180},
  urldate = {2024-02-22},
  abstract = {A spatial network (sometimes also geometric graph) is a graph in which the vertices or edges are spatial elements associated with geometric objects, i.e., the nodes are located in a space equipped with a certain metric. The simplest mathematical realization of spatial network is a lattice or a random geometric graph (see figure in the right), where nodes are distributed uniformly at random over a two-dimensional plane; a pair of nodes are connected if the Euclidean distance is smaller than a given neighborhood radius. Transportation and mobility networks, Internet, mobile phone networks, power grids, social and contact networks and biological neural networks are all examples where the underlying space is relevant and where the graph's topology alone does not contain all the information. Characterizing and understanding the structure, resilience and the evolution of spatial networks is crucial for many different fields ranging from urbanism to epidemiology.},
  langid = {english},
  annotation = {Page Version ID: 1179742180},
  file = {/home/krawczuk/Zotero/storage/5XRMLJ55/Spatial_network.html}
}

@online{SpecificationTopologyAutomatica,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=ZzmTrAML-mAAAAAA:gl7tX0UqdKrlNwLRzn1Uj9717pljBhgCrxEE9JcvSntZSTQRWnsLqWbx1UQC6PP_V4OfCQoJvMTq},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/CLZT4656/9643552.html}
}

@online{SpecificationTopologyAutomaticb,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=OfMFhpoMuAQAAAAA:AfACosBN6oY2Vpsbk8_cjU86UjZRF_Ea-T_xy-llZuNF7C5ZRvRRToOmxHA4g0vG5tvB5wt09S5x},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/9NG9IXC3/9643552.html}
}

@online{SpecificationTopologyAutomaticc,
  title = {From {{Specification}} to {{Topology}}: {{Automatic Power Converter Design}} via {{Reinforcement Learning}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9643552?casa_token=OfMFhpoMuAQAAAAA:AfACosBN6oY2Vpsbk8_cjU86UjZRF_Ea-T_xy-llZuNF7C5ZRvRRToOmxHA4g0vG5tvB5wt09S5x},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/89F7WMIR/9643552.html}
}

@online{SpectralCharacterizationHierarchical,
  title = {Spectral {{Characterization}} of {{Hierarchical Network Modularity}} and {{Limits}} of {{Modularity Detection}} - {{PMC}}},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3557301/},
  urldate = {2024-02-22},
  file = {/home/krawczuk/Zotero/storage/BGNZ54PD/PMC3557301.html}
}

@article{sudermann-merxCrossingMinimalEdgeConstrained2021,
  title = {Crossing {{Minimal Edge-Constrained Layout Planning}} Using {{Benders Decomposition}}},
  author = {Sudermann-Merx, Nathan and Rebennack, Steffen and Timpe, Christian},
  date = {2021},
  journaltitle = {Production and Operations Management},
  volume = {30},
  number = {10},
  pages = {3429--3447},
  issn = {1937-5956},
  doi = {10.1111/poms.13441},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/poms.13441},
  urldate = {2024-02-22},
  abstract = {We present a new crossing number problem, which we refer to as the edge-constrained weighted two-layer crossing number problem (ECW2CN). The ECW2CN arises in layout planning of hose coupling stations at BASF, where the challenge is to find a crossing minimal assignment of tube-connected units to given positions on two opposing layers. This allows the use of robots in an effort to reduce the probability of operational disruptions and to increase human safety. Physical limitations imply maximal length and maximal curvature conditions on the tubes as well as spatial constraints imposed by the surrounding walls. This is the major difference of ECW2CN to all known variants of the crossing number problem. Such as many variants of the crossing number problem, ECW2CN is -hard. Because the optimization model grows fast with respect to the input data, we face out-of-memory errors for the monolithic model. Therefore, we develop two solution methods. In the first method, we tailor Benders decomposition toward the problem. The Benders subproblems are solved analytically and the Benders master problem is strengthened by additional cuts. Furthermore, we combine this Benders decomposition with ideas borrowed from fix-and-relax heuristics to design the Dynamic Fix-and-Relax Pump (DFRP). Based on an initial solution, DFRP improves successively feasible points by solving dynamically sampled smaller problems with Benders decomposition. Because the optimization model is a surrogate model for its time-dependent formulation, we evaluate the obtained solutions for different choices of the objective function via a simulation model. All algorithms are implemented efficiently using advanced features of the GuRoBi-Python API, such as callback functions and lazy constraints. We present a case study for BASF using real data and make the real-world data openly available.},
  langid = {english},
  keywords = {Benders decomposition with direct cut calculation,Benders-based branch-and-cut,crossing number,dynamic fix-and-relax pump,mixed integer linear programming,optimization,simulation},
  file = {/home/krawczuk/Zotero/storage/WXI4ZC8A/Sudermann-Merx et al. - 2021 - Crossing Minimal Edge-Constrained Layout Planning .pdf;/home/krawczuk/Zotero/storage/WDKX5D7Y/poms.html}
}

@inproceedings{suhFACTGenFrameworkAutomated2022a,
  title = {{{FACTGen}}: {{Framework}} for {{Automated Circuit Topology Generator}}},
  shorttitle = {{{FACTGen}}},
  booktitle = {2022 19th {{International SoC Design Conference}} ({{ISOCC}})},
  author = {Suh, Jangwon and Jung, Wanyeong},
  date = {2022-10},
  pages = {27--28},
  issn = {2163-9612},
  doi = {10.1109/ISOCC56007.2022.10031578},
  url = {https://ieeexplore.ieee.org/abstract/document/10031578},
  urldate = {2024-02-24},
  abstract = {Besides the popularity of computer-aided design methods, the circuit topology design itself still relies on human insights. We propose a framework for automated circuit topology generator (FACTGen). Our framework transforms the circuit topology generation problem into a continuous black-box optimization problem. FACTGen has succeeded to generate several 2T and 4T CMOS digital gate topologies.},
  eventtitle = {2022 19th {{International SoC Design Conference}} ({{ISOCC}})},
  keywords = {black-box optimization,circuit topology,Circuit topology,Closed box,Design automation,Generators,Logic gates,Search problems,topology generation,Transforms},
  file = {/home/krawczuk/Zotero/storage/YZL5WG3J/Suh and Jung - 2022 - FACTGen Framework for Automated Circuit Topology .pdf;/home/krawczuk/Zotero/storage/2ATXPR3G/10031578.html}
}

@inproceedings{sullivanParallelAlgorithmsGraph2013,
  title = {Parallel {{Algorithms}} for {{Graph Optimization Using Tree Decompositions}}},
  booktitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  author = {Sullivan, Blair D. and Weerapurage, Dinesh and Groër, Chris},
  date = {2013-05},
  pages = {1838--1847},
  doi = {10.1109/IPDPSW.2013.242},
  url = {https://ieeexplore.ieee.org/document/6651084},
  urldate = {2024-02-20},
  abstract = {Although many NP-hard graph optimization problems can be solved in polynomial time on graphs of bounded tree-width, the adoption of these techniques into mainstream scientific computation has been limited due to the high memory requirements of the dynamic programming tables and excessive runtimes of sequential implementations. This work addresses both challenges by proposing a set of new parallel algorithms for all steps of a tree decomposition-based approach to solve the maximum weighted independent set problem. A hybrid OpenMP/MPI implementation includes a highly scalable parallel dynamic programming algorithm leveraging the MADNESS task based runtime, and computational results demonstrate scaling. This work enables a significant expansion of the scale of graphs on which exact solutions to maximum weighted independent set can be obtained, and forms a framework for solving additional graph optimization problems with similar techniques.},
  eventtitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  keywords = {dynamic programming,Dynamic programming,graph algorithms,Heuristic algorithms,independent set,Memory management,Optimization,Parallel algorithms,parallel programming,Runtime,tree decomposition,Vegetation},
  file = {/home/krawczuk/Zotero/storage/2C5599ET/Sullivan et al. - 2013 - Parallel Algorithms for Graph Optimization Using T.pdf}
}

@inproceedings{sullivanParallelAlgorithmsGraph2013a,
  title = {Parallel {{Algorithms}} for {{Graph Optimization Using Tree Decompositions}}},
  booktitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  author = {Sullivan, Blair D. and Weerapurage, Dinesh and Groër, Chris},
  date = {2013-05},
  pages = {1838--1847},
  doi = {10.1109/IPDPSW.2013.242},
  url = {https://ieeexplore.ieee.org/document/6651084},
  urldate = {2024-02-20},
  abstract = {Although many NP-hard graph optimization problems can be solved in polynomial time on graphs of bounded tree-width, the adoption of these techniques into mainstream scientific computation has been limited due to the high memory requirements of the dynamic programming tables and excessive runtimes of sequential implementations. This work addresses both challenges by proposing a set of new parallel algorithms for all steps of a tree decomposition-based approach to solve the maximum weighted independent set problem. A hybrid OpenMP/MPI implementation includes a highly scalable parallel dynamic programming algorithm leveraging the MADNESS task based runtime, and computational results demonstrate scaling. This work enables a significant expansion of the scale of graphs on which exact solutions to maximum weighted independent set can be obtained, and forms a framework for solving additional graph optimization problems with similar techniques.},
  eventtitle = {2013 {{IEEE International Symposium}} on {{Parallel}} \& {{Distributed Processing}}, {{Workshops}} and {{Phd Forum}}},
  keywords = {dynamic programming,Dynamic programming,graph algorithms,Heuristic algorithms,independent set,Memory management,Optimization,Parallel algorithms,parallel programming,Runtime,tree decomposition,Vegetation},
  file = {/home/krawczuk/Zotero/storage/24CZIRZW/Sullivan et al. - 2013 - Parallel Algorithms for Graph Optimization Using T.pdf}
}

@article{sunDIFUSCOGraphbasedDiffusion2024,
  title = {{{DIFUSCO}}: {{Graph-based Diffusion Solvers}} for {{Combinatorial Optimization}}},
  shorttitle = {{{DIFUSCO}}},
  author = {Sun, Zhiqing and Yang, Yiming},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/0ba520d93c3df592c83a611961314c98-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/BKW6HNRM/Sun and Yang - 2024 - DIFUSCO Graph-based Diffusion Solvers for Combina.pdf}
}

@online{SynthesisSystemAnalog,
  title = {A Synthesis System for Analog Circuits Based on Evolutionary Search and Topological Reuse | {{IEEE Journals}} \& {{Magazine}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/1413261?casa_token=MvOhJbKQnnoAAAAA:eAu-H_diRTVWFMj8uTmLx55rJKDIA2AX9FdBgDPxJTGvE-D1FBdVfr1gL5Q5gUphvZ0V1io8GBqk},
  urldate = {2024-02-19},
  file = {/home/krawczuk/Zotero/storage/R8WLTJWA/1413261.html}
}

@incollection{thomasChurchProblemTour2008,
  title = {Church’s {{Problem}} and a {{Tour}} through {{Automata Theory}}},
  booktitle = {Pillars of {{Computer Science}}: {{Essays Dedicated}} to {{Boris}} ({{Boaz}}) {{Trakhtenbrot}} on the {{Occasion}} of {{His}} 85th {{Birthday}}},
  author = {Thomas, Wolfgang},
  editor = {Avron, Arnon and Dershowitz, Nachum and Rabinovich, Alexander},
  date = {2008},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {635--655},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-78127-1_35},
  url = {https://doi.org/10.1007/978-3-540-78127-1_35},
  urldate = {2024-02-20},
  abstract = {Church’s Problem, stated fifty years ago, asks for a finite-state machine that realizes the transformation of an infinite sequence α into an infinite sequence β such that a requirement on (α, β), expressed in monadic second-order logic, is satisfied. We explain how three fundamental techniques of automata theory play together in a solution of Church’s Problem: Determinization (starting from the subset construction), appearance records (for stratifying acceptance conditions), and reachability analysis (for the solution of games).},
  isbn = {978-3-540-78127-1},
  langid = {english},
  keywords = {Automaton Theory,Game Graph,Tree Automaton,Weak Parity,Winning Strategy}
}

@inreference{TopologicalGraph2024,
  title = {Topological Graph},
  booktitle = {Wikipedia},
  date = {2024-01-11T07:43:47Z},
  url = {https://en.wikipedia.org/w/index.php?title=Topological_graph&oldid=1194890238},
  urldate = {2024-02-22},
  abstract = {In mathematics, a topological graph is a representation of a graph in the plane, where the vertices of the graph are represented by distinct points and the edges by Jordan arcs (connected pieces of Jordan curves) joining the corresponding pairs of points. The points representing the vertices of a graph and the arcs representing its edges are called the vertices and the edges of the topological graph.  It is usually assumed that any two edges of a topological graph cross a finite number of times, no edge passes through a vertex different from its endpoints, and no two edges touch each other (without crossing). A topological graph is also called a drawing of a graph. An important special class of topological graphs is the class of geometric graphs, where the edges are represented by line segments. (The term geometric graph is sometimes used in a broader, somewhat vague sense.) The theory of topological graphs is an area of graph theory, mainly concerned with combinatorial properties of topological graphs, in particular, with the crossing patterns of their edges. It is closely related to graph drawing, a field which is more application oriented, and topological graph theory, which focuses on embeddings of graphs in surfaces (that is, drawings without crossings).},
  langid = {english},
  annotation = {Page Version ID: 1194890238},
  file = {/home/krawczuk/Zotero/storage/FE8D2MFE/Topological_graph.html}
}

@online{TopologyOptimizationOperational,
  title = {Topology {{Optimization}} of {{Operational Amplifier}} in {{Continuous Space}} via {{Graph Embedding}} | {{IEEE Conference Publication}} | {{IEEE Xplore}}},
  url = {https://ieeexplore.ieee.org/abstract/document/9774676?casa_token=SM8jM4GK_qMAAAAA:gPM11JjhdtGnORbikJKlZiaUsja3S33V-N-070gPHDeJyJDRR-U4LbhlSOy-yqm5DCBnFU6H3FL2},
  urldate = {2024-02-20},
  file = {/home/krawczuk/Zotero/storage/8R2XVXUP/9774676.html}
}

@online{trivediLeveragingGraphDiffusion2023a,
  title = {Leveraging {{Graph Diffusion Models}} for {{Network Refinement Tasks}}},
  author = {Trivedi, Puja and Rossi, Ryan and Arbour, David and Yu, Tong and Dernoncourt, Franck and Kim, Sungchul and Lipka, Nedim and Park, Namyong and Ahmed, Nesreen K. and Koutra, Danai},
  date = {2023-11-29},
  eprint = {2311.17856},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.17856},
  url = {http://arxiv.org/abs/2311.17856},
  urldate = {2024-02-20},
  abstract = {Most real-world networks are noisy and incomplete samples from an unknown target distribution. Refining them by correcting corruptions or inferring unobserved regions typically improves downstream performance. Inspired by the impressive generative capabilities that have been used to correct corruptions in images, and the similarities between "in-painting" and filling in missing nodes and edges conditioned on the observed graph, we propose a novel graph generative framework, SGDM, which is based on subgraph diffusion. Our framework not only improves the scalability and fidelity of graph diffusion models, but also leverages the reverse process to perform novel, conditional generation tasks. In particular, through extensive empirical analysis and a set of novel metrics, we demonstrate that our proposed model effectively supports the following refinement tasks for partially observable networks: T1: denoising extraneous subgraphs, T2: expanding existing subgraphs and T3: performing "style" transfer by regenerating a particular subgraph to match the characteristics of a different node or subgraph.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/home/krawczuk/Zotero/storage/YUHFI8KJ/Trivedi et al. - 2023 - Leveraging Graph Diffusion Models for Network Refi.pdf;/home/krawczuk/Zotero/storage/LR3Y9M7M/2311.html}
}

@inproceedings{uhlmannDeepReinforcementLearning2022a,
  title = {Deep {{Reinforcement Learning}} for {{Analog Circuit Sizing}} with an {{Electrical Design Space}} and {{Sparse Rewards}}},
  booktitle = {Proceedings of the 2022 {{ACM}}/{{IEEE Workshop}} on {{Machine Learning}} for {{CAD}}},
  author = {Uhlmann, Yannick and Essich, Michael and Bramlage, Lennart and Scheible, Jürgen and Curio, Cristóbal},
  date = {2022-09-12},
  series = {{{MLCAD}} '22},
  pages = {21--26},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3551901.3556474},
  url = {https://doi.org/10.1145/3551901.3556474},
  urldate = {2024-02-19},
  abstract = {There is still a great reliance on human expert knowledge during the analog integrated circuit sizing design phase due to its complexity and scale, with the result that there is a very low level of automation associated with it. Current research shows that reinforcement learning is a promising approach for addressing this issue. Similarly, it has been shown that the convergence of conventional optimization approaches can be improved by transforming the design space from the geometrical domain into the electrical domain. Here, this design space transformation is employed as an alternative action space for deep reinforcement learning agents. The presented approach is based entirely on reinforcement learning, whereby agents are trained in the craft of analog circuit sizing without explicit expert guidance. After training and evaluating agents on circuits of varying complexity, their behavior when confronted with a different technology, is examined, showing the applicability, feasibility as well as transferability of this approach.},
  isbn = {978-1-4503-9486-4},
  keywords = {analog circuit sizing,neural networks,reinforcement learning}
}

@online{US11409934B2GenerationHardware,
  title = {{{US11409934B2}} - {{Generation}} of Hardware Design Using a Constraint Solver Module for Topology Synthesis - {{Google Patents}}},
  url = {https://patents.google.com/patent/US11409934B2/en},
  urldate = {2024-02-20}
}

@inproceedings{vaswaniAttentionAllYou2017c,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
  date = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  urldate = {2024-02-25},
  abstract = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/KINQC94K/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@inproceedings{vignacBuildingPowerfulEquivariant2020,
  title = {Building Powerful and Equivariant Graph Neural Networks with Structural Message-Passing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vignac, Clément and Loukas, Andreas and Frossard, Pascal},
  date = {2020},
  volume = {33},
  pages = {14143--14155},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper_files/paper/2020/hash/a32d7eeaae19821fd9ce317f3ce952a7-Abstract.html},
  urldate = {2024-02-24},
  abstract = {Message-passing has proved to be an effective way to design graph neural networks, as it is able to leverage both permutation equivariance and an inductive bias towards learning local structures in order to achieve good generalization. However, current message-passing architectures have a limited representation power and fail to learn basic topological properties of graphs. We address this problem and propose a powerful and equivariant message-passing framework based on two ideas: first, we propagate a one-hot encoding of the nodes, in addition to the features, in order to learn a local context matrix around each node. This matrix contains rich local information about both features and topology and can eventually be pooled to build node representations. Second, we propose methods for the parametrization of the message and update functions that ensure permutation equivariance. Having a representation that is independent of the specific choice of the one-hot encoding permits inductive reasoning and leads to better generalization properties. Experimentally, our model can predict various graph topological properties on synthetic data more accurately than previous methods and achieves state-of-the-art results on molecular graph regression on the ZINC dataset.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/34L3ZT5K/Vignac et al. - 2020 - Building powerful and equivariant graph neural net.pdf}
}

@online{vignacDiGressDiscreteDenoising2023b,
  title = {{{DiGress}}: {{Discrete Denoising}} Diffusion for Graph Generation},
  shorttitle = {{{DiGress}}},
  author = {Vignac, Clement and Krawczuk, Igor and Siraudin, Antoine and Wang, Bohan and Cevher, Volkan and Frossard, Pascal},
  date = {2023-05-23},
  eprint = {2209.14734},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2209.14734},
  url = {http://arxiv.org/abs/2209.14734},
  urldate = {2024-02-25},
  abstract = {This work introduces DiGress, a discrete denoising diffusion model for generating graphs with categorical node and edge attributes. Our model utilizes a discrete diffusion process that progressively edits graphs with noise, through the process of adding or removing edges and changing the categories. A graph transformer network is trained to revert this process, simplifying the problem of distribution learning over graphs into a sequence of node and edge classification tasks. We further improve sample quality by introducing a Markovian noise model that preserves the marginal distribution of node and edge types during diffusion, and by incorporating auxiliary graph-theoretic features. A procedure for conditioning the generation on graph-level features is also proposed. DiGress achieves state-of-the-art performance on molecular and non-molecular datasets, with up to 3x validity improvement on a planar graph dataset. It is also the first model to scale to the large GuacaMol dataset containing 1.3M drug-like molecules without the use of molecule-specific representations.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/KLZEE85X/Vignac et al. - 2023 - DiGress Discrete Denoising diffusion for graph ge.pdf;/home/krawczuk/Zotero/storage/ATJG5E24/2209.html}
}

@online{vignacMiDiMixedGraph2023c,
  title = {{{MiDi}}: {{Mixed Graph}} and {{3D Denoising Diffusion}} for {{Molecule Generation}}},
  shorttitle = {{{MiDi}}},
  author = {Vignac, Clement and Osman, Nagham and Toni, Laura and Frossard, Pascal},
  date = {2023-06-05},
  eprint = {2302.09048},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.09048},
  url = {http://arxiv.org/abs/2302.09048},
  urldate = {2024-02-20},
  abstract = {This work introduces MiDi, a novel diffusion model for jointly generating molecular graphs and their corresponding 3D arrangement of atoms. Unlike existing methods that rely on predefined rules to determine molecular bonds based on the 3D conformation, MiDi offers an end-to-end differentiable approach that streamlines the molecule generation process. Our experimental results demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS dataset, MiDi generates 92\% of stable molecules, against 6\% for the previous EDM model that uses interatomic distances for bond prediction, and 40\% using EDM followed by an algorithm that directly optimize bond orders for validity. Our code is available at github.com/cvignac/MiDi.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/4AGAL9AL/Vignac et al. - 2023 - MiDi Mixed Graph and 3D Denoising Diffusion for M.pdf;/home/krawczuk/Zotero/storage/YCL72G5R/2302.html}
}

@inproceedings{vignacTopNEquivariantSet2021d,
  title = {Top-{{N}}: {{Equivariant Set}} and {{Graph Generation}} without {{Exchangeability}}},
  shorttitle = {Top-{{N}}},
  author = {Vignac, Clement and Frossard, Pascal},
  date = {2021-10-06},
  url = {https://openreview.net/forum?id=-Gk_IPJWvk},
  urldate = {2024-02-20},
  abstract = {This work addresses one-shot set and graph generation, and, more specifically, the parametrization of probabilistic decoders that map a vector-shaped prior to a distribution over sets or graphs. Sets and graphs are most commonly generated by first sampling points i.i.d. from a normal distribution, and then processing these points along with the prior vector using Transformer layers or Graph Neural Networks. This architecture is designed to generate exchangeable distributions, i.e., all permutations of the generated outputs are equally likely. We however show that it only optimizes a proxy to the evidence lower bound, which makes it hard to train. We then study equivariance in generative settings and show that non-exchangeable methods can still achieve permutation equivariance. Using this result, we introduce Top-n creation, a differentiable generation mechanism that uses the latent vector to select the most relevant points from a trainable reference set. Top-n can replace i.i.d. generation in any Variational Autoencoder or Generative Adversarial Network. Experimentally, our method outperforms i.i.d. generation by 15\% at SetMNIST reconstruction, by 33\% at object detection on CLEVR, generates sets that are 74\% closer to the true distribution on a synthetic molecule-like dataset, and generates more valid molecules on QM9.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/4N9TGKTG/Vignac and Frossard - 2021 - Top-N Equivariant Set and Graph Generation withou.pdf}
}

@article{villarFullyCovariantMachine2023,
  title = {Towards Fully Covariant Machine Learning},
  author = {Villar, Soledad and Hogg, David W. and Yao, Weichi and Kevrekidis, George A. and Schölkopf, Bernhard},
  date = {2023-08-27},
  journaltitle = {Transactions on Machine Learning Research},
  issn = {2835-8856},
  url = {https://openreview.net/forum?id=gllUnpYuXg},
  urldate = {2024-02-20},
  abstract = {Any representation of data involves arbitrary investigator choices. Because those choices are external to the data-generating process, each choice leads to an exact symmetry, corresponding to the group of transformations that takes one possible representation to another. These are the passive symmetries; they include coordinate freedom, gauge symmetry, and units covariance, all of which have led to important results in physics. In machine learning, the most visible passive symmetry is the relabeling or permutation symmetry of graphs. The active symmetries are those that must be established by observation and experiment. They include, for instance, translations invariances or rotation invariances of physical law. These symmetries are the subject of most of the equivariant machine learning literature. Our goal, in this conceptual contribution, is to understand the implications for machine learning of the many passive and active symmetries in play. We discuss dos and don'ts for machine learning practice if passive symmetries are to be respected. We discuss links to causal modeling and argue that the implementation of passive symmetries is particularly valuable when the goal of the learning problem is to generalize out of sample. We conjecture that the implementation of passive symmetries might help machine learning in the same ways that it transformed physics in the twentieth century.},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/LKPHC5SI/Villar et al. - 2023 - Towards fully covariant machine learning.pdf}
}

@article{wagnerRoadModularity2007,
  title = {The Road to Modularity},
  author = {Wagner, Günter P. and Pavlicev, Mihaela and Cheverud, James M.},
  date = {2007-12},
  journaltitle = {Nature Reviews Genetics},
  shortjournal = {Nat Rev Genet},
  volume = {8},
  number = {12},
  pages = {921--931},
  publisher = {{Nature Publishing Group}},
  issn = {1471-0064},
  doi = {10.1038/nrg2267},
  url = {https://www.nature.com/articles/nrg2267},
  urldate = {2024-02-22},
  abstract = {A network of interactions is called modular if it is subdivided into relatively autonomous, internally highly connected components.Modularity has emerged as a rallying point for research in developmental and evolutionary biology, as well as in molecular systems biology.Modularity has been found on all levels of biological organization: the structure of macromolecules, protein–protein interaction networks, gene regulation and the variation of quantitative traits.Various mechanisms can lead to modularity, including gene duplication, neutral mutations and selection for robustness or selection in response to varying environmental conditions.The main challenge for future research is to provide evidence to refute some of these models, and to determine whether modularity has an impact on the rate and pattern of evolution.},
  issue = {12},
  langid = {english},
  keywords = {Agriculture,Animal Genetics and Genomics,Biomedicine,Cancer Research,Gene Function,general,Human Genetics},
  file = {/home/krawczuk/Zotero/storage/PSR29HDF/Wagner et al. - 2007 - The road to modularity.pdf}
}

@inproceedings{wangApproachTopologySynthesis2006,
  title = {An Approach to Topology Synthesis of Analog Circuits Using Hierarchical Blocks and Symbolic Analysis},
  booktitle = {Proceedings of the 2006 {{Asia}} and {{South Pacific Design Automation Conference}}},
  author = {Wang, Xiaoying and Hedrich, Lars},
  date = {2006-01-24},
  series = {{{ASP-DAC}} '06},
  pages = {700--705},
  publisher = {{IEEE Press}},
  location = {{Yokohama, Japan}},
  doi = {10.1145/1118299.1118463},
  url = {https://dl.acm.org/doi/10.1145/1118299.1118463},
  urldate = {2024-02-19},
  abstract = {This paper presents a method of design automation for analog circuits, focusing on topology generation and quick performance evaluation. First we describe mechanisms to generate circuit topologies with hierarchical blocks. Those blocks are specialized by adding terminal information. The connection between blocks is in compliance with a set of synthesis rules, which are extracted from typical schematics in the literature. Symbolic analysis has been used to select an appropriate topology quickly and to help the designer gain a better understanding of a circuit's behavior. Finally, experimental results show the creativity and efficiency of our method.},
  isbn = {978-0-7803-9451-3},
  file = {/home/krawczuk/Zotero/storage/CTJZP5WM/Wang and Hedrich - 2006 - An approach to topology synthesis of analog circui.pdf}
}

@article{wangEfficientArithmeticBlock2023,
  title = {Efficient {{Arithmetic Block Identification With Graph Learning}} and {{Network-Flow}}},
  author = {Wang, Ziyi and He, Zhuolun and Bai, Chen and Yang, Haoyu and Yu, Bei},
  date = {2023-08},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {42},
  number = {8},
  pages = {2591--2603},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2022.3227815},
  url = {https://ieeexplore.ieee.org/abstract/document/9975800?casa_token=dcI5WrXDmQkAAAAA:2HalhgJglK4bc3PJlTs7YnN_Ob_ZIiYPkdbTBPN3YUpPXs1Flyu9mRZpt3DZtkUvBE2dHJCe2TRV},
  urldate = {2024-02-20},
  abstract = {Arithmetic block identification in gate-level netlists plays an essential role for various purposes, including malicious logic detection, functional verification, or macro-block optimization. However, current methods usually suffer from either low performance or poor scalability. To address the issue, we come up with a novel framework based on graph learning and network flow analysis, that extracts desired logic components from a complete circuit netlist. We design a novel asynchronous bidirectional graph neural network (ABGNN) dedicated to representation learning on directed acyclic graphs. In addition, we develop a convex cost network-flow-based datapath extraction approach to match the predicted block inputs with predicted block outputs. Experimental results on open-source RISC-V CPU designs demonstrate that our proposed solution significantly outperforms several state-of-the-art arithmetic block identification flows.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Arithmetic,Graph neural networks,graph neural networks (GNNs),logic gates,Logic gates,predictive models,Predictive models,representation learning,Representation learning,task analysis,Task analysis,wires,Wires},
  file = {/home/krawczuk/Zotero/storage/F7STBSFE/Wang et al. - 2023 - Efficient Arithmetic Block Identification With Gra.pdf}
}

@inproceedings{wangFunctionalityMattersNetlist2022c,
  title = {Functionality Matters in Netlist Representation Learning},
  booktitle = {Proceedings of the 59th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Wang, Ziyi and Bai, Chen and He, Zhuolun and Zhang, Guangliang and Xu, Qiang and Ho, Tsung-Yi and Yu, Bei and Huang, Yu},
  date = {2022-08-23},
  series = {{{DAC}} '22},
  pages = {61--66},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3489517.3530410},
  url = {https://dl.acm.org/doi/10.1145/3489517.3530410},
  urldate = {2024-02-20},
  abstract = {Learning feasible representation from raw gate-level netlists is essential for incorporating machine learning techniques in logic synthesis, physical design, or verification. Existing message-passing-based graph learning methodologies focus merely on graph topology while overlooking gate functionality, which often fails to capture underlying semantic, thus limiting their generalizability. To address the concern, we propose a novel netlist representation learning framework that utilizes a contrastive scheme to acquire generic functional knowledge from netlists effectively. We also propose a customized graph neural network (GNN) architecture that learns a set of independent aggregators to better cooperate with the above framework. Comprehensive experiments on multiple complex real-world designs demonstrate that our proposed solution significantly outperforms state-of-the-art netlist feature learning flows.},
  isbn = {978-1-4503-9142-9},
  file = {/home/krawczuk/Zotero/storage/5H5IGJT2/Wang et al. - 2022 - Functionality matters in netlist representation le.pdf}
}

@inproceedings{wangFunctionalityMattersNetlist2022d,
  title = {Functionality Matters in Netlist Representation Learning},
  booktitle = {Proceedings of the 59th {{ACM}}/{{IEEE Design Automation Conference}}},
  author = {Wang, Ziyi and Bai, Chen and He, Zhuolun and Zhang, Guangliang and Xu, Qiang and Ho, Tsung-Yi and Yu, Bei and Huang, Yu},
  date = {2022-07-10},
  pages = {61--66},
  publisher = {{ACM}},
  location = {{San Francisco California}},
  doi = {10.1145/3489517.3530410},
  url = {https://dl.acm.org/doi/10.1145/3489517.3530410},
  urldate = {2024-02-20},
  eventtitle = {{{DAC}} '22: 59th {{ACM}}/{{IEEE Design Automation Conference}}},
  isbn = {978-1-4503-9142-9},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/MWQ8CFGA/Wang et al. - 2022 - Functionality matters in netlist representation le.pdf}
}

@article{weiNetlistManufacturableLayout2023,
  title = {From {{Netlist}} to {{Manufacturable Layout}}: {{An Auto-Layout Algorithm Optimized}} for {{Radio Frequency Integrated Circuits}}},
  shorttitle = {From {{Netlist}} to {{Manufacturable Layout}}},
  author = {Wei, Yiding and Liu, Jun and Sun, Dengbao and Su, Guodong and Wang, Junchao},
  date = {2023-06},
  journaltitle = {Symmetry},
  volume = {15},
  number = {6},
  pages = {1272},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2073-8994},
  doi = {10.3390/sym15061272},
  url = {https://www.mdpi.com/2073-8994/15/6/1272},
  urldate = {2024-02-19},
  abstract = {Layout stitching is a repetitive and tedious task of the radio frequency integrated circuit (RFIC) design process. While academic research on layout splicing algorithms mainly focuses on analog and digital circuits, there is still a lack of well-developed algorithms for RFICs. An RFIC system usually has a symmetrical layout, such as transmitter and receiver components, low-noise amplifier (LNA), an SPDT switch, etc. This paper aims to address this gap by proposing an automated procedure for the layout of RFICs by relying on the basic device/PCell structure based on the interconnection among circuit topologies. This approach makes the in-series generation of layouts and automatic splicing based on circuit logic possible, resulting in superior stitching performance compared with related modules in Advanced Design System. To demonstrate the physical application possibilities, we implemented our algorithm on an LNA and a switch circuit.},
  issue = {6},
  langid = {english},
  keywords = {layout splicing,PCell structure,RF circuit,topology},
  file = {/home/krawczuk/Zotero/storage/K7ZPJTH8/Wei et al. - 2023 - From Netlist to Manufacturable Layout An Auto-Lay.pdf}
}

@online{winklerLearningLikelihoodsConditional2023,
  title = {Learning {{Likelihoods}} with {{Conditional Normalizing Flows}}},
  author = {Winkler, Christina and Worrall, Daniel and Hoogeboom, Emiel and Welling, Max},
  date = {2023-11-12},
  eprint = {1912.00042},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1912.00042},
  url = {http://arxiv.org/abs/1912.00042},
  urldate = {2024-02-24},
  abstract = {Normalizing Flows (NFs) are able to model complicated distributions p(y) with strong inter-dimensional correlations and high multimodality by transforming a simple base density p(z) through an invertible neural network under the change of variables formula. Such behavior is desirable in multivariate structured prediction tasks, where handcrafted per-pixel loss-based methods inadequately capture strong correlations between output dimensions. We present a study of conditional normalizing flows (CNFs), a class of NFs where the base density to output space mapping is conditioned on an input x, to model conditional densities p(y|x). CNFs are efficient in sampling and inference, they can be trained with a likelihood-based objective, and CNFs, being generative flows, do not suffer from mode collapse or training instabilities. We provide an effective method to train continuous CNFs for binary problems and in particular, we apply these CNFs to super-resolution and vessel segmentation tasks demonstrating competitive performance on standard benchmark datasets in terms of likelihood and conventional metrics.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GI3XMVJN/Winkler et al. - 2023 - Learning Likelihoods with Conditional Normalizing .pdf;/home/krawczuk/Zotero/storage/Q8CD6FWI/1912.html}
}

@inproceedings{wolkowiczSemidefiniteLagrangianRelaxations2000,
  title = {Semidefinite and {{Lagrangian Relaxations}} for {{Hard Combinatorial Problems}}},
  booktitle = {System {{Modelling}} and {{Optimization}}},
  author = {Wolkowicz, Henry},
  editor = {Powell, M. J. D. and Scholtes, S.},
  date = {2000},
  series = {{{IFIP}} — {{The International Federation}} for {{Information Processing}}},
  pages = {269--309},
  publisher = {{Springer US}},
  location = {{Boston, MA}},
  doi = {10.1007/978-0-387-35514-6_13},
  abstract = {Semidefinite Programming is currently a very exciting and active area of research. Semidefinite relaxations generally provide very tight bounds for many classes of numerically hard problems. In addition, these relaxations can be solved efficiently by interior-point methods.},
  isbn = {978-0-387-35514-6},
  langid = {english},
  keywords = {Hard Combinatorial Problems.,Lagrangian Duality,Quadratic Constrained Quadratic Programs,Relaxations,Semidefinite Programming},
  file = {/home/krawczuk/Zotero/storage/5DLJB6GD/Wolkowicz - 2000 - Semidefinite and Lagrangian Relaxations for Hard C.pdf}
}

@online{wuEDGEImprovedTraining2023a,
  title = {{{EDGE}}++: {{Improved Training}} and {{Sampling}} of {{EDGE}}},
  shorttitle = {{{EDGE}}++},
  author = {Wu, Mingyang and Chen, Xiaohui and Liu, Li-Ping},
  date = {2023-10-28},
  eprint = {2310.14441},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2310.14441},
  url = {http://arxiv.org/abs/2310.14441},
  urldate = {2024-02-20},
  abstract = {Recently developed deep neural models like NetGAN, CELL, and Variational Graph Autoencoders have made progress but face limitations in replicating key graph statistics on generating large graphs. Diffusion-based methods have emerged as promising alternatives, however, most of them present challenges in computational efficiency and generative performance. EDGE is effective at modeling large networks, but its current denoising approach can be inefficient, often leading to wasted computational resources and potential mismatches in its generation process. In this paper, we propose enhancements to the EDGE model to address these issues. Specifically, we introduce a degree-specific noise schedule that optimizes the number of active nodes at each timestep, significantly reducing memory consumption. Additionally, we present an improved sampling scheme that fine-tunes the generative process, allowing for better control over the similarity between the synthesized and the true network. Our experimental results demonstrate that the proposed modifications not only improve the efficiency but also enhance the accuracy of the generated graphs, offering a robust and scalable solution for graph generation tasks.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/7IBW7EXE/Wu et al. - 2023 - EDGE++ Improved Training and Sampling of EDGE.pdf;/home/krawczuk/Zotero/storage/L9HBK8R6/2310.html}
}

@online{wuGamoraGraphLearning2023,
  title = {Gamora: {{Graph Learning}} Based {{Symbolic Reasoning}} for {{Large-Scale Boolean Networks}}},
  shorttitle = {Gamora},
  author = {Wu, Nan and Li, Yingjie and Hao, Cong and Dai, Steve and Yu, Cunxi and Xie, Yuan},
  date = {2023-06-12},
  eprint = {2303.08256},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2303.08256},
  url = {http://arxiv.org/abs/2303.08256},
  urldate = {2024-02-20},
  abstract = {Reasoning high-level abstractions from bit-blasted Boolean networks (BNs) such as gate-level netlists can significantly benefit functional verification, logic minimization, datapath synthesis, malicious logic identification, etc. Mostly, conventional reasoning approaches leverage structural hashing and functional propagation, suffering from limited scalability and inefficient usage of modern computing power. In response, we propose a novel symbolic reasoning framework exploiting graph neural networks (GNNs) and GPU acceleration to reason high-level functional blocks from gate-level netlists, namely Gamora, which offers high reasoning performance w.r.t exact reasoning algorithms, strong scalability to BNs with over 33 million nodes, and generalization capability from simple to complex designs. To further demonstrate the capability of Gamora, we also evaluate its reasoning performance after various technology mapping options, since technology-dependent optimizations are known to make functional reasoning much more challenging. Experimental results show that (1) Gamora reaches almost 100\% and over 97\% reasoning accuracy for carry-save-array (CSA) and Booth-encoded multipliers, respectively, with up to six orders of magnitude speedups compared to the state-of-the-art implementation in the ABC framework; (2) Gamora maintains high reasoning accuracy ({$>$}92\%) in finding functional modules after complex technology mapping, upon which we comprehensively analyze the impacts on Gamora reasoning from technology mapping.},
  pubstate = {preprint},
  keywords = {Computer Science - Hardware Architecture},
  file = {/home/krawczuk/Zotero/storage/4CT8S98H/Wu et al. - 2023 - Gamora Graph Learning based Symbolic Reasoning fo.pdf;/home/krawczuk/Zotero/storage/932QZTZJ/2303.html}
}

@article{wuInapproximabilityTreewidthRelated2014,
  title = {Inapproximability of {{Treewidth}} and {{Related Problems}}},
  author = {Wu, Y. and Austrin, P. and Pitassi, T. and Liu, D.},
  date = {2014-04-06},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {49},
  pages = {569--600},
  issn = {1076-9757},
  doi = {10.1613/jair.4030},
  url = {https://www.jair.org/index.php/jair/article/view/10872},
  urldate = {2024-02-22},
  abstract = {Graphical models, such as Bayesian Networks and Markov networks play an important role in artificial intelligence and machine learning. Inference is a central problem to be solved on these networks. This, and other problems on these graph models are often known to be hard to solve in general, but tractable on graphs with bounded Treewidth. Therefore, finding or approximating the Treewidth of a graph is a fundamental problem related to inference in graphical models. In this paper, we study the approximability of a number of graph problems:   Treewidth and Pathwidth of graphs, Minimum Fill-In, One-Shot Black (and Black-White) pebbling costs of directed acyclic graphs, and a variety of different graph layout problems such as Minimum Cut Linear Arrangement and Interval Graph Completion.  We show that, assuming  the recently introduced Small Set Expansion Conjecture, all of these  problems are NP-hard to approximate to within any constant factor in polynomial time.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/S3PIZAUS/Wu et al. - 2014 - Inapproximability of Treewidth and Related Problem.pdf}
}

@online{wuProgramtoCircuitExploitingGNNs2021d,
  title = {Program-to-{{Circuit}}: {{Exploiting GNNs}} for {{Program Representation}} and {{Circuit Translation}}},
  shorttitle = {Program-to-{{Circuit}}},
  author = {Wu, Nan and He, Huake and Xie, Yuan and Li, Pan and Hao, Cong},
  date = {2021-09-13},
  eprint = {2109.06265},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2109.06265},
  url = {http://arxiv.org/abs/2109.06265},
  urldate = {2024-02-20},
  abstract = {Circuit design is complicated and requires extensive domain-specific expertise. One major obstacle stuck on the way to hardware agile development is the considerably time-consuming process of accurate circuit quality evaluation. To significantly expedite the circuit evaluation during the translation from behavioral languages to circuit designs, we formulate it as a Program-to-Circuit problem, aiming to exploit the representation power of graph neural networks (GNNs) by representing C/C++ programs as graphs. The goal of this work is four-fold. First, we build a standard benchmark containing 40k C/C++ programs, each of which is translated to a circuit design with actual hardware quality metrics, aiming to facilitate the development of effective GNNs targeting this high-demand circuit design area. Second, 14 state-of-the-art GNN models are analyzed on the Program-to-Circuit problem. We identify key design challenges of this problem, which should be carefully handled but not yet solved by existing GNNs. The goal is to provide domain-specific knowledge for designing GNNs with suitable inductive biases. Third, we discuss three sets of real-world benchmarks for GNN generalization evaluation, and analyze the performance gap between standard programs and the real-case ones. The goal is to enable transfer learning from limited training data to real-world large-scale circuit design problems. Fourth, the Program-to-Circuit problem is a representative within the Program-to-X framework, a set of program-based analysis problems with various downstream tasks. The in-depth understanding of strength and weaknesses in applying GNNs on Program-to-Circuit could largely benefit the entire family of Program-to-X. Pioneering in this direction, we expect more GNN endeavors to revolutionize this high-demand Program-to-Circuit problem and to enrich the expressiveness of GNNs on programs.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/PJCZDW65/Wu et al. - 2021 - Program-to-Circuit Exploiting GNNs for Program Re.pdf;/home/krawczuk/Zotero/storage/2EIMJH56/2109.html}
}

@article{xiangGeneralGraphGenerators2022,
  title = {General Graph Generators: Experiments, Analyses, and Improvements},
  shorttitle = {General Graph Generators},
  author = {Xiang, Sheng and Wen, Dong and Cheng, Dawei and Zhang, Ying and Qin, Lu and Qian, Zhengping and Lin, Xuemin},
  date = {2022-09-01},
  journaltitle = {The VLDB Journal},
  shortjournal = {The VLDB Journal},
  volume = {31},
  number = {5},
  pages = {897--925},
  issn = {0949-877X},
  doi = {10.1007/s00778-021-00701-5},
  url = {https://doi.org/10.1007/s00778-021-00701-5},
  urldate = {2024-02-20},
  abstract = {Graph simulation is one of the most fundamental problems in graph processing and analytics. It can help users to generate new graphs on different scales to mimic observed real-life graphs in many applications such as social networks, biology networks, and information technology. In this paper, we focus on one of the most important types of graph generators: general graph generators, which aim to reproduce the properties of the observed graphs regardless of the domains. Though a variety of graph generators have been proposed in the literature, there are still several important research gaps in this area. In this paper, we first give an overview of the existing general graph generators, including recently emerged deep learning-based approaches. We classify them into four categories: simple model-based generators, complex model-based generators, autoencoder-based generators, and GAN-based generators. Then we conduct a comprehensive experimental evaluation of 20 representative graph generators based on 17 evaluation metrics and 12 real-life graphs. We provide a general roadmap of recommendations for how to select general graph generators under different settings. Furthermore, we propose a new method that can achieve a good trade-off between simulation quality and efficiency. To help researchers and practitioners apply general graph generators in their applications or make a comprehensive evaluation of their proposed general graph generators, we also implement an end-to-end platform that is publicly available.},
  langid = {english},
  keywords = {Experimental evaluation,Graph generator,Graph neural networks,Graph simulation},
  file = {/home/krawczuk/Zotero/storage/DJB6GJ4F/Xiang et al. - 2022 - General graph generators experiments, analyses, a.pdf}
}

@article{xuGeometricGraphLearning,
  title = {Geometric {{Graph Learning From Representation}} to {{Generation}}},
  author = {Xu, Minkai and Leskovec, Jure},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/5YLDJ7MT/Xu and Leskovec - Geometric Graph Learning From Representation to Ge.pdf}
}

@inproceedings{xuGlobalConvergenceLangevin2018,
  title = {Global {{Convergence}} of {{Langevin Dynamics Based Algorithms}} for {{Nonconvex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xu, Pan and Chen, Jinghui and Zou, Difan and Gu, Quanquan},
  date = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2018/hash/9c19a2aa1d84e04b0bd4bc888792bd1e-Abstract.html},
  urldate = {2024-02-25},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/J8ATAQLQ/Xu et al. - 2018 - Global Convergence of Langevin Dynamics Based Algo.pdf}
}

@online{xuHowPowerfulAre2019e,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2019-02-22},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.1810.00826},
  url = {http://arxiv.org/abs/1810.00826},
  urldate = {2024-02-24},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/GRKJWLXG/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/home/krawczuk/Zotero/storage/G2WJPL6Y/1810.html}
}

@online{xuHowPowerfulAre2019f,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  date = {2019-02-22},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  url = {http://arxiv.org/abs/1810.00826},
  urldate = {2024-02-24},
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  pubstate = {preprint},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/8HIDQ5ZG/Xu et al. - 2019 - How Powerful are Graph Neural Networks.pdf;/home/krawczuk/Zotero/storage/ZLJ7T5I6/1810.html}
}

@inproceedings{xuSNSNotSynthesizer2022a,
  title = {{{SNS}}'s Not a Synthesizer: A Deep-Learning-Based Synthesis Predictor},
  shorttitle = {{{SNS}}'s Not a Synthesizer},
  booktitle = {Proceedings of the 49th {{Annual International Symposium}} on {{Computer Architecture}}},
  author = {Xu, Ceyu and Kjellqvist, Chris and Wills, Lisa Wu},
  date = {2022-06-11},
  series = {{{ISCA}} '22},
  pages = {847--859},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3470496.3527444},
  url = {https://dl.acm.org/doi/10.1145/3470496.3527444},
  urldate = {2024-02-20},
  abstract = {The number of transistors that can fit on one monolithic chip has reached billions to tens of billions in this decade thanks to Moore's Law. With the advancement of every technology generation, the transistor counts per chip grow at a pace that brings about exponential increase in design time, including the synthesis process used to perform design space explorations. Such a long delay in obtaining synthesis results hinders an efficient chip development process, significantly impacting time-to-market. In addition, these large-scale integrated circuits tend to have larger and higher-dimension design spaces to explore, making it prohibitively expensive to obtain physical characteristics of all possible designs using traditional synthesis tools. In this work, we propose a deep-learning-based synthesis predictor called SNS (SNS's not a Synthesizer), that predicts the area, power, and timing physical characteristics of a broad range of designs at two to three orders of magnitude faster than the Synopsys Design Compiler while providing on average a 0.4998 RRSE (root relative square error). We further evaluate SNS via two representative case studies, a general-purpose out-of-order CPU case study using RISC-V Boom open-source design and an accelerator case study using an in-house Chisel implementation of DianNao, to demonstrate the capabilities and validity of SNS.},
  isbn = {978-1-4503-8610-4},
  keywords = {integrated circuits,neural networks,RTL-level synthesis},
  file = {/home/krawczuk/Zotero/storage/HBLKUQS2/Xu et al. - 2022 - SNS's not a synthesizer a deep-learning-based syn.pdf}
}

@inproceedings{yangConditionalStructureGeneration2019e,
  title = {Conditional {{Structure Generation}} through {{Graph Variational Generative Adversarial Nets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yang, Carl and Zhuang, Peiye and Shi, Wenhan and Luu, Alan and Li, Pan},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/e57c6b956a6521b28495f2886ca0977a-Abstract.html},
  urldate = {2024-02-20},
  abstract = {Graph embedding has been intensively studied recently, due to the advance of various neural network models. Theoretical analyses and empirical studies have pushed forward the translation of discrete graph structures into distributed representation vectors, but seldom considered the reverse direction, i.e., generation of graphs from given related context spaces. Particularly, since graphs often become more meaningful when associated with semantic contexts (e.g., social networks of certain communities, gene networks of certain diseases), the ability to infer graph structures according to given semantic conditions could be of great value. While existing graph generative models only consider graph structures without semantic contexts, we formulate the novel problem of conditional structure generation, and propose a novel unified model of graph variational generative adversarial nets (CondGen) to handle the intrinsic challenges of flexible context-structure conditioning and permutation-invariant generation. Extensive experiments on two deliberately created benchmark datasets of real-world context-enriched networks demonstrate the supreme effectiveness and generalizability of CondGen.},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/ZAJ5W7QH/Yang et al. - 2019 - Conditional Structure Generation through Graph Var.pdf}
}

@online{yangScalableDiffusionMaterials2023,
  title = {Scalable {{Diffusion}} for {{Materials Generation}}},
  author = {Yang, Mengjiao and Cho, KwangHwan and Merchant, Amil and Abbeel, Pieter and Schuurmans, Dale and Mordatch, Igor and Cubuk, Ekin Dogus},
  date = {2023-10-18},
  eprint = {2311.09235},
  eprinttype = {arxiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2311.09235},
  url = {http://arxiv.org/abs/2311.09235},
  urldate = {2024-02-20},
  abstract = {Generative models trained on internet-scale data are capable of generating novel and realistic texts, images, and videos. A natural next question is whether these models can advance science, for example by generating novel stable materials. Traditionally, models with explicit structures (e.g., graphs) have been used in modeling structural relationships in scientific data (e.g., atoms and bonds in crystals), but generating structures can be difficult to scale to large and complex systems. Another challenge in generating materials is the mismatch between standard generative modeling metrics and downstream applications. For instance, common metrics such as the reconstruction error do not correlate well with the downstream goal of discovering stable materials. In this work, we tackle the scalability challenge by developing a unified crystal representation that can represent any crystal structure (UniMat), followed by training a diffusion probabilistic model on these UniMat representations. Our empirical results suggest that despite the lack of explicit structure modeling, UniMat can generate high fidelity crystal structures from larger and more complex chemical systems, outperforming previous graph-based approaches under various generative modeling metrics. To better connect the generation quality of materials to downstream applications, such as discovering novel stable materials, we propose additional metrics for evaluating generative models of materials, including per-composition formation energy and stability with respect to convex hulls through decomposition energy from Density Function Theory (DFT). Lastly, we show that conditional generation with UniMat can scale to previously established crystal datasets with up to millions of crystals structures, outperforming random structure search (the current leading method for structure discovery) in discovering new stable materials.},
  pubstate = {preprint},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/QYL3NASV/Yang et al. - 2023 - Scalable Diffusion for Materials Generation.pdf;/home/krawczuk/Zotero/storage/XQ6B6YKD/2311.html}
}

@article{yangVersatileMultistageGraph2022a,
  title = {Versatile {{Multi-stage Graph Neural Network}} for {{Circuit Representation}}},
  author = {Yang, Shuwen and Yang, Zhihao and Li, Dong and Zhang, Yingxueff and Zhang, Zhanguang and Song, Guojie and Hao, Jianye},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {20313--20324},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/7fa548155f40c014372146be387c4f6a-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/F6TCNA5G/Yang et al. - 2022 - Versatile Multi-stage Graph Neural Network for Cir.pdf}
}

@article{yiGraphDenoisingDiffusion2024a,
  title = {Graph {{Denoising Diffusion}} for {{Inverse Protein Folding}}},
  author = {Yi, Kai and Zhou, Bingxin and Shen, Yiqing and Lió, Pietro and Wang, Yuguang},
  date = {2024-02-13},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/20888d00c5df685de2c09790040e0327-Abstract-Conference.html},
  urldate = {2024-02-20},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/5EKY6TSP/Yi et al. - 2024 - Graph Denoising Diffusion for Inverse Protein Fold.pdf}
}

@article{yuLearningEnergyBasedPrior2023a,
  title = {Learning {{Energy-Based Prior Model}} with {{Diffusion-Amortized MCMC}}},
  author = {Yu, Peiyu and Zhu, Yaxuan and Xie, Sirui and Ma, Xiaojian (Shawn) and Gao, Ruiqi and Zhu, Song-Chun and Wu, Ying Nian},
  date = {2023-12-15},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/85381f4549b5ddf1d48e2e287d7d3d15-Abstract-Conference.html},
  urldate = {2024-02-25},
  langid = {english},
  keywords = {⛔ No DOI found},
  file = {/home/krawczuk/Zotero/storage/P9UM6DWN/Yu et al. - 2023 - Learning Energy-Based Prior Model with Diffusion-A.pdf}
}

@article{yusterTreeDecompositionGraphs1998,
  title = {Tree Decomposition of Graphs},
  author = {Yuster, Raphael},
  date = {1998},
  journaltitle = {Random Structures \& Algorithms},
  volume = {12},
  number = {3},
  pages = {237--251},
  issn = {1098-2418},
  doi = {10.1002/(SICI)1098-2418(199805)12:3<237::AID-RSA2>3.0.CO;2-W},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291098-2418%28199805%2912%3A3%3C237%3A%3AAID-RSA2%3E3.0.CO%3B2-W},
  urldate = {2024-02-20},
  abstract = {Let H be a tree on h≥2 vertices. It is shown that if G=(V, E) is a graph with \textbackslash delta (G)\textbackslash ge (|V|/2)+10h\^{}4\textbackslash sqrt|V|\textbackslash log|V|, and h−1 divides |E|, then there is a decomposition of the edges of G into copies of H. This result is asymptotically the best possible for all trees with at least three vertices. © 1998 John Wiley \& Sons, Inc. Random Struct. Alg., 12, 237–251, 1998},
  langid = {english},
  keywords = {decomposition,trees},
  file = {/home/krawczuk/Zotero/storage/6JC9E8UL/Yuster - 1998 - Tree decomposition of graphs.pdf}
}

@article{yuvarajTopologicalClusteringMultilayer2021,
  title = {Topological Clustering of Multilayer Networks},
  author = {Yuvaraj, Monisha and Dey, Asim K. and Lyubchich, Vyacheslav and Gel, Yulia R. and Poor, H. Vincent},
  date = {2021-05-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {21},
  pages = {e2019994118},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2019994118},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.2019994118},
  urldate = {2024-02-22},
  abstract = {Multilayer networks continue to gain significant attention in many areas of study, particularly due to their high utility in modeling interdependent systems such as critical infrastructures, human brain connectome, and socioenvironmental ecosystems. However, clustering of multilayer networks, especially using the information on higher-order interactions of the system entities, still remains in its infancy. In turn, higher-order connectivity is often the key in such multilayer network applications as developing optimal partitioning of critical infrastructures in order to isolate unhealthy system components under cyber-physical threats and simultaneous identification of multiple brain regions affected by trauma or mental illness. In this paper, we introduce the concepts of topological data analysis to studies of complex multilayer networks and propose a topological approach for network clustering. The key rationale is to group nodes based not on pairwise connectivity patterns or relationships between observations recorded at two individual nodes but based on how similar in shape their local neighborhoods are at various resolution scales. Since shapes of local node neighborhoods are quantified using a topological summary in terms of persistence diagrams, we refer to the approach as clustering using persistence diagrams (CPD). CPD systematically accounts for the important heterogeneous higher-order properties of node interactions within and in-between network layers and integrates information from the node neighbors. We illustrate the utility of CPD by applying it to an emerging problem of societal importance: vulnerability zoning of residential properties to weather- and climate-induced risks in the context of house insurance claim dynamics.},
  file = {/home/krawczuk/Zotero/storage/VZELHN56/Yuvaraj et al. - 2021 - Topological clustering of multilayer networks.pdf}
}

@article{zhangEfficientBatchConstrainedBayesian2022,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}}},
  author = {Zhang, Shuhan and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2021.3054811},
  url = {https://ieeexplore.ieee.org/document/9336041},
  urldate = {2024-02-19},
  abstract = {Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this article, we propose an efficient parallelizable Bayesian optimization algorithm via multiobjective acquisition function ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI), and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74\textbackslash times compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15\textbackslash times compared to the weighted EI-based Bayesian optimization (WEIBO) approach, when the batch size is 15.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Acquisition function,analog circuit synthesis,Analog circuits,batch Bayesian optimization,Bayes methods,Circuit synthesis,constrained optimization problem,Gaussian processes,Integrated circuit modeling,Optimization,Space exploration},
  file = {/home/krawczuk/Zotero/storage/JAXPWZ7N/Zhang et al. - 2022 - An Efficient Batch-Constrained Bayesian Optimizati.pdf;/home/krawczuk/Zotero/storage/Z8TYCPTR/9336041.html}
}

@article{zhangEfficientBatchConstrainedBayesian2022a,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}}},
  author = {Zhang, Shuhan and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2021.3054811},
  url = {https://ieeexplore.ieee.org/abstract/document/9336041?casa_token=fPG-7PuzrdgAAAAA:S9Y9hlsyrUarBMsLk8oVfvtisLBEAA9pVyN4KY3L9BNOOoDr4ZhBs5t5v0M1pw27rOSlhvXllF66},
  urldate = {2024-02-20},
  abstract = {Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this article, we propose an efficient parallelizable Bayesian optimization algorithm via multiobjective acquisition function ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI), and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74\textbackslash times compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15\textbackslash times compared to the weighted EI-based Bayesian optimization (WEIBO) approach, when the batch size is 15.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Acquisition function,analog circuit synthesis,Analog circuits,batch Bayesian optimization,Bayes methods,Circuit synthesis,constrained optimization problem,Gaussian processes,Integrated circuit modeling,Optimization,Space exploration},
  file = {/home/krawczuk/Zotero/storage/5J5D7W5V/Zhang et al. - 2022 - An Efficient Batch-Constrained Bayesian Optimizati.pdf}
}

@article{zhangEfficientBatchConstrainedBayesian2022b,
  title = {An {{Efficient Batch-Constrained Bayesian Optimization Approach}} for {{Analog Circuit Synthesis}} via {{Multiobjective Acquisition Ensemble}}},
  author = {Zhang, Shuhan and Yang, Fan and Yan, Changhao and Zhou, Dian and Zeng, Xuan},
  date = {2022-01},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  shortjournal = {IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.},
  volume = {41},
  number = {1},
  pages = {1--14},
  issn = {0278-0070, 1937-4151},
  doi = {10.1109/TCAD.2021.3054811},
  url = {https://ieeexplore.ieee.org/document/9336041/},
  urldate = {2024-02-20},
  abstract = {Bayesian optimization is a promising methodology for analog circuit synthesis. However, the sequential nature of the Bayesian optimization framework significantly limits its ability to fully utilize real-world computational resources. In this article, we propose an efficient parallelizable Bayesian optimization algorithm via multiobjective acquisition function ensemble (MACE) to further accelerate the optimization procedure. By sampling query points from the Pareto front of the probability of improvement (PI), expected improvement (EI), and lower confidence bound (LCB), we combine the benefits of state-of-the-art acquisition functions to achieve a delicate tradeoff between exploration and exploitation for the unconstrained optimization problem. Based on this batch design, we further adjust the algorithm for the constrained optimization problem. By dividing the optimization procedure into two stages and first focusing on finding an initial feasible point, we manage to gain more information about the valid region and can better avoid sampling around the infeasible area. After achieving the first feasible point, we favor the feasible region by adopting a specially designed penalization term to the acquisition function ensemble. The experimental results quantitatively demonstrate that our proposed algorithm can reduce the overall simulation time by up to 74× compared to differential evolution (DE) for the unconstrained optimization problem when the batch size is 15. For the constrained optimization problem, our proposed algorithm can speed up the optimization process by up to 15× compared to the weighted EI-based Bayesian optimization (WEIBO) approach, when the batch size is 15.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/FH9FHIIE/Zhang et al. - 2022 - An Efficient Batch-Constrained Bayesian Optimizati.pdf}
}

@article{zhangMg2vecLearningRelationshipPreserving2022,
  title = {Mg2vec: {{Learning Relationship-Preserving Heterogeneous Graph Representations}} via {{Metagraph Embedding}}},
  shorttitle = {Mg2vec},
  author = {Zhang, Wentao and Fang, Yuan and Liu, Zemin and Wu, Min and Zhang, Xinming},
  date = {2022-03},
  journaltitle = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {34},
  number = {3},
  pages = {1317--1329},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2020.2992500},
  url = {https://ieeexplore.ieee.org/abstract/document/9089251?casa_token=4RGA3jCG5zsAAAAA:oEZwjXhBIJreFavjipIsBtilcbT4v_yGF7oi9rVPWRP40c4s3R9KfQs_2jyNrP4Nln_k7ztEjnkg},
  urldate = {2024-02-22},
  abstract = {Given that heterogeneous information networks (HIN) encompass nodes and edges belonging to different semantic types, they can model complex data in real-world scenarios. Thus, HIN embedding has received increasing attention, which aims to learn node representations in a low-dimensional space, in order to preserve the structural and semantic information on the HIN. In this regard, metagraphs, which model common and recurring patterns on HINs, emerge as a powerful tool to capture semantic-rich and often latent relationships on HINs. Although metagraphs have been employed to address several specific data mining tasks, they have not been thoroughly explored for the more general HIN embedding. In this paper, we leverage metagraphs to learn relationship-preserving HIN embedding in a self-supervised setting, to support various relationship mining tasks. In particular, we observe that most of the current approaches often under-utilize metagraphs, which are only applied in a pre-processing step and do not actively guide representation learning afterwards. Thus, we propose the novel framework of mg2vec, which learns the embeddings for metagraphs and nodes jointly. That is, metagraphs actively participates in the learning process by mapping themselves to the same embedding space as the nodes do. Moreover, metagraphs guide the learning through both first- and second-order constraints on node embeddings, to model not only latent relationships between a pair of nodes, but also individual preferences of each node. Finally, we conduct extensive experiments on three public datasets. Results show that mg2vec significantly outperforms a suite of state-of-the-art baselines in relationship mining tasks including relationship prediction, search and visualization.},
  eventtitle = {{{IEEE Transactions}} on {{Knowledge}} and {{Data Engineering}}},
  keywords = {Companies,Data mining,Heterogeneous information networks,network embedding,Peer-to-peer computing,relationship mining,Semantics,Task analysis,Tools,Toy manufacturing industry},
  file = {/home/krawczuk/Zotero/storage/6474M8ZH/Zhang et al. - 2022 - mg2vec Learning Relationship-Preserving Heterogen.pdf;/home/krawczuk/Zotero/storage/S6LGJG5Q/9089251.html}
}

@inproceedings{zhangPruningEdgesGradients2022b,
  title = {Pruning {{Edges}} and {{Gradients}} to {{Learn Hypergraphs From Larger Sets}}},
  booktitle = {Proceedings of the {{First Learning}} on {{Graphs Conference}}},
  author = {Zhang, David W. and Burghouts, Gertjan J. and Snoek, Cees G. M.},
  date = {2022-12-21},
  pages = {53:1-53:18},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v198/zhang22a.html},
  urldate = {2024-02-20},
  abstract = {This paper aims for set-to-hypergraph prediction, where the goal is to infer the set of relations for a given set of entities. This is a common abstraction for applications in particle physics, biological systems and combinatorial optimization. We address two common scaling problems encountered in set-to-hypergraph tasks that limit the size of the input set: the exponentially growing number of hyperedges and the run-time complexity, both leading to higher memory requirements. We make three contributions. First, we propose to predict and supervise the positive edges only, which changes the asymptotic memory scaling from exponential to linear. Second, we introduce a training method that encourages iterative refinement of the predicted hypergraph, which allows us to skip iterations in the backward pass for improved efficiency and constant memory usage. Third, we combine both contributions in a single set-to-hypergraph model that enables us to address problems with larger input set sizes. We provide ablations for our main technical contributions and show that our model outperforms prior state-of-the-art, especially for larger sets.},
  eventtitle = {Learning on {{Graphs Conference}}},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/P5EN4LFX/Zhang et al. - 2022 - Pruning Edges and Gradients to Learn Hypergraphs F.pdf}
}

@article{zhaoAutomatedTopologySynthesis2020d,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}}},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {12},
  pages = {4325--4337},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2020.2977605},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939?casa_token=3xBY3Muf444AAAAA:T1GnaWYGkpXGyr_Iyex2Z4cDkDe0zsRZlKTAp9br3AnagS94FGBj_5SpgcAeqAZc630quHfvRzHA},
  urldate = {2024-02-19},
  abstract = {This article presents an analog integrated circuit automated topology synthesis framework, where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction. Then the tree structures are decoded into circuit topologies. Our proposed method can not only handle large circuit designs but also generate creative topologies. To ensure only unique circuit topologies to be generated, two levels of isomorphism checks are performed at both tree structure level and circuit topology level. Then the generated un-sized circuit topologies are efficiently evaluated through a new method, which integrates topological symbolic analysis with gm/ID methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Circuit synthesis,Circuit topology,Circuit topology synthesis,curve-fitting,gm/ID methodology,Grammar,graph grammar,isomorphism,Libraries,linear and nonlinear programming (NLP),Reliability,symbolic analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/UMNJ8B9U/Zhao and Zhang - 2020 - An Automated Topology Synthesis Framework for Anal.pdf}
}

@article{zhaoAutomatedTopologySynthesis2020e,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}}},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {12},
  pages = {4325--4337},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2020.2977605},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939},
  urldate = {2024-02-19},
  abstract = {This article presents an analog integrated circuit automated topology synthesis framework, where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction. Then the tree structures are decoded into circuit topologies. Our proposed method can not only handle large circuit designs but also generate creative topologies. To ensure only unique circuit topologies to be generated, two levels of isomorphism checks are performed at both tree structure level and circuit topology level. Then the generated un-sized circuit topologies are efficiently evaluated through a new method, which integrates topological symbolic analysis with gm/ID methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Circuit synthesis,Circuit topology,Circuit topology synthesis,curve-fitting,gm/ID methodology,Grammar,graph grammar,isomorphism,Libraries,linear and nonlinear programming (NLP),Reliability,symbolic analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/4MI4JT5C/Zhao and Zhang - 2020 - An Automated Topology Synthesis Framework for Anal.pdf;/home/krawczuk/Zotero/storage/ZVUEI6LU/9022939.html}
}

@article{zhaoAutomatedTopologySynthesis2020f,
  title = {An {{Automated Topology Synthesis Framework}} for {{Analog Integrated Circuits}}},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2020-12},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  volume = {39},
  number = {12},
  pages = {4325--4337},
  issn = {1937-4151},
  doi = {10.1109/TCAD.2020.2977605},
  url = {https://ieeexplore.ieee.org/abstract/document/9022939?casa_token=7uxs-X0h7PsAAAAA:cb9E_EcXOv70Pyezoy0nQ_Ka-U-MpECq4dc5861LbjSvTGYzg3kUQ8itqlQFuRlbq3_bWJusNHLG},
  urldate = {2024-02-20},
  abstract = {This article presents an analog integrated circuit automated topology synthesis framework, where circuit topology synthesis can be efficiently realized by encoding circuit topology generation process as tree structure construction. Then the tree structures are decoded into circuit topologies. Our proposed method can not only handle large circuit designs but also generate creative topologies. To ensure only unique circuit topologies to be generated, two levels of isomorphism checks are performed at both tree structure level and circuit topology level. Then the generated un-sized circuit topologies are efficiently evaluated through a new method, which integrates topological symbolic analysis with gm/ID methodology and curve-fitting technique. Along with the small-signal analysis, both linear and nonlinear programming techniques are utilized for topology feasibility checking. With only a small number of circuit topologies through the fast evaluation stage toward the subsequent detailed sizing and further evaluation, the efficiency of the whole circuit synthesis process can be significantly improved. The experimental results demonstrate high efficiency, strong reliability, and wide applicability of our proposed methods.},
  eventtitle = {{{IEEE Transactions}} on {{Computer-Aided Design}} of {{Integrated Circuits}} and {{Systems}}},
  keywords = {Analog circuits,Circuit synthesis,Circuit topology,Circuit topology synthesis,curve-fitting,gm/ID methodology,Grammar,graph grammar,isomorphism,Libraries,linear and nonlinear programming (NLP),Reliability,symbolic analysis,Topology},
  file = {/home/krawczuk/Zotero/storage/U7DHXFTD/9022939.html}
}

@thesis{zhaoAutomatedTopologySynthesis2022a,
  type = {doctoral},
  title = {Automated Topology Synthesis for Analog Integrated Circuits},
  author = {Zhao, Zhenxin},
  date = {2022-05},
  institution = {{Memorial University of Newfoundland}},
  url = {https://research.library.mun.ca/15733/},
  urldate = {2024-02-20},
  abstract = {Currently, except for circuit topology synthesis, all the other phases in the analog integrated circuit design procedure are equipped with electronic design automation (EDA) commercial tools to greatly facilitate the human laborious work and significantly improve the design productivity, even though they are still not as mature as digital EDA counterparts. This dissertation focuses on developing a circuit topology synthesis EDA tool for analog integrated circuits. In order to make the developed EDA tool commercializable, there are many challenges that have to be solved, including trustworthy solutions, innovative solutions, wide applicability, sound generalization capability, and affordable computation effort. This thesis proposes a graph-based generation method to automatically synthesize analog integrated circuits, which has partially solved some challenges. But one serious problem of this method is its unaffordable computation effort due to the time-consuming sizing process for a huge number of generated circuit structures. To address this problem, we propose a novel performance modeling method that can boost the sizing efficiency by more than 30 times with ignorable model building overhead, which is especially suitable for the circuit synthesis work that involves generating various circuit structures. With the assistance of the emerging machine learning advancement, EDA tools can be more efficient and effective. We have employed the deep reinforcement learning technique in this dissertation to synthesize analog integrated circuit structures. Its technical merits make it be able to address those pending challenges much better than the graph-based generation method. But it still suffers from a shortcoming, that is, the learning process has to be performed from scratch once the technology or design specification changes. In order to overcome this shortcoming, the transfer learning technique is applied to transfer the learned knowledge from a learning process to another in order to largely save the learning effort. The experimental results exhibit strong efficacy and great applicability of our proposed methods.},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/696XPU7M/Zhao - 2022 - Automated topology synthesis for analog integrated.pdf;/home/krawczuk/Zotero/storage/IJE6F468/15733.html}
}

@inproceedings{zhaoDeepReinforcementLearning2022,
  title = {Deep {{Reinforcement Learning}} for {{Analog Circuit Structure Synthesis}}},
  booktitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2022-03},
  pages = {1157--1160},
  issn = {1558-1101},
  doi = {10.23919/DATE54114.2022.9774699},
  url = {https://ieeexplore.ieee.org/abstract/document/9774699?casa_token=6DJz5YkmihoAAAAA:yJJNEopf_gZwhkjWLDq_MMOBt-pOL0OkwWypqhPaXy_5cnNr4LG9ELx1Ylz6af9ARurxEo4go_Lc},
  urldate = {2024-02-19},
  abstract = {This paper presents a novel deep-reinforcement-learning-based method for analog circuit structure synthesis. It behaves like a designer, who learns from trials, derives design knowledge and experience, and evolves gradually to eventually figure out a way to construct circuit structures that can meet the given design specifications. Necessary design rules are defined and applied to set up the specialized environment of reinforcement learning in order to reasonably construct circuit structures. The produced circuit structures are then verified by the simulation-in-loop sizing. In addition, hash table and symbolic analysis techniques are employed to significantly promote the evaluation efficiency. Our experimental results demonstrate the sound efficiency, strong reliability, and wide applicability of the proposed method.},
  eventtitle = {2022 {{Design}}, {{Automation}} \& {{Test}} in {{Europe Conference}} \& {{Exhibition}} ({{DATE}})},
  keywords = {analog circuit synthesis,Analog circuits,Circuit synthesis,deep reinforcement learning,hash table,Integrated circuits,MIMICs,Neural networks,Reinforcement learning,Reliability},
  file = {/home/krawczuk/Zotero/storage/C5QWLXPU/Zhao and Zhang - 2022 - Deep Reinforcement Learning for Analog Circuit Str.pdf;/home/krawczuk/Zotero/storage/EH5RAFJ7/9774699.html}
}

@inproceedings{zhaoGraphGrammarBasedAnalogCircuit2019,
  title = {Graph-{{Grammar-Based Analog Circuit Topology Synthesis}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2019-05},
  pages = {1--5},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2019.8702574},
  url = {https://ieeexplore.ieee.org/document/8702574},
  urldate = {2024-02-19},
  abstract = {Automatically constructing analog circuit topology according to specifications is always a challenging task, due to the high complexity and substantial design expertise required. This paper proposes a graph-grammar-based method that can efficiently and automatically generate analog circuit topologies, which can be applied to general analog circuit synthesis frameworks for analog circuit design. The topology generation process is encoded by constructing a binary tree, in which the leaf nodes are decomposed according to a set of grammar rules. In order to guarantee only unique circuit structures to be generated, double isomorphism checks are applied at both tree structure level and circuit transistor level. Our experimental results demonstrate the high efficiency and wide applicability of the proposed method.},
  eventtitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  keywords = {analog circuit synthesis,Analog circuits,binary tree,Binary trees,Circuit synthesis,Circuit topology,Grammar,graph grammar,isomorphism,Production,Topology},
  file = {/home/krawczuk/Zotero/storage/YZS3BC2S/Zhao and Zhang - 2019 - Graph-Grammar-Based Analog Circuit Topology Synthe.pdf;/home/krawczuk/Zotero/storage/9HZUYZFJ/8702574.html}
}

@inproceedings{zhaoGraphGrammarBasedAnalogCircuit2019a,
  title = {Graph-{{Grammar-Based Analog Circuit Topology Synthesis}}},
  booktitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Zhao, Zhenxin and Zhang, Lihong},
  date = {2019-05},
  pages = {1--5},
  issn = {2158-1525},
  doi = {10.1109/ISCAS.2019.8702574},
  url = {https://ieeexplore.ieee.org/abstract/document/8702574?casa_token=uSk50cYGkdkAAAAA:ku0uzr88bmIkOPMFRzBdLio60QozMU4N_jqxbZXjfAZ5d81YnskNlgn565S4ZSVECDNOa1qq2xA},
  urldate = {2024-02-24},
  abstract = {Automatically constructing analog circuit topology according to specifications is always a challenging task, due to the high complexity and substantial design expertise required. This paper proposes a graph-grammar-based method that can efficiently and automatically generate analog circuit topologies, which can be applied to general analog circuit synthesis frameworks for analog circuit design. The topology generation process is encoded by constructing a binary tree, in which the leaf nodes are decomposed according to a set of grammar rules. In order to guarantee only unique circuit structures to be generated, double isomorphism checks are applied at both tree structure level and circuit transistor level. Our experimental results demonstrate the high efficiency and wide applicability of the proposed method.},
  eventtitle = {2019 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  keywords = {analog circuit synthesis,Analog circuits,binary tree,Binary trees,Circuit synthesis,Circuit topology,Grammar,graph grammar,isomorphism,Production,Topology},
  file = {/home/krawczuk/Zotero/storage/IUI9XMBX/Zhao and Zhang - 2019 - Graph-Grammar-Based Analog Circuit Topology Synthe.pdf;/home/krawczuk/Zotero/storage/D4AARICB/8702574.html}
}

@online{zhaoPardPermutationInvariantAutoregressive2024a,
  title = {Pard: {{Permutation-Invariant Autoregressive Diffusion}} for {{Graph Generation}}},
  shorttitle = {Pard},
  author = {Zhao, Lingxiao and Ding, Xueying and Akoglu, Leman},
  date = {2024-02-05},
  eprint = {2402.03687},
  eprinttype = {arxiv},
  eprintclass = {cs, stat},
  doi = {10.48550/arXiv.2402.03687},
  url = {http://arxiv.org/abs/2402.03687},
  urldate = {2024-02-20},
  abstract = {Graph generation has been dominated by autoregressive models due to their simplicity and effectiveness, despite their sensitivity to ordering. Yet diffusion models have garnered increasing attention, as they offer comparable performance while being permutation-invariant. Current graph diffusion models generate graphs in a one-shot fashion, but they require extra features and thousands of denoising steps to achieve optimal performance. We introduce PARD, a Permutation-invariant Auto Regressive Diffusion model that integrates diffusion models with autoregressive methods. PARD harnesses the effectiveness and efficiency of the autoregressive model while maintaining permutation invariance without ordering sensitivity. Specifically, we show that contrary to sets, elements in a graph are not entirely unordered and there is a unique partial order for nodes and edges. With this partial order, PARD generates a graph in a block-by-block, autoregressive fashion, where each block's probability is conditionally modeled by a shared diffusion model with an equivariant network. To ensure efficiency while being expressive, we further propose a higher-order graph transformer, which integrates transformer with PPGN. Like GPT, we extend the higher-order graph transformer to support parallel training of all blocks. Without any extra features, PARD achieves state-of-the-art performance on molecular and non-molecular datasets, and scales to large datasets like MOSES containing 1.9M molecules.},
  pubstate = {preprint},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/krawczuk/Zotero/storage/YA3TLZZJ/Zhao et al. - 2024 - Pard Permutation-Invariant Autoregressive Diffusi.pdf;/home/krawczuk/Zotero/storage/GCLEUHII/2402.html}
}

@article{zhaoSignalDivisionAwareAnalogCircuit2023,
  title = {Signal-{{Division-Aware Analog Circuit Topology Synthesis Aided}} by {{Transfer Learning}}},
  author = {Zhao, Zhenxin and Luo, Jiang and Liu, Jun and Zhang, Lihong},
  date = {2023},
  journaltitle = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
  publisher = {{IEEE}},
  doi = {10.1109/TCAD.2023.3245979},
  url = {https://ieeexplore.ieee.org/abstract/document/10045726/?casa_token=pKwJBn97qqUAAAAA:IAX_s71j4QkY8MGOfPxOaCi0kZHrw4UcN1-IFIxTCZOuOg47IgDbbugxtSij33jUHwGsqbFdAMkv},
  urldate = {2024-02-19}
}

@inproceedings{zhouSPARCSelfPacedNetwork2018,
  title = {{{SPARC}}: {{Self-Paced Network Representation}} for {{Few-Shot Rare Category Characterization}}},
  shorttitle = {{{SPARC}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Zhou, Dawei and He, Jingrui and Yang, Hongxia and Fan, Wei},
  date = {2018-07-19},
  series = {{{KDD}} '18},
  pages = {2807--2816},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3219819.3219968},
  url = {https://dl.acm.org/doi/10.1145/3219819.3219968},
  urldate = {2024-02-22},
  abstract = {In the era of big data, it is often the rare categories that are of great interest in many high-impact applications, ranging from financial fraud detection in online transaction networks to emerging trend detection in social networks, from network intrusion detection in computer networks to fault detection in manufacturing. As a result, rare category characterization becomes a fundamental learning task, which aims to accurately characterize the rare categories given limited label information. The unique challenge of rare category characterization, i.e., the non-separability nature of the rare categories from the majority classes, together with the availability of the multi-modal representation of the examples, poses a new research question: how can we learn a salient rare category oriented embedding representation such that the rare examples are well separated from the majority class examples in the embedding space, which facilitates the follow-up rare category characterization? To address this question, inspired by the family of curriculum learning that simulates the cognitive mechanism of human beings, we propose a self-paced framework named SPARC that gradually learns the rare category oriented network representation and the characterization model in a mutually beneficial way by shifting from the 'easy' concept to the target 'difficult' one, in order to facilitate more reliable label propagation to the large number of unlabeled examples. The experimental results on various real data demonstrate that our proposed SPARC algorithm: (1) shows a significant improvement over state-of-the-art graph embedding methods on representing the rare categories that are non-separable from the majority classes; (2) outperforms the existing methods on rare category characterization tasks.},
  isbn = {978-1-4503-5552-0},
  keywords = {network embedding,rare category analysis,self-paced learning},
  file = {/home/krawczuk/Zotero/storage/BADXTNDM/Zhou et al. - 2018 - SPARC Self-Paced Network Representation for Few-S.pdf}
}

@inproceedings{zhuEffectiveAnalogMixedsignal2020b,
  title = {Effective Analog/Mixed-Signal Circuit Placement Considering System Signal Flow},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Computer-Aided Design}}},
  author = {Zhu, Keren and Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Sun, Nan and Pan, David Z.},
  date = {2020-12-17},
  series = {{{ICCAD}} '20},
  pages = {1--9},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3400302.3415625},
  url = {https://dl.acm.org/doi/10.1145/3400302.3415625},
  urldate = {2024-02-20},
  abstract = {Placement is among the most critical steps in analog/mixed-signal (AMS) circuit layout synthesis. It implicitly determines the wiring topology and therefore has considerable impacts on post-layout parasitics and coupling. Existing analog placement techniques are mainly focusing on geometric constraints in analog building blocks. However, there yet lacks an effective way to consider the systemlevel signal flow for sensitive AMS circuits. Leveraging prior knowledge from schematics, we propose to consider the critical signal paths in automatic AMS placement and present an efficient framework. Experimental results demonstrate our proposed framework's efficiency and effectiveness with a 22.8\% reduction in routed wire-length compared to state-of-the-art AMS placer and 10 dB improvement in the signal-to-noise-and-distortion ratio (SNDR) for an ADC.},
  isbn = {978-1-4503-8026-3},
  file = {/home/krawczuk/Zotero/storage/MW643V64/Zhu et al. - 2020 - Effective analogmixed-signal circuit placement co.pdf}
}

@inproceedings{zhuEffectiveAnalogMixedsignal2020c,
  title = {Effective Analog/Mixed-Signal Circuit Placement Considering System Signal Flow},
  booktitle = {Proceedings of the 39th {{International Conference}} on {{Computer-Aided Design}}},
  author = {Zhu, Keren and Chen, Hao and Liu, Mingjie and Tang, Xiyuan and Sun, Nan and Pan, David Z.},
  date = {2020-11-02},
  pages = {1--9},
  publisher = {{ACM}},
  location = {{Virtual Event USA}},
  doi = {10.1145/3400302.3415625},
  url = {https://dl.acm.org/doi/10.1145/3400302.3415625},
  urldate = {2024-02-20},
  eventtitle = {{{ICCAD}} '20: {{IEEE}}/{{ACM International Conference}} on {{Computer-Aided Design}}},
  isbn = {978-1-4503-8026-3},
  langid = {english},
  file = {/home/krawczuk/Zotero/storage/5NFLZPNZ/Zhu et al. - 2020 - Effective analogmixed-signal circuit placement co.pdf}
}
